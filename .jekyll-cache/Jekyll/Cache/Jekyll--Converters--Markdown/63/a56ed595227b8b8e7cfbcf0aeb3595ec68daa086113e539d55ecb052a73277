I"ô<p><a href="https://arxiv.org/pdf/1704.08082.pdf">Paper link</a> and <a href="https://github.com/ducksoup/autodial">code</a></p>

<h4 id="take-away-message">Take away message</h4>
<ol>
  <li>limitation of AdaBN: the target samples have no influence on the network parameters, as they are not observed during training.</li>
  <li>å¯¹è®­ç»ƒè¿‡ç¨‹æè¿°çš„å¾ˆè¯¦ç»†ï¼Œå¯ä»¥å‚è€ƒ</li>
</ol>

<h4 id="model">Model</h4>
<p><img src="/img/15873547625687.jpg" alt="-w821" /></p>

<p><img src="/img/15873551722703.jpg" width="25%" height="100%" /><img src="/img/15873552325041.jpg" width="25%" height="100%" /><img src="/img/15873552520236.jpg" width="50%" height="100%" /><br />
å…¶ä¸­ï¼Œ $\alpha$ åœ¨[0.5, 1]ä¸­ï¼š1å¯¹åº”AdaBN (full degree of DA)ï¼›0.5å¯¹åº”æ²¡æœ‰DAï¼Œå› ä¸ºæ­¤æ—¶$q_\alpha^{st}$å’Œ$q_\alpha^{ts}$ç›¸åŒï¼Œå³sourceå’Œtarget domainä¼šåšç›¸åŒçš„transformationã€‚é€šè¿‡è®©ç½‘ç»œè‡ªå·±å­¦ä¹ å‚æ•°$\alpha$ï¼Œå®ç°äº†automatically learn the degree of alignment that should be pursued at different levels of the network.</p>

<h4 id="tricks">Tricks</h4>
<p>Bayes based training process:</p>
<ol>
  <li>objective:<br />
<img src="/img/15873556873967.jpg" width="45%" height="100%" /><img src="/img/15873557363954.jpg" width="30%" height="100%" /></li>
  <li>sampling distribution<br />
<img src="/img/15873558553822.jpg" width="35%" height="100%" /></li>
  <li>
    <p>prior
<img src="/img/15873559027282.jpg" width="45%" height="100%" /><br />
<img src="/img/15873559171140.jpg" width="45%" height="100%" /></p>
  </li>
  <li>è½¬åŒ–ä¸ºtraining objectiveï¼š
<img src="/img/15873559673684.jpg" width="45%" height="100%" />  <br />
source domainç”¨standard cross entropy lossï¼Œ target domainç”¨entropy minimization.</li>
</ol>

<h4 id="result">Result</h4>
<ol>
  <li>$\alpha$ å¤§éƒ¨åˆ†æ¥è¿‘1.<br />
<img src="/img/15873565357251.jpg" alt="-w907" /></li>
</ol>

<p>others:</p>
<ol>
  <li>the entropy regularization term is especially beneficial when source and target data representations are aligned. (?)</li>
  <li>lower layers in a network are subject to domain shift even more than the very last layers.</li>
</ol>

<h4 id="reflection">Reflection</h4>
<ol>
  <li>åŒæ—¶ç»“åˆäº†AdaBNå’Œentropy minimizationï¼Œ ä¸ç¡®å®šç»“æœçš„æé«˜æ˜¯å¦æ¥æºäºæå‡ºçš„DA layer.</li>
</ol>

:ET