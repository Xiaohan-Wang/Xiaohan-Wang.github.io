I"¥
<p>åŸæ–‡ï¼š</p>
<ol>
  <li>https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced</li>
  <li>https://freecontent.manning.com/the-computer-vision-pipeline-part-3-image-preprocessing/</li>
  <li>https://www.codecademy.com/articles/normalization</li>
</ol>

<h4 id="why-image-preprocessing">why image preprocessing</h4>
<ol>
  <li>The acquired data are usually messy and come from different sources, they need to be standardized and cleaned up.</li>
  <li>We canâ€™t write a unique algorithm for each of the condition in which an image is taken, thus, when we acquire an image, we tend to convert it into a form that allows a general algorithm to solve it.</li>
  <li>It can reduce the complexity and increase the accuracy of the applied algorithm.</li>
</ol>

<h4 id="å½©è‰²å›¾åƒ--ç°åº¦å›¾åƒ">å½©è‰²å›¾åƒ / ç°åº¦å›¾åƒ</h4>
<ol>
  <li>In many objects, color isnâ€™t necessary to recognize and interpret an image. Grayscale can be good enough for recognizing certain objects. Because color images contain more information than black and white images, they can add unnecessary complexity and take up more space in memory.
 <img src="/img/15904315018825.jpg" width="100%" height="100%" /></li>
  <li>In other applications, color is important to define certain objects. Like skin cancer detection which relies heavily on the skin colors (red rashes).</li>
</ol>

<h4 id="standardize-image">Standardize Image</h4>
<h5 id="reason">Reason</h5>
<ol>
  <li>If we didnâ€™t scale our input training vectors, the ranges of our distributions of feature values would likely be different for each feature, and thus the learning rate would <strong>cause corrections in each dimension that would differ (proportionally speaking) from one another</strong>. We might be over compensating a correction in one weight dimension while undercompensating in another.</li>
  <li>In the process of training our network, weâ€™re going to be multiplying (weights) and adding to (biases) these initial inputs in order to cause activations that we then backpropogate with the gradients to train the model. Weâ€™d like in this process for each feature to have a similar range so that our <strong>gradients donâ€™t go out of control</strong>.</li>
</ol>

<h5 id="ways">ways</h5>
<ol>
  <li>Min-max normalization $\frac{\text{value}-\text{min}}{\text{max}-\text{min}}$: Guarantees all features will have the exact same scale but does not handle outliers well.<br />
 <img src="/img/15904331747160.jpg" width="60%" height="100%" /><br />
 Normalizing fixed the squishing problem on the y-axis, but the x-axis is still problematic. Now if we were to compare these points, the y-axis would dominate; the y-axis can differ by 1, but the x-axis can only differ by 0.4.</li>
  <li>Z-score normalization $\frac{value-mean}{std}$: Handles outliers, but does not produce normalized data with the exact same scale.<br />
 <img src="/img/15904332447812.jpg" width="60%" height="100%" /><br />
 While the data still looks squished, notice that the points are now on roughly the same scale for both features â€” almost all points are between -2 and 2 on both the x-axis and y-axis.</li>
</ol>

<h4 id="data-augmentation">Data Augmentation</h4>
<h5 id="offline--online-augmentation">Offline / Online augmentation</h5>
<ol>
  <li>Offline augmentation:
    <ul>
      <li>äº‹å…ˆè¿›è¡Œæ‰€æœ‰å¿…éœ€çš„å›¾åƒå¹³ç§»å·¥ä½œï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯å¢åŠ æ•°æ®é›†å¤§å°</li>
      <li>å½“æ•°æ®é›†ç›¸å¯¹è¾ƒå°æ—¶ï¼Œä¼˜å…ˆé€‰æ‹©è¿™ç§æ–¹æ³•ï¼Œå› ä¸ºä¼šå°†æ•°æ®é›†å¢å¤§Nå€ï¼ˆN=æ‰§è¡Œçš„è½¬æ¢æ•°é‡ï¼‰</li>
    </ul>
  </li>
  <li>Online augmentation:
    <ul>
      <li>å°†å›¾åƒè¾“å…¥æœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹å‰ï¼Œä»¥å°æ‰¹é‡è¿›è¡Œå›¾åƒå¹³ç§»</li>
      <li>æ¯”è¾ƒé€‚åˆæ•°æ®é›†è¾ƒå¤§æ—¶çš„æƒ…å†µï¼Œå› ä¸ºæˆ‘ä»¬å¾ˆéš¾åº”ä»˜æ•°æ®é›†çˆ†ç‚¸æ€§å˜å¤§ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥å°æ‰¹é‡å¹³ç§»è¾“å…¥åˆ°æ¨¡å‹ä¸­çš„å›¾åƒã€‚æœ‰äº›æœºå™¨å­¦ä¹ æ¡†æ¶æ”¯æŒåœ¨çº¿å¢å¼ºï¼Œä½¿ç”¨GPUå¯ä»¥åŠ å¿«å¢å¼ºé€Ÿåº¦ã€‚</li>
    </ul>
  </li>
</ol>

<h5 id="åŸºæœ¬æ•°æ®å¢å¼ºæ–¹æ³•">åŸºæœ¬æ•°æ®å¢å¼ºæ–¹æ³•</h5>
<ol>
  <li>ç¿»è½¬</li>
  <li>æ—‹è½¬
    <ul>
      <li>ç”±äºå¯èƒ½å¼•å…¥äº†å›¾åƒè¾¹ç•Œä¹‹å¤–çš„ä½ç½®ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ²¡æœ‰å›¾åƒæ²¡æœ‰è¦†ç›–çš„é»‘è‰²åŒºåŸŸ</li>
      <li>è§£å†³æ–¹æ³•ï¼šå¡«å……æœªçŸ¥åŒºåŸŸ
        <ul>
          <li>å¸¸æ•°ã€è¾¹ç¼˜ã€åå°„ã€å¯¹ç§°</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>å¹³ç§»</li>
  <li>è£å‰ª
    <ul>
      <li>ä»åŸå§‹å›¾åƒä¸­éšæœºé‡‡æ ·ä¸€éƒ¨åˆ†ã€‚ç„¶åæˆ‘ä»¬å°†é‡‡æ ·åŒºåŸŸå¤§å°è°ƒæ•´ä¸ºåŸå§‹å›¾åƒå¤§å°</li>
    </ul>
  </li>
  <li>ç¼©æ”¾</li>
  <li>é«˜æ–¯å™ªå£°
    <ul>
      <li>å½“ç¥ç»ç½‘ç»œè¯•å›¾å­¦ä¹ å¯èƒ½å¹¶æ— ç”¨å¤„çš„é«˜é¢‘ç‰¹å¾æ—¶ï¼ˆå³é¢‘ç¹å‘ç”Ÿçš„æ— æ„ä¹‰æ¨¡å¼ï¼‰ï¼Œå¸¸å¸¸ä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆã€‚å…·æœ‰é›¶å‡å€¼ç‰¹å¾çš„é«˜æ–¯å™ªå£°æœ¬è´¨ä¸Šå°±æ˜¯åœ¨æ‰€æœ‰é¢‘ç‡ä¸Šéƒ½æœ‰æ•°æ®ç‚¹ï¼Œèƒ½æœ‰æ•ˆä½¿å¾—é«˜é¢‘ç‰¹å¾å¤±çœŸï¼Œå‡å¼±å®ƒå¯¹æ¨¡å‹çš„å½±å“ã€‚è¿™ä¹Ÿæ„å‘³ç€ä½é¢‘æˆåˆ†ï¼ˆé€šå¸¸ä¹Ÿæ˜¯æˆ‘ä»¬å…³å¿ƒçš„æ•°æ®ï¼‰ä¹Ÿä¼šå¤±çœŸï¼Œä½†ç¥ç»ç½‘ç»œèƒ½å¤Ÿé€šè¿‡å­¦ä¹ å¿½ç•¥è¿™éƒ¨åˆ†å½±å“ã€‚æ·»åŠ æ­£ç¡®æ•°é‡çš„å™ªå£°å°±èƒ½å¢å¼ºç¥ç»ç½‘ç»œçš„å­¦ä¹ èƒ½åŠ›ã€‚</li>
    </ul>
  </li>
</ol>

<ul>
  <li>1-3éƒ½æ˜¯ä»¿å°„å˜åŒ–ï¼Œå¯ä»¥é€šè¿‡$2\times3$çš„çŸ©é˜µå®ç°
    <ul>
      <li>R: $2 \times 2$ï¼Œçº¿æ€§å˜æ¢çŸ©é˜µ</li>
      <li>t: $2 \times 1$, å¹³ç§»çŸ©é˜µ</li>
    </ul>
  </li>
</ul>

<h5 id="åŸºäºç½‘ç»œçš„æ•°æ®å¢å¼ºæ–¹æ³•">åŸºäºç½‘ç»œçš„æ•°æ®å¢å¼ºæ–¹æ³•</h5>
<ol>
  <li>æ¡ä»¶å¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</li>
</ol>

<blockquote>
  <p>No free lunch theorem for optimization:
In ML project, it means that thereâ€™s no single prescribed recipe that is guaranteed to work well in all situations. We must make certain assumption about the dataset and the problem we are trying to solve.</p>
</blockquote>

:ET