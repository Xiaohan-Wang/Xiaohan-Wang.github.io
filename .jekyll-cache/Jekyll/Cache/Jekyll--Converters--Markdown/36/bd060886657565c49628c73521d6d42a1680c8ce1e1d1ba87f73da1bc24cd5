I"G
<h4 id="warm-up">Warm up</h4>
<ol>
  <li>减缓模型在训练初期过拟合mini-batch</li>
  <li>有助于保持</li>
</ol>

<h4 id="learning-rate-decay">Learning rate decay</h4>
<p>学习率衰减的基本思想是学习率随着训练的进行逐渐衰减，即在开始的时候使用较大的学习率，加快靠近最小值的速度，在后来些时候用较小的学习率，提高稳定性，避免因学习率太大跳过最小值，保证能够收敛到最小值。</p>

<ol>
  <li>
    <p>step(固定步长衰减): 每隔step_size个epoch后，lr衰减为原来的gamma倍</p>

    <p><img src="/img/15906775729941.jpg" width="50%" height="100%" /></p>
  </li>
  <li>
    <p>multistep(多步长衰减): 动态步长控制，lr衰减为原来的gamma倍</p>

    <p><img src="/img/15906776874073.jpg" width="50%" height="100%" /></p>
  </li>
  <li>poly(多项式衰减): $lr = \text{base_lr} * (1 - \frac{T_{cur}}{T_{max}}) ^ {power} $
    <ul>
      <li>学习率曲线的形状主要由参数 power 的值来控制。当 power = 1 的时候，学习率曲线为一条直线。当 power &lt; 1 的时候，下降速率由慢到快。当 power &gt; 1 的时候，下降速率由快到慢。
 <img src="/img/15906784236558.jpg" width="50%" height="100%" /></li>
    </ul>
  </li>
  <li>cosine(余弦衰减): $lr = \frac{1}{2}*\text{base_lr} * \left(1+ \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)$
    <ul>
      <li>余弦值首先缓慢下降，然后加速下降，之后再次缓慢下降。</li>
    </ul>

    <p><img src="/img/15906782692333.jpg" width="50%" height="100%" /></p>
  </li>
</ol>

<h4 id="参考资料">参考资料</h4>
<ol>
  <li>https://lumingdong.cn/setting-strategy-of-gradient-descent-learning-rate.html</li>
  <li>https://zhuanlan.zhihu.com/p/39565465</li>
  <li>https://www.zhihu.com/question/338066667</li>
</ol>

:ET