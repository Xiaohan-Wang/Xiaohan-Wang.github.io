I"o
<h4 id="warm-up">Warm up</h4>
<ol>
  <li>减缓模型在训练初期过拟合mini-batch
    <ul>
      <li>个人猜测：在第一轮训练的时候（尤其是训练刚开始），模型只见过部分数据，此时梯度大概率是偏离相对全局真正较优的方向的（此时很大概率是过拟合当前小部分数据的方向）</li>
    </ul>

    <p>第一种情况很好理解，可以认为，刚开始模型对数据的“分布”理解为零，或者是说“均匀分布”（当然这取决于你的初始化）；在第一轮训练的时候，每个数据点对模型来说都是新的，模型会很快地进行数据分布修正，如果这时候学习率就很大，极有可能导致开始的时候就对该数据“过拟合”，后面要通过多轮训练才能拉回来，浪费时间。当训练了一段时间（比如两轮、三轮）后，模型已经对每个数据点看过几遍了，或者说对当前的batch而言有了一些正确的先验，较大的学习率就不那么容易会使模型学偏，所以可以适当调大学习率。这个过程就可以看做是warmup。</p>
  </li>
  <li>有助于保持模型深层的稳定性</li>
</ol>

<h4 id="learning-rate-decay">Learning rate decay</h4>
<p>学习率衰减的基本思想是学习率随着训练的进行逐渐衰减，即在开始的时候使用较大的学习率，加快靠近最小值的速度，在后来些时候用较小的学习率，提高稳定性，避免因学习率太大跳过最小值，保证能够收敛到最小值。</p>

<ol>
  <li>
    <p>step(固定步长衰减): 每隔step_size个epoch后，lr衰减为原来的gamma倍</p>

    <p><img src="/img/15906775729941.jpg" width="50%" height="100%" /></p>
  </li>
  <li>
    <p>multistep(多步长衰减): 动态步长控制，lr衰减为原来的gamma倍</p>

    <p><img src="/img/15906776874073.jpg" width="50%" height="100%" /></p>
  </li>
  <li>poly(多项式衰减): $lr = \text{base_lr} * (1 - \frac{T_{cur}}{T_{max}}) ^ {power} $
    <ul>
      <li>学习率曲线的形状主要由参数 power 的值来控制。当 power = 1 的时候，学习率曲线为一条直线。当 power &lt; 1 的时候，下降速率由慢到快。当 power &gt; 1 的时候，下降速率由快到慢。
 <img src="/img/15906784236558.jpg" width="50%" height="100%" /></li>
    </ul>
  </li>
  <li>cosine(余弦衰减): $lr = \frac{1}{2}*\text{base_lr} * \left(1+ \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)$
    <ul>
      <li>余弦值首先缓慢下降，然后加速下降，之后再次缓慢下降。</li>
    </ul>

    <p><img src="/img/15906782692333.jpg" width="50%" height="100%" /></p>
  </li>
</ol>

<h4 id="参考资料">参考资料</h4>
<ol>
  <li>https://lumingdong.cn/setting-strategy-of-gradient-descent-learning-rate.html</li>
  <li>https://zhuanlan.zhihu.com/p/39565465</li>
  <li>https://www.zhihu.com/question/338066667</li>
</ol>

:ET