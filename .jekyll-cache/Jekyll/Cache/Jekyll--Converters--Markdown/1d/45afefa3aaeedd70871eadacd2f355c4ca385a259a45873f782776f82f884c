I"ñ
<h4 id="em-algorithm">EM algorithm</h4>
<ul>
  <li>EMç®—æ³•æ˜¯æœŸæœ›æœ€å¤§åŒ–(Expectation Maximization)ç®—æ³•çš„ç®€ç§°</li>
  <li>ç”¨äº<strong>å«æœ‰éšå˜é‡(hidden variable)</strong>çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹å‚æ•°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡</li>
  <li>EMç®—æ³•æ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ï¼Œæ¯æ¬¡è¿­ä»£ç”±ä¸¤æ­¥ç»„æˆï¼š
    <ul>
      <li>Eæ­¥ï¼šæ ¹æ®æ¨¡å‹å‚æ•°çš„å‡è®¾å€¼ï¼Œç»™å‡ºéšå˜é‡çš„æœŸæœ›ä¼°è®¡ï¼Œåº”ç”¨äºç¼ºå¤±å€¼</li>
      <li>Mæ­¥ï¼šæ ¹æ®éšå˜é‡çš„ä¼°è®¡å€¼ï¼Œç»™å‡ºå½“å‰çš„å‚æ•°çš„æå¤§ä¼¼ç„¶ä¼°è®¡</li>
    </ul>
  </li>
</ul>

<h4 id="k-means">K-means</h4>
<p>å…ˆéšæœºé€‰å®škä¸ªç‚¹ä½œä¸ºè´¨å¿ƒ $\mu_1, \mu_2, \cdots, \mu_ğ‘˜$ï¼š</p>
<ol>
  <li>
    <p>å›ºå®š$\mu_k$ï¼Œå°†æ ·æœ¬åˆ’åˆ†åˆ°è·ç¦»æœ€è¿‘çš„$\mu_k$æ‰€å±çš„ç°‡ä¸­</p>

    <script type="math/tex; mode=display">% <![CDATA[
r_{nk} = \left. \begin{cases} 1 \,\, & \text{if} \;\;\;k = \mathop{argmin}_j ||\mathbf{x}_n - \boldsymbol{\mu}_j||^2 \\
0 \,\, & \text{otherwise} \end{cases} \right. %]]></script>
  </li>
  <li>
    <p>å¯¹äºæ¯ä¸€ä¸ªæ•°æ®ç°‡ï¼Œé‡æ–°è®¡ç®—å…¶ä¸­å¿ƒï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–ç°‡ä¸­æ¯ä¸ªæ ·æœ¬ä¸ä¸­å¿ƒçš„è·ç¦»ï¼Œå¯è¡¨ç¤ºä¸º</p>

    <script type="math/tex; mode=display">J = \sum\limits_{n=1}^N r_{nk} ||\mathbf{x}_n - \boldsymbol{\mu}_k||^2</script>

    <p>è¦æ±‚å¾—æœ€å°åŒ–$J$çš„$\mu_k$</p>

    <p>å¯é€šè¿‡ $\frac{\partial J}{\partial\mathbf{\mu}<em>k}=2\sum\limits</em>{n=1}^N r_{nk}(\boldsymbol{x}<em>n - \boldsymbol{\mu}_k) = 0$ï¼Œ æ±‚å¾—$
\boldsymbol{\mu}_k = \frac{\sum_nr</em>{nk} \mathbf{x}<em>n}{\sum_n r</em>{nk}}$ï¼Œå³ç°‡ä¸­æ¯ä¸ªæ ·æœ¬çš„å‡å€¼å‘é‡ã€‚</p>
  </li>
</ol>

<h4 id="gaussian-mixture-model-gmm">Gaussian Mixture Model (GMM)</h4>
<blockquote>
  <p>æ··åˆæ¨¡å‹ (mixture model)ï¼šç”¨äºè¡¨ç¤ºæ€»ä½“ä¸­äºšç¾¤ä½“çš„å­˜åœ¨ï¼Œä½†ä¸éœ€è¦ç»™å®šæ¯ä¸ªindividual observationæ‰€å¤„çš„äºšç¾¤ä½“ã€‚</p>

</blockquote>

<p>é€šè¿‡å°†å¤šä¸ªåŸºç¡€åˆ†å¸ƒæ··åˆèµ·æ¥è¿‘ä¼¼æ•´ä½“åˆ†å¸ƒ (ç»„åˆæ–¹å¼ä¸»è¦æ˜¯çº¿æ€§ç»„åˆ), å…¶ä¸­ä¸åŒçš„ç»„ä»¶åˆ†å¸ƒå¤©ç„¶åœ°æ„æˆäº†å„è‡ªçš„ä¸€ç±»ï¼Œç»¼åˆèµ·æ¥çœ‹ï¼Œå³é€šè¿‡çº¿æ€§ç»„åˆåˆ†å¸ƒçš„æ–¹å¼æ„é€ æ¦‚ç‡æ¨¡å‹ï¼Œå¹¶é€šè¿‡æ¦‚ç‡ç”Ÿæˆè¿‡ç¨‹æ¥å­¦ä¹ å‚æ•°ï¼Œå¹¶æœ€ç»ˆé€šè¿‡æ•°æ®ç‚¹å±äºå“ªä¸€ä¸ªç»„ä»¶åˆ†å¸ƒæ¥å†³å®šèšç±»çš„æ ‡ç­¾ã€‚</p>

<h4 id="difference">difference</h4>
<ol>
  <li>K-means: hard assignment<br />
in each iteration, we are absolutely certain as to which cluster the point belongs to</li>
  <li>GMM: soft assignment
 It starts with some prior belief about how certain we are about each pointâ€™s cluster assignments. As it goes on, it revises those beliefs</li>
</ol>

<h4 id="reference">Reference</h4>
<ol>
  <li>https://www.quora.com/What-is-the-difference-between-K-means-and-the-mixture-model-of-Gaussian</li>
  <li>https://sites.northwestern.edu/msia/2016/12/08/k-means-shouldnt-be-our-only-choice/</li>
</ol>
:ET