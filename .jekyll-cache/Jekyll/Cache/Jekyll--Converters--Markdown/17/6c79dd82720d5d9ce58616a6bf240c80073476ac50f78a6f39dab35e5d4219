I"m<p><a href="https://www.aaai.org/Papers/AAAI/2007/AAAI07-262.pdf">AAAI 2007 paper</a> and <a href="http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf">JMLR 2012 paper</a></p>

<h4 id="mmd作用">MMD作用</h4>
<ul>
  <li>传统的用于衡量两个概率分布P和Q差别的方法，例如KL divergence，要求概率分布P和Q已知。也就是说，<strong>如果我们只有来自P和Q的样本，那么我们需要先进行概率密度估计 (density estimation)，然后才能衡量两个分布的差异</strong>。</li>
  <li>MMD：利用来自P和Q的样本，直接获得它们对应的总体概率分布的差异，而无需density estimation这一中间步骤。</li>
</ul>

<h4 id="mmd-metric">MMD metric</h4>
<ul>
  <li>Give observations $X := {x_1, \cdots , x_m}$ and $Y := {y_1, \cdots, y_n}$, which are independently and identically distributed (i.i.d.) from distribution $p$ and $q$. Let $\mathcal{F}$ be a class of functions $f : X \to \mathbb{R}$, and shorthand notation $E_x[ f(x)] :=E_{x \sim p}[ f(x)]$ and $E_y[ f(y)] := E_{y\sim q}[ f(y)]$ denote expectations with respect to $p$ and $q$, respectively, where $x \sim p$ indicates x has distribution $p$.</li>
  <li>
    <p>Maximum mean discrepancy (MMD) is defined as:</p>

    <script type="math/tex; mode=display">MMD[\mathcal{F}, p,q] := \sup_{f \in \mathcal{F}}(E_x[ f(x)]−E_y[ f(y)]) .</script>

    <p>We must therefore dentify a function class that is <strong>rich enough to uniquely identify whether $p = q$</strong>. And since we want to obtain this discrepancy by sample $X$ and $Y$,  the function class should also be <strong>restrictive enough to provide useful finite sample estimates</strong>.</p>
  </li>
  <li>
    <p>Propose the unit ball in a reproducing kernel Hilbert space $\mathcal H$ as our MMD function class $\mathcal{F}$. Define <strong>mean embedding</strong> $\mu_p(t) \in \mathcal{H}$ such that <script type="math/tex">E_x [f(x)] = \lt f, \mu_p \gt _{\mathcal{H}}</script> for all $f \in \mathcal{H}$. If we set $f= \phi (t) = k(t, \cdot)$, we obtain $\mu_p(t) = &lt;\mu_p, k(t, ·)&gt;_\mathcal{H} =E_xk(t, x)$, in other words, the mean embedding of the distribution $p$ is the expectation under $p$ of the canonical feature map.</p>
  </li>
  <li>We have<br />
 <script type="math/tex">% <![CDATA[
\begin{split}
 MMD^2 \left[ \mathcal{F}, p,q \right] &= \left[ \sup_{||f||_\mathcal{H} \leq 1}(E_x [ f(x)]−E_y [ f(y)])
\right]^2\\&=    \left[ \sup_{||f||_\mathcal{H} \leq 1}<\mu_p-\mu_q,f>_\mathcal{H}
\right]^2\\&=||\mu_p-\mu_q||_\mathcal{H}^2\\&=||E_x\phi(x)-E_y\phi(y)||_\mathcal{H}^2
 \end{split} %]]></script></li>
  <li>The MMD is a metric, when $\mathcal{H}$ is a universal RKHSs, defined on a compact metric space X. It can be proven that <strong>the Gaussian and Laplace RKHSs</strong> are universal.</li>
</ul>

<h4 id="finite-sample-estimate">finite sample estimate</h4>
<ol>
  <li></li>
</ol>

:ET