I"<p><a href="https://research.mapillary.com/img/publications/CVPR18b.pdf">Paper link</a> and <a href="https://github.com/mancinimassimiliano/latent_domains_DA">code</a></p>

<h4 id="take-away-message">Take away message</h4>

<h4 id="model">Model</h4>
<p><img src="/img/15854715937974.jpg" width="60%" height="60%" />
ç¬¬ä¸€ç¯‡é’ˆå¯¹classification taskçš„deep learning based multi-source domain adaptation. æœ¬è´¨ä¸Šæ˜¯multi-sourceçš„AdaBN.</p>

<p><img src="/img/15857203223787.jpg" alt="-w763" /></p>
<ol>
  <li>a side-output branch is empolyed to predict domain assignment probabilities for each input sample.</li>
  <li>mDA-layer to renormalize the multi-modal feature distributions</li>
</ol>

<h5 id="domain-discovery">domain discovery</h5>
<ol>
  <li>domain prediction branch: sample points at different mDA-layers corresponding to a single input element to the network should share the same probabilities</li>
  <li>split the network into a domain prediction branch and classification branch at some low level layer: features tend to become increasingly more domain invariant going deeper into the network, meaning that it becomes increasingly harder to compute a sampleâ€™s domain as a function of deeper features.</li>
</ol>

<h5 id="multi-source-domain-adaptation-layer">multi-source domain adaptation layer</h5>
<blockquote>
  <p>Domain Alignment layers: aim to reduce internal domain shift at different levels within the network by re-normalizing features in a domain-dependent way, matching their distributions to a pre-determined one.</p>
</blockquote>

<p><script type="math/tex">q_{d|x}(d | x_i)</script> is the conditional probability of $x_i$ belonging to $d$, given $x_i$</p>
<ol>
  <li>compute $\mu$ and $\sigma^2$<br />
 <img src="/img/15857223593758.jpg" width="40%" height="40%" /><br />
 <img src="/img/15857224509940.jpg" width="60%" height="40%" /></li>
  <li>normalize each sample<br />
By substituting $w_{i,d}$ for $q_{d|x}(d | x_i)$<br />
<img src="/img/15857225154947.jpg" width="60%" height="40%" /></li>
</ol>

<h5 id="overall-training-objective">overall training objective</h5>
<p>only use source dataset now
<img src="/img/15857216573544.jpg" alt="-w359" /></p>
<ol>
  <li>labeled dataçš„åˆ†ç±»loss (cross entropy)</li>
  <li>data with known domainçš„é¢„æµ‹loss (cross entropy)</li>
  <li>unlabeled dataçš„åˆ†ç±» (minimizing uncertainty)</li>
  <li>data without domain labelçš„é¢„æµ‹loss (minimizing uncertainty)</li>
</ol>

<h4 id="result">Result</h4>
<p><img src="/img/15857232316372.jpg" alt="-w372" />
<img src="/img/15857232406316.jpg" alt="-w368" />
åœ¨latent domainæ˜æ˜¾çš„æ—¶å€™(digitå’ŒPACS), modelè¡¨ç°æ›´å¥½ã€‚ä½†å¯¹äºoffice-31, å¯èƒ½å› ä¸ºå¾ˆéš¾ç›´æ¥å‘ç°latent domainï¼Œæ‰€ä»¥åŸºæœ¬æ²¡æœ‰æé«˜ã€‚</p>

<h4 id="insight">Insight</h4>
<ol>
  <li>multi-source DAå¯ä»¥åœ°æ–¹å¯ä»¥å€Ÿé‰´single-sourceçš„æ€æƒ³ï¼Œæ¯”å¦‚è¯´æœ¬æ–‡ç”¨çš„domain-alignment layer</li>
  <li>éš¾ç‚¹åœ¨äº å¥½çš„latent domain discovery</li>
</ol>

<h4 id="further-reference-for-latent-domain-discovery">Further reference (for latent domain discovery)</h4>
<ol>
  <li>J. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discovering latent domains for multisource domain adaptation. In ECCV, 2012.</li>
  <li>B. Gong, K. Grauman, and F. Sha. Reshaping visual datasets for domain adaptation. In NIPS, 2013.</li>
  <li>C. Xiong, S. McCloskey, S.-H. Hsieh, and J. J. Corso. Latent domains modeling for visual domain adaptation. In AAAI, 2014.</li>
</ol>

:ET