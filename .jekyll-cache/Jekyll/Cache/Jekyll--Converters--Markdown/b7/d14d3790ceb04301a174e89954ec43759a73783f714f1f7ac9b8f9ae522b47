I"½<h4 id="em-algorithm">EM algorithm</h4>
<ul>
  <li>EMç®—æ³•æ˜¯æœŸæœ›æœ€å¤§åŒ–(Expectation Maximization)ç®—æ³•çš„ç®€ç§°</li>
  <li>ç”¨äº<strong>å«æœ‰éšå˜é‡(hidden variable)</strong>çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹å‚æ•°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡</li>
  <li>EMç®—æ³•æ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ï¼Œæ¯æ¬¡è¿­ä»£ç”±ä¸¤æ­¥ç»„æˆï¼š
    <ul>
      <li>Eæ­¥ï¼šæ ¹æ®æ¨¡å‹å‚æ•°çš„å‡è®¾å€¼ï¼Œç»™å‡ºéšå˜é‡çš„æœŸæœ›ä¼°è®¡ï¼Œåº”ç”¨äºç¼ºå¤±å€¼</li>
      <li>Mæ­¥ï¼šæ ¹æ®éšå˜é‡çš„ä¼°è®¡å€¼ï¼Œç»™å‡ºå½“å‰çš„å‚æ•°çš„æå¤§ä¼¼ç„¶ä¼°è®¡</li>
    </ul>
  </li>
</ul>

<h4 id="k-means">K-means</h4>
<p>å…ˆéšæœºé€‰å®škä¸ªç‚¹ä½œä¸ºè´¨å¿ƒ $\mu_1, \mu_2, \cdots, \mu_ğ‘˜$ï¼š</p>
<ol>
  <li>
    <p>å›ºå®š$\mu_k$ï¼Œå°†æ ·æœ¬åˆ’åˆ†åˆ°è·ç¦»æœ€è¿‘çš„$\mu_k$æ‰€å±çš„ç°‡ä¸­</p>

    <script type="math/tex; mode=display">% <![CDATA[
r_{nk} = \left. \begin{cases} 1 \,\, & \text{if} \;\;\;k = \mathop{argmin}_j ||\mathbf{x}_n - \boldsymbol{\mu}_j||^2 \\
0 \,\, & \text{otherwise} \end{cases} \right. %]]></script>
  </li>
  <li>
    <p>å¯¹äºæ¯ä¸€ä¸ªæ•°æ®ç°‡ï¼Œé‡æ–°è®¡ç®—å…¶ä¸­å¿ƒï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–ç°‡ä¸­æ¯ä¸ªæ ·æœ¬ä¸ä¸­å¿ƒçš„è·ç¦»ï¼Œå¯è¡¨ç¤ºä¸º</p>

    <script type="math/tex; mode=display">J = \sum\limits_{n=1}^N r_{nk} ||\mathbf{x}_n - \boldsymbol{\mu}_k||^2.</script>

    <p>ä¸ºæ±‚å¾—æœ€å°åŒ– $J$ çš„ $\mu_k$ï¼Œå¯é€šè¿‡</p>

    <script type="math/tex; mode=display">\frac{\partial J}{\partial\mathbf{\mu}_k}=2\sum\limits_{n=1}^N r_{nk}(\boldsymbol{x}_n - \boldsymbol{\mu}_k) = 0,</script>

    <p>æ±‚å¾—</p>

    <script type="math/tex; mode=display">\boldsymbol{\mu}_k = \frac{\sum_nr_{nk} \mathbf{x}_n}{\sum_n r_{nk}},</script>

    <p>å³ç°‡ä¸­æ¯ä¸ªæ ·æœ¬çš„å‡å€¼å‘é‡ã€‚</p>
  </li>
</ol>

<h4 id="gaussian-mixture-model-gmm">Gaussian Mixture Model (GMM)</h4>
<ul>
  <li>æ··åˆæ¨¡å‹ (mixture model)ï¼šæ˜¯ä¸€ä¸ªå¯ä»¥ç”¨æ¥è¡¨ç¤ºåœ¨æ€»ä½“åˆ†å¸ƒä¸­å«æœ‰ K ä¸ªå­åˆ†å¸ƒçš„æ¦‚ç‡æ¨¡å‹ã€‚æ¢å¥è¯è¯´ï¼Œæ€»ä½“çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ˜¯ä¸€ä¸ªç”± K ä¸ªå­åˆ†å¸ƒç»„æˆçš„æ··åˆåˆ†å¸ƒã€‚<strong>æ··åˆæ¨¡å‹ä¸è¦æ±‚è§‚æµ‹æ•°æ®æä¾›å…¶å±äºå“ªä¸ªå­åˆ†å¸ƒ</strong>ã€‚</li>
  <li>é«˜æ–¯æ··åˆæ¨¡å‹ï¼šç”± K ä¸ªå•é«˜æ–¯æ¨¡å‹ç»„åˆè€Œæˆçš„æ¨¡å‹ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œä¸€ä¸ªæ··åˆæ¨¡å‹å¯ä»¥ä½¿ç”¨ä»»ä½•æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™é‡Œä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹æ˜¯å› ä¸ºé«˜æ–¯åˆ†å¸ƒå…·å¤‡å¾ˆå¥½çš„æ•°å­¦æ€§è´¨ä»¥åŠè‰¯å¥½çš„è®¡ç®—æ€§èƒ½ã€‚
    <ul>
      <li></li>
    </ul>
  </li>
</ul>

<h4 id="difference">difference</h4>
<ol>
  <li>K-means: hard assignment<br />
in each iteration, we are absolutely certain as to which cluster the point belongs to</li>
  <li>GMM: soft assignment
 It starts with some prior belief about how certain we are about each pointâ€™s cluster assignments. As it goes on, it revises those beliefs</li>
</ol>

<h4 id="reference">Reference</h4>
<ol>
  <li>https://www.quora.com/What-is-the-difference-between-K-means-and-the-mixture-model-of-Gaussian</li>
  <li>https://sites.northwestern.edu/msia/2016/12/08/k-means-shouldnt-be-our-only-choice/</li>
  <li>https://zhuanlan.zhihu.com/p/30483076</li>
</ol>
:ET