I"<p><a href="https://arxiv.org/pdf/1902.06162.pdf">Paper link</a></p>

<h4 id="self-supervised-feature-learning">self-supervised feature learning</h4>
<ul>
  <li>learn visual features from large-scale unlabeled images or videos without using any human annotations</li>
  <li>a subset of unsupervised learning methods</li>
  <li>pretext task: the supervisory signal is generated from the data itself by leveraging its structure</li>
  <li>visual features of images or videos need to be captured by ConvNets to solve the pretext tasks</li>
</ul>

<h4 id="作用">作用</h4>
<ol>
  <li>作为pretext task获得pre-trained model
    <ul>
      <li>提供一个好的起始点，加速收敛</li>
      <li>已经学习到hierarchy features，即使downstream task的数据集很小，也不会过拟合太严重</li>
    </ul>
  </li>
  <li>作为auxiliary task来添加regularization</li>
</ol>

<h4 id="分类">分类</h4>
<p><img src="/img/15932024508723.jpg" alt="-w1112" /></p>
<ol>
  <li>Information recovery: 先抹除图片的一部分信息，然后让网络学习恢复这些信息
    <ul>
      <li>generation based:
        <ul>
          <li><strong>image generation</strong>: help the network to capture the real distribution of the real data and generate realists data</li>
          <li><strong>color recovery</strong>: network need to recognize objects and to group pixels of the same part together</li>
          <li><strong>inpainting</strong>: networks are required to learn the common knowledge including the color and structure of the common objects</li>
          <li><strong>super resolution</strong>: learn the semantic features of images</li>
        </ul>
      </li>
      <li>context based:
        <ul>
          <li><strong>Context Similarity (contrasting)</strong>: learn the invariance within one class and the variance among different classes
            <ul>
              <li>contrasting: train networks to maximum agreement of different views of same scene while minimizing agreement of views from different scenes</li>
            </ul>
          </li>
          <li><strong>Spatial Context Structure</strong>：learn spatial context information such as the shape of the objects and the relative positions of different parts of an object</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Hard-code program
    <ul>
      <li>This type of methods generally has two steps:
        <ul>
          <li>label generation by employing hard-code programs on images or videos to obtain labels,</li>
          <li>train ConvNets with the generated labels.</li>
        </ul>
      </li>
      <li>distill knowledge from hard-code detector</li>
      <li>one drawback is that the semantic labels generated by hard-code detector usually are very noisy which need to specifically cope with.</li>
    </ul>
  </li>
  <li>Game Engines
    <ul>
      <li>game engines are able to render realistic images and provide accurate pixel-level labels</li>
      <li>one problem is the domain gap between synthetic and real-world images</li>
    </ul>
  </li>
</ol>

<h4 id="performance">Performance</h4>
<p><img src="/img/15932060535480.jpg" alt="-w913" />
The performance of self-supervised methods are comparable to the supervised methods on some downstream tasks, especially for the object detection and semantic segmentation tasks.</p>

:ET