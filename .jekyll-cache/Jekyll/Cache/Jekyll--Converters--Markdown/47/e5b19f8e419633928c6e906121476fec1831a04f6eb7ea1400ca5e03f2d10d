I"Ú<p><a href="https://arxiv.org/pdf/1706.05587.pdf">Paper link</a></p>

<h4 id="take-away-message">Take away message</h4>
<ol>
  <li>modules employing atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates.</li>
  <li>‰∏ªË¶ÅËøòÊòØÂêÑÁßçtricks</li>
</ol>

<h4 id="model">Model</h4>
<p>output_stride: the ratio of input image spatial resolution to final output resolution.</p>
<h5 id="atrous-convolution-in-cascade">atrous convolution in cascade</h5>
<p><img src="/img/15893990006461.jpg" alt="-w840" /></p>
<ol>
  <li>duplicate several copies of the last ResNet block (Block 5, Block 6, Block 7)</li>
  <li>apply atrous convolution with rates determined by the desired output stride value (output stride = 256 if no atrous convolution is applied)</li>
  <li>multi-grid method: adopt different atrous rates within block4 to block7 in the proposed model. For example, when $\text{output stride} = 16$ and $\text{Multi Grid} = (1, 2, 4)$, the three convolutions will have $rates = 2 ¬∑ (1, 2, 4) = (2, 4, 8)$ in the block4, respectively.</li>
</ol>

<h5 id="atrous-convolution-in-parallel">atrous convolution in parallel</h5>
<p><img src="/img/15893997101137.jpg" alt="-w778" /></p>
<ol>
  <li>include batch normalization within ASPP.</li>
  <li>image level feature: apply global average pooling on the last feature map of the model, feed the resulting image-level features to a 1 √ó 1 convolution with 256 filters (and batch normalization), and then bilinearly upsample the feature to the desired spatial dimension.</li>
  <li>the improved ASPP consists of
    <ul>
      <li>one 1√ó1 convolution, and three 3 √ó 3 convolutions with rates = (6, 12, 18) when output stride = 16 (all with 256 filters and batch normalization). Note that the rates are doubled when output stride = 8.</li>
      <li>the image-level features (the rates are doubled when output stride = 8)</li>
    </ul>
  </li>
  <li>The resulting features from all the branches are concatenated and pass through another 1 √ó 1 convolution (with 256 filters and batch normalization) before the final 1 √ó 1 convolution which generates the final logits.</li>
</ol>

<h4 id="tricks">Tricks</h4>
<ol>
  <li>‚Äúpoly‚Äù learning rate policy: the initial learning rate is
multiplied by $(1 ‚àí\text{maxiter})^{\text{power}}$ with $\text{power} = 0.9 $.</li>
  <li><strong>upsampling logits: upsample the final logits, since downsampling the groundtruths removes the fine annotations resulting in no back-propagation of details.</strong></li>
  <li>$\text{output stride}=16$ during training, and $\text{output stride}=8$ during inference to get more detailed feature map.</li>
</ol>

<h4 id="result">Result</h4>
<ol>
  <li>cascade:
<img src="/img/15894010210339.jpg" width="60%" height="100%" /></li>
  <li>parallel:
<img src="/img/15894010431381.jpg" width="60%" height="100%" /></li>
</ol>

<ul>
  <li>Both their best cascaded model (in Tab. 4) and ASPP model (in Tab. 6) (in both cases without DenseCRF post-processing or MS-COCO pre-training) already outperform DeepLabv2 (77.69% with DenseCRF and pretrained on MS-COCO) on the PASCAL VOC 2012 val set.</li>
  <li>The improvement mainly comes from including and fine-tuning batch normalization parameters in the proposed models and having a better way to encode multi-scale context.</li>
</ul>

<h4 id="further-reference">Further reference</h4>
<ol>
  <li>S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167, 2015.</li>
</ol>
:ET