I"—<h4 id="em-algorithm">EM algorithm</h4>
<ul>
  <li>EMç®—æ³•æ˜¯æœŸæœ›æœ€å¤§åŒ–(Expectation Maximization)ç®—æ³•çš„ç®€ç§°</li>
  <li>ç”¨äº<strong>å«æœ‰éšå˜é‡(hidden variable)</strong>çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹å‚æ•°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡</li>
  <li>EMç®—æ³•æ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ï¼Œæ¯æ¬¡è¿­ä»£ç”±ä¸¤æ­¥ç»„æˆï¼š
    <ul>
      <li>Eæ­¥ï¼šæ ¹æ®æ¨¡å‹å‚æ•°çš„å‡è®¾å€¼ï¼Œç»™å‡ºéšå˜é‡çš„æœŸæœ›ä¼°è®¡ï¼Œåº”ç”¨äºç¼ºå¤±å€¼</li>
      <li>Mæ­¥ï¼šæ ¹æ®éšå˜é‡çš„ä¼°è®¡å€¼ï¼Œç»™å‡ºå½“å‰çš„å‚æ•°çš„æå¤§ä¼¼ç„¶ä¼°è®¡</li>
    </ul>
  </li>
</ul>

<h4 id="k-means">K-means</h4>
<ol>
  <li>å…ˆéšæœºé€‰å®škä¸ªç‚¹ä½œä¸ºè´¨å¿ƒ $\mu_1, \mu_2, \cdots, \mu_ğ‘˜$</li>
  <li>
    <p>E step: å›ºå®š$\mu_k$ï¼Œå°†æ ·æœ¬åˆ’åˆ†åˆ°è·ç¦»æœ€è¿‘çš„$\mu_k$æ‰€å±çš„ç°‡ä¸­</p>

    <script type="math/tex; mode=display">% <![CDATA[
r_{nk} = \left. \begin{cases} 1 \,\, & \text{if} \;\;\;k = \mathop{argmin}_j ||\mathbf{x}_n - \boldsymbol{\mu}_j||^2 \\
0 \,\, & \text{otherwise} \end{cases} \right. %]]></script>
  </li>
  <li>
    <p>M step: å¯¹äºæ¯ä¸€ä¸ªæ•°æ®ç°‡ï¼Œé‡æ–°è®¡ç®—å…¶ä¸­å¿ƒï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–ç°‡ä¸­æ¯ä¸ªæ ·æœ¬ä¸ä¸­å¿ƒçš„è·ç¦»ï¼Œå¯è¡¨ç¤ºä¸º</p>

    <script type="math/tex; mode=display">J = \sum\limits_{n=1}^N r_{nk} ||\mathbf{x}_n - \boldsymbol{\mu}_k||^2.</script>

    <p>ä¸ºæ±‚å¾—æœ€å°åŒ– $J$ çš„ $\mu_k$ï¼Œå¯é€šè¿‡</p>

    <script type="math/tex; mode=display">\frac{\partial J}{\partial\mathbf{\mu}_k}=2\sum\limits_{n=1}^N r_{nk}(\boldsymbol{x}_n - \boldsymbol{\mu}_k) = 0,</script>

    <p>æ±‚å¾—</p>

    <script type="math/tex; mode=display">\boldsymbol{\mu}_k = \frac{\sum_nr_{nk} \mathbf{x}_n}{\sum_n r_{nk}},</script>

    <p>å³ç°‡ä¸­æ¯ä¸ªæ ·æœ¬çš„å‡å€¼å‘é‡ã€‚</p>
  </li>
  <li>é‡å¤2-3</li>
</ol>

<h4 id="gaussian-mixture-model-gmm">Gaussian Mixture Model (GMM)</h4>
<ul>
  <li>æ··åˆæ¨¡å‹ (mixture model)ï¼šæ˜¯ä¸€ä¸ªå¯ä»¥ç”¨æ¥è¡¨ç¤ºåœ¨æ€»ä½“åˆ†å¸ƒä¸­å«æœ‰ K ä¸ªå­åˆ†å¸ƒçš„æ¦‚ç‡æ¨¡å‹ã€‚æ¢å¥è¯è¯´ï¼Œæ€»ä½“çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ˜¯ä¸€ä¸ªç”± K ä¸ªå­åˆ†å¸ƒç»„æˆçš„æ··åˆåˆ†å¸ƒã€‚
    <ul>
      <li>æ··åˆæ¨¡å‹ä¸è¦æ±‚è§‚æµ‹æ•°æ®æä¾›å…¶å±äºå“ªä¸ªå­åˆ†å¸ƒ =&gt; hiddle varibale</li>
      <li><strong>å¯¹äºæ··åˆæ¨¡å‹æ¥è¯´ï¼Œæ¯ä¸ªå­åˆ†å¸ƒå¤©ç„¶åœ°æ„æˆäº†å„è‡ªçš„ä¸€ç±»</strong></li>
    </ul>
  </li>
  <li>é«˜æ–¯æ··åˆæ¨¡å‹ï¼šç”± K ä¸ªå•é«˜æ–¯æ¨¡å‹ç»„åˆè€Œæˆçš„æ¨¡å‹ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œä¸€ä¸ªæ··åˆæ¨¡å‹å¯ä»¥ä½¿ç”¨ä»»ä½•æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™é‡Œä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹æ˜¯å› ä¸ºé«˜æ–¯åˆ†å¸ƒå…·å¤‡å¾ˆå¥½çš„æ•°å­¦æ€§è´¨ä»¥åŠè‰¯å¥½çš„è®¡ç®—æ€§èƒ½ã€‚
    <ul>
      <li>
        <p>é«˜æ–¯æ··åˆæ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒä¸ºï¼š</p>

        <script type="math/tex; mode=display">P(x|\theta) = \sum_{k=1}^{K}{\alpha_{k}\phi(x|\theta_{k})}</script>
      </li>
      <li>å¯¹äºè¿™ä¸ªæ¨¡å‹è€Œè¨€ï¼Œå‚æ•° $\theta = (\tilde{\mu_{k}}, \tilde{\sigma_{k}}, \tilde{\alpha_{k}})$ ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªå­æ¨¡å‹çš„æœŸæœ›ã€æ–¹å·®ï¼ˆæˆ–åæ–¹å·®ï¼‰ã€åœ¨æ··åˆæ¨¡å‹ä¸­å‘ç”Ÿçš„æ¦‚ç‡</li>
      <li>å¦‚æœé€šè¿‡ä½¿ç”¨è¶³å¤Ÿå¤šçš„é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶ä¸”è°ƒèŠ‚å®ƒä»¬çš„å‡å€¼å’Œæ–¹å·®ä»¥åŠçº¿æ€§ç»„åˆçš„ç³»æ•°ï¼Œé‚£ä¹ˆå‡ ä¹æ‰€æœ‰çš„è¿ç»­æ¦‚ç‡å¯†åº¦éƒ½èƒ½å¤Ÿä»¥ä»»æ„çš„ç²¾åº¦è¿‘ä¼¼ã€‚</li>
    </ul>
  </li>
</ul>

<ol>
  <li>éšæœºåˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼ˆå„ä¸ªé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ã€æ–¹å·®ã€å‘ç”Ÿæ¦‚ç‡ï¼‰</li>
  <li>
    <p>E step: ä¾æ®å½“å‰å‚æ•°ï¼Œè®¡ç®—æ¯ä¸ªæ•°æ® $j$ æ¥è‡ªå­æ¨¡å‹ $k$ çš„å¯èƒ½æ€§</p>

    <script type="math/tex; mode=display">\gamma_{jk} = \frac{\alpha_{k}\phi(x_{j}|\theta_{k})}{\sum_{k=1}^{K}{\alpha_{k}\phi(x_{j}|\theta_{k})}}, j = 1,2,...,N; k = 1,2,...,K</script>
  </li>
  <li>
    <p>M step: è®¡ç®—æ–°çš„æ¨¡å‹å‚æ•°</p>

    <script type="math/tex; mode=display">\mu_{k} = \frac{\sum_{j=1}^{N}{(\gamma_{jk}}x_{j})}{\sum_{j=1}^{N}{\gamma_{jk}}}, k=1,2,...,K</script>

    <script type="math/tex; mode=display">\Sigma_{k} = \frac{\sum_{j=1}^{N}{\gamma_{jk}}(x_{j}-\mu_{k})(x_{j}-\mu_{k})^{T}}{\sum_{j=1}^{N}{\gamma_{jk}}}, k = 1,2,...,K</script>

    <script type="math/tex; mode=display">\alpha_{k} = \frac{\sum_{j=1}^{N}{\gamma_{jk}}}{N}, k=1,2,...,K</script>
  </li>
  <li>é‡å¤è®¡ç®— E-step å’Œ M-step ç›´è‡³æ”¶æ•›</li>
</ol>

<h4 id="difference">difference</h4>
<ol>
  <li><strong>K-means: hard assignment</strong>
in each iteration, we are absolutely certain as to which cluster the point belongs to</li>
  <li><strong>GMM: soft assignment</strong>
 It starts with some prior belief about how certain we are about each pointâ€™s cluster assignments. As it goes on, it revises those beliefs</li>
</ol>

<h4 id="reference">Reference</h4>
<ol>
  <li><a href="https://www.quora.com/What-is-the-difference-between-K-means-and-the-mixture-model-of-Gaussian">difference of k-means and GMM</a></li>
  <li><a href="https://sites.northwestern.edu/msia/2016/12/08/k-means-shouldnt-be-our-only-choice/">k-means shouldnâ€™t be our only choice</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/30483076">EM algorithm, K-means, and GMM 1</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/75554749">EM algorithm, K-means, and GMM 2</a></li>
</ol>
:ET