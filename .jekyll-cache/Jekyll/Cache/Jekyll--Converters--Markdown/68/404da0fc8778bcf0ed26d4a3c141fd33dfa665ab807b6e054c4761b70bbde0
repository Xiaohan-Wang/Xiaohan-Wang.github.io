I"<h4 id="核方法kernel-method">核方法（kernel method）</h4>
<ul>
  <li>理论基础: Cover’s theorem，其指出，在低维空间中线性不可分的数据，通过非线性变换将其投影到高维空间之后，大概率会变为线性可分的数据</li>
  <li><strong>将低维空间的非线性可分问题，转化为高维空间的线性可分问题</strong></li>
</ul>

<h4 id="核技巧kernel-trick">核技巧（kernel trick）</h4>
<p>设$\phi(x)$为映射函数，$\phi(x): \mathcal{X} \to \mathcal{H}$，其中$\mathcal{X}$为输入空间，$\mathcal{H}$为特征空间 (特征空间需要是Hilbert space，即完备的内积空间)。</p>

<p>欲求$&lt;\phi(x_1), \phi(x_2)&gt;$：</p>
<ul>
  <li>传统方法：先分别计算$\phi(x_1)$和$\phi(x_2)$，再在特征空间中计算二者的内积
    <ul>
      <li>缺点：当特征空间维度很大时，计算非常复杂</li>
    </ul>
  </li>
  <li>核技巧：在输入空间找到一个函数$K(x_1, x_2)$，使得$K(x_1, x_2)=&lt;\phi(x_1), \phi(x_2)&gt;$，从而可以直接在低维空间中计算出结果，加速核方法计算。对应的函数 $K$ 就是核函数</li>
</ul>

<h4 id="核函数kernels">核函数（kernels）</h4>
<ul>
  <li>在实际应用时，映射函数 $\phi(x)$ 不需要是已知的。换句话说，核技巧的目的就是在不需要显式地定义特征空间和映射函数的条件下，计算映射之后的内积</li>
  <li>核函数 $K$ 本质上需要满足的条件 (不需要$\phi(x)$已知):
    <ul>
      <li>对称性: $K(\mathrm{x_1},\mathrm{x_2}) = K(\mathrm{x_2},\mathrm{x_1})$</li>
      <li>半正定性: 对于任意 $n$ 和任意 $x_1, x_2, \cdots, x_n  \in \mathcal{X}$，由 $K(x_i, x_j)$ 定义的 Gram matrix 总是半正定的</li>
    </ul>
  </li>
  <li>只要 $K$ 是核函数，那么一定存在一个Hilbert space和一个映射函数$\phi$，使得$K(x_1, x_2)=&lt;\phi(x_1), \phi(x_2)&gt;$</li>
  <li>常见核函数<br />
  <img src="/img/15923213053700.jpg" width="90%" height="100%" /><br />
  其它变换得到的核函数：参考资料3 $\text{P}_{17, 18}$</li>
</ul>

<h4 id="再生核希尔伯特空间reproducing-kernel-hilbert-spacesrkhs">再生核希尔伯特空间（reproducing kernel Hilbert spaces，RKHS）</h4>
<ul>
  <li>再生核希尔伯特空间
    <ul>
      <li>$\displaystyle \mathcal{H}$$\displaystyle f:X\rightarrow R$</li>
      <li>RKHS是一个函数空间，其中的元素 $f$ 为函数。通常情况下特征空间 $\mathcal{H}$ 为 RKHS。</li>
    </ul>
  </li>
  <li>再生核 $K$ 是核函数的一种，其满足
    <ul>
      <li>对于任意固定的 $x_0\in\mathcal{X}$，$K(x,x_0)$作为 $x$ 的函数属于我们的函数空间$\mathcal{H}$</li>
      <li>对于任意 $x\in\mathcal{X}$ 和 $f(\cdot)\in\mathcal{H}$，有$f(x) = \langle f(\cdot),K(\cdot,x)\rangle$ (再生性质 / reproducing property)</li>
    </ul>
  </li>
  <li>
    <p>对于再生核 $K$，我们可以自然的定义映射函数 $\phi(x)=K(\cdot, x)$，此时，通过再生核的再生性质，可知
<script type="math/tex">\langle \phi(x_1),\phi(x_2)\rangle = \langle K(\cdot,x_1),K(\cdot,x_2)\rangle = K(x_1,x_2)</script></p>
  </li>
  <li></li>
  <li>不是所有的Hilbert space都具有reproducing kernel</li>
  <li></li>
  <li>对于任意一个核函数 $K$，都存在多个对应的特征空间 $\mathcal{H}<em>0$，在这之中，RKHS是最为“精简”的一个，这里的精简体现在无论在 $\mathcal{H}_0$ 中得到怎样的分类模型$⟨w,\phi_0(x)⟩</em>{\mathcal{H}_0}$，在RKHS中都存在一个$f$可以得到和它相同的效果，因此对于某个核函数，RKHS代表了它最本征的信息</li>
</ul>

<h4 id="参考资料">参考资料</h4>
<ol>
  <li>https://zhuanlan.zhihu.com/p/61794781</li>
  <li>http://www.fanyeong.com/2017/11/13/the-kernel-trick/</li>
  <li><a href="https://www.stat.berkeley.edu/~bartlett/courses/2014spring-cs281bstat241b/lectures/20-notes.pdf">CS281B/Stat241B. Statistical Learning Theory. Lecture
20.</a></li>
  <li>https://cosx.org/2014/05/svm-series-add-2-kernel-ii/</li>
</ol>
:ET