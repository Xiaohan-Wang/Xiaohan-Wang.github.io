I"~
<h4 id="warm-up">Warm up</h4>

<h4 id="learning-rate-decay">Learning rate decay</h4>
<p>学习率衰减的基本思想是学习率随着训练的进行逐渐衰减，即在开始的时候使用较大的学习率，加快靠近最小值的速度，在后来些时候用较小的学习率，提高稳定性，避免因学习率太大跳过最小值，保证能够收敛到最小值。</p>

<ol>
  <li>
    <p>step(固定步长衰减): 每隔step_size个epoch后，lr衰减为原来的gamma倍</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1"># pytorch
</span>    
 <span class="n">CLASS</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>

    <p><img src="/img/15906775729941.jpg" width="50%" height="100%" /></p>
  </li>
  <li>
    <p>multistep(多步长衰减): 动态步长控制，lr衰减为原来的gamma倍</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># pytorch
</span>    
  <span class="n">CLASS</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>

    <p><img src="/img/15906776874073.jpg" width="50%" height="100%" /></p>
  </li>
  <li>
    <p>poly: 以多项式曲线衰减学习率, $lr = \text{base_lr} * (1 - \frac{T_{cur}}{T_{max}}) ^ {power} $
 <img src="/img/15906784236558.jpg" width="50%" height="100%" />
 学习率曲线的形状主要由参数 power 的值来控制。当 power = 1 的时候，学习率曲线为一条直线。当 power &lt; 1 的时候，下降速率由慢到快。当 power &gt; 1 的时候，下降速率由快到慢。</p>
  </li>
  <li>
    <p>cosine：以cosine曲线衰减学习率，$lr = \frac{1}{2}*\text{base_lr} * \left(1+ \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)$
<img src="/img/15906782692333.jpg" width="50%" height="100%" />
余弦值首先缓慢下降，然后加速下降，之后再次缓慢下降。</p>
  </li>
</ol>

<h4 id="参考资料">参考资料</h4>
<ol>
  <li>https://lumingdong.cn/setting-strategy-of-gradient-descent-learning-rate.html</li>
  <li>https://zhuanlan.zhihu.com/p/39565465</li>
</ol>

:ET