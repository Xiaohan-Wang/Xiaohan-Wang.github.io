I"Ÿ<p><a href="https://arxiv.org/pdf/1412.7062.pdf">Paper link</a></p>

<h4 id="take-away-message">Take away message</h4>
<ol>
  <li>two technical hurdles in the application of DCNNs to image labeling tasks: signal downsampling, and spatial â€˜insensitivityâ€™ (invariance).</li>
  <li>signal downsampling: reduction of signal resolution incurred by the repeated max-pooling -&gt; employ the atrous algorithm (å°±æ˜¯dilated convolution)</li>
  <li>spatial insensitivity: object-centric decisions from a classifier requires invariance to spatial transformations, inherently limiting the spatial accuracy of the DCNN model -&gt; capture fine details by employing a fully-connected Conditional Random Field (CRF) + multi-scale prediction</li>
</ol>

<h4 id="model">Model</h4>
<ol>
  <li>ä¿®æ”¹è‡ªVGG-16</li>
  <li>decouple the DCNN and CRF training stages, assuming the DCNN unary terms are fixed when setting the CRF parameters</li>
</ol>

<p><img src="/img/15888811976149.jpg" alt="-w724" /></p>

<h5 id="efficient-feature-extraction-atrous-algorithm--reducing-model">efficient feature extraction: atrous algorithm + reducing model</h5>
<ol>
  <li>To compute scores more densely at target stride of 8 pixels:<br />
 <img src="/img/15888799732428.jpg" width="70%" height="100%" />
    <ol>
      <li>convert the fully-connected layers of VGG-16 into convolutional ones and run the network in a convolutional fashion on the image at its original resolution</li>
      <li>instead of subsampling after the last two max-pooling layers in the network, use the convolutional filters with an input stride of 2 or 4 pixels, respectively.</li>
    </ol>
  </li>
  <li>After converting the network to a fully convolutional one, the first fully connected layer has 4,096 filters of large 7Ã—7 spatial size and becomes the computational bottleneck.
    <ol>
      <li>Use filters with 4Ã—4 (or 3Ã—3) spatial size but with the same receptive fields.</li>
      <li>reducing the number of channels at the fully connected layers from 4,096 down to 1,024.</li>
    </ol>
  </li>
</ol>

<h5 id="detail-boundary-recovery-fully-connected-crf--multi-scale-prediction">detail boundary recovery: fully-connected CRF + multi-scale prediction</h5>
<ol>
  <li>
    <p>fully-connected CRF 
 <img src="/img/15888811265917.jpg" alt="-w723" /></p>

    <ol>
      <li>Energy function:<br />
 <img src="/img/15888817215422.jpg" width="40%" height="100%" /></li>
      <li>first term:<br />
 <img src="/img/15888818741237.jpg" width="25%" height="100%" /></li>
      <li>second term:<br />
 <img src="/img/15888819146749.jpg" width="55%" height="100%" /><br />
 <img src="/img/15888820310003.jpg" width="25%" height="100%" /><br />
 the kernels are:<br />
 <img src="/img/15888821802918.jpg" width="55%" height="100%" /><br />
 The first kernel forces pixels with similar color and position to have similar labels, while the second kernel only considers spatial proximity when enforcing smoothness.</li>
    </ol>
  </li>
  <li>
    <p>multi-scale prediction
 attach to the input image and the output of each of the first four max pooling layers a two-layer MLP (first layer: 128 3x3 convolutional filters, second layer: 128 1x1 convolutional fil- ters) whose feature map is concatenated to the main networkâ€™s last layer feature map. The aggregate feature map fed into the softmax layer is thus enhanced by 5 * 128 = 640 channels.</p>
  </li>
</ol>

<h4 id="result">Result</h4>
<p><img src="/img/15888828891435.jpg" alt="-w794" />
<img src="/img/15888829216375.jpg" alt="-w739" />
<img src="/img/15888829687129.jpg" alt="-w702" />
<img src="/img/15888830739237.jpg" alt="-w725" /></p>

<h4 id="further-reference">Further reference</h4>
<ol>
  <li><a href="http://hellodfan.com/2018/01/22/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E8%AE%BA%E6%96%87-DeepLab%E7%B3%BB%E5%88%97/">éƒ¨åˆ†ç¿»è¯‘</a></li>
  <li>Krahenbuhl, P. and Koltun, V. Efficient inference in fully connected crfs with gaussian edge poten- tials. In NIPS, 2011.</li>
</ol>
:ET