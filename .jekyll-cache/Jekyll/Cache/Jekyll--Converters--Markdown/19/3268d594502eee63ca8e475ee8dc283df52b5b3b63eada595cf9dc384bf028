I"Ú<p><a href="https://arxiv.org/pdf/1611.06612.pdf">Paper link</a></p>

<h4 id="take-away-message">Take away message</h4>
<ol>
  <li>è®¾è®¡RefineNetï¼Œç»“åˆé«˜å±‚è¯­ä¹‰ä¿¡æ¯å’Œä½å±‚ç»†èŠ‚ä¿¡æ¯ï¼Œå¾—åˆ°é«˜åˆ†è¾¨ç‡çš„åˆ†å‰²å›¾</li>
  <li>RefineNetä¸­å¤§é‡ä½¿ç”¨äº†ResNetä¸­Identity mapping (both short range and long range) çš„æ€æƒ³ï¼Œä¿è¯äº†æœ‰æ•ˆçš„ç«¯åˆ°ç«¯è®­ç»ƒ</li>
</ol>

<h4 id="introduction">Introduction</h4>
<p>ç½‘ç»œä¸­poolingæ“ä½œä¼šé™ä½åˆ†å‰²å›¾çš„åˆ†è¾¨ç‡ï¼Œç›®å‰æœ‰ä¸‰ç§è§£å†³æ–¹å¼ï¼š</p>
<ol>
  <li>learn deconvolutional filters as an up-sampling operation: The deconvolution operations are not able to recover the low-level visual features which are lost after the downsampling operation in the convolution forward stage.</li>
  <li>Deeplabç³»åˆ—ä¸­çš„atrous convolution: <strong>a significant cost in memory, because unlike the image subsampling methods, one must retain very large numbers of feature maps at higher resolution.</strong> In practice, therefore, dilation convolution methods usually have a resolution prediction of no more than 1/8 size of the original rather than 1/4, when using a deep network.</li>
  <li>exploits features from intermediate layers for generating high-resolution prediction (æœ¬æ–‡åŸºäºçš„æ–¹æ³•)</li>
</ol>

<h4 id="model">Model</h4>
<p><img src="/img/15890589297975.jpg" width="70%" height="100%" />
<img src="/img/15890589494152.jpg" width="100%" height="100%" /></p>

<h4 id="result">Result</h4>
<p><img src="/img/15890590750133.jpg" width="50%" height="100%" /></p>

<p><img src="/img/15890590946983.jpg" width="50%" height="100%" /></p>

<p><img src="/img/15890591146152.jpg" width="100%" height="100%" /></p>

<h4 id="reflection">Reflection</h4>
<p>æ„Ÿè§‰æ•´ä¸ªç½‘ç»œè®¾è®¡å¹¶æ²¡æœ‰å¤ªæ–°é¢–çš„æ¨¡å—æˆ–æ€è·¯ï¼Œä½†å¯èƒ½æ˜¯é€šè¿‡å¤§é‡çš„identity mappingè¾¾æˆäº†æœ‰æ•ˆçš„ç«¯å¯¹ç«¯è®­ç»ƒï¼Œæœ€åå¾—åˆ°äº†éå¸¸æ£’çš„ç»“æœã€‚å¦‚æœæ˜¯çš„è¯ï¼Œè¯´æ˜è®¾è®¡ç½‘ç»œæ—¶ï¼Œè¿˜æ˜¯è¦å…³æ³¨ç½‘ç»œå„ä¸ªæ¨¡å—èƒ½å¦å¾—åˆ°å……åˆ†çš„è®­ç»ƒã€‚</p>
:ET