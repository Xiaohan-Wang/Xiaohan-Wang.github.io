I"l<p><a href="https://www.aaai.org/Papers/AAAI/2007/AAAI07-262.pdf">AAAI 2007 paper</a> and <a href="http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf">JMLR 2012 paper</a></p>

<h4 id="summary">Summary</h4>
<ul>
  <li>ä¼ ç»Ÿçš„ç”¨äºè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒPå’ŒQå·®åˆ«çš„æ–¹æ³•ï¼Œä¾‹å¦‚KL divergenceï¼Œè¦æ±‚æ¦‚ç‡åˆ†å¸ƒPå’ŒQå·²çŸ¥ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ<strong>å¦‚æœæˆ‘ä»¬åªæœ‰æ¥è‡ªPå’ŒQçš„æ ·æœ¬ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦å…ˆè¿›è¡Œæ¦‚ç‡å¯†åº¦ä¼°è®¡ (density estimation)ï¼Œç„¶åæ‰èƒ½è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚</strong>ã€‚</li>
  <li>MMDï¼šåˆ©ç”¨æ¥è‡ªPå’ŒQçš„æ ·æœ¬ï¼Œç›´æ¥è·å¾—å®ƒä»¬å¯¹åº”çš„æ€»ä½“æ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ï¼Œè€Œæ— éœ€density estimationè¿™ä¸€ä¸­é—´æ­¥éª¤ã€‚</li>
</ul>

<h4 id="mmd-metric">MMD metric</h4>
<ul>
  <li>Give observations $X := {x_1, \cdots , x_m}$ and $Y := {y_1, \cdots, y_n}$, which are independently and identically distributed (i.i.d.) from distribution $p$ and $q$. Let $\mathcal{F}$ be a class of functions $f : X \to \mathbb{R}$, and shorthand notation $E_x[ f(x)] :=E_{x \sim p}[ f(x)]$ and $E_y[ f(y)] := E_{y\sim q}[ f(y)]$ denote expectations with respect to $p$ and $q$, respectively, where $x \sim p$ indicates x has distribution $p$.</li>
  <li>
    <p>Maximum mean discrepancy (MMD) is defined as:</p>

    <script type="math/tex; mode=display">MMD[\mathcal{F}, p,q] := \sup_{f \in \mathcal{F}}(E_x[ f(x)]âˆ’E_y[ f(y)]) .</script>

    <p>We must therefore dentify a function class that is <strong>rich enough to uniquely identify whether $p = q$</strong>. And since we want to obtain this discrepancy by sample $X$ and $Y$,  the function class should also be <strong>restrictive enough to provide useful finite sample estimates</strong>.</p>
  </li>
  <li>
    <p>Propose the unit ball in a reproducing kernel Hilbert space $\mathcal H$ as our MMD function class $\mathcal{F}$. Define <strong>mean embedding</strong> $\mu_p(t) \in \mathcal{H}$ such that <script type="math/tex">E_x [f(x)] = \lt f, \mu_p \gt _{\mathcal{H}}</script> for all $f \in \mathcal{H}$. If we set $f= \phi (t) = k(t, \cdot)$, we obtain $\mu_p(t) = &lt;\mu_p, k(t, Â·)&gt;_\mathcal{H} =E_xk(t, x)$, in other words, the mean embedding of the distribution $p$ is the expectation under $p$ of the canonical feature map.</p>
  </li>
  <li>We have
    <center> 
 $$
 \begin{split}
 MMD^2 \left[ \mathcal{F}, p,q \right] &amp;= \left[ \sup_{||f||_\mathcal{H} \leq 1}(E_x [ f(x)]âˆ’E_y [ f(y)])
\right]^2\\&amp;=    \left[ \sup_{||f||_\mathcal{H} \leq 1}&lt;\mu_p-\mu_q,f&gt;_\mathcal{H}
\right]^2\\&amp;=||\mu_p-\mu_q||_\mathcal{H}^2\\&amp;=||E_x\phi(x)-E_y\phi(y)||_\mathcal{H}^2
 \end{split}
 $$
 </center>
  </li>
  <li>The MMD is a metric, when $\mathcal{H}$ is a universal RKHSs, defined on a compact metric space X. It can be proven that <strong>the Gaussian and Laplace RKHSs</strong> are universal.</li>
</ul>

<h4 id="finite-sample-estimate">Finite sample estimate</h4>
<p>Given $x$ and $x^{\prime}$ independent random variables with distribution $p$, and $y$ and $y^\prime$ independent random variables with distribution $q$, the squared population MMD is</p>
<center>
$$
MMD^2 [\mathcal{F}, p,q] = E_{x,x^\prime}
\left[k(x, x^\prime)\right] +E_{y,y^\prime}\left[ k(y, yâ€²)\right]âˆ’2E_{x,y} \left[k(x, y)\right]
$$
</center>

<ol>
  <li>
    <p>An unbiased empirical estimate
 <img src="/img/15924519721534.jpg" width="75%" height="100%" /></p>
  </li>
  <li>
    <p>An biased empirical estimate
 <img src="/img/15924520728791.jpg" width="75%" height="100%" /></p>

    <p>It costs $O((m+n)^2)$ time to compute both statistics</p>

    <p>#### å‚è€ƒèµ„æ–™</p>
    <ol>
      <li>https://www.cnblogs.com/kailugaji/p/11004246.html</li>
    </ol>
  </li>
</ol>

:ET