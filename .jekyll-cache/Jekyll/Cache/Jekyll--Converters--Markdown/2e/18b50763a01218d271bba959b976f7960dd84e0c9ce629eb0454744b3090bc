I"¸<p><a href="https://arxiv.org/pdf/1511.07122.pdf">Paper link</a></p>

<h4 id="take-away-message">Take away message</h4>
<ol>
  <li>Dilated convolution operator, which can expend the receptive field without losing resolution or coverage, is very suitable for dense prediction task.</li>
  <li>multi-scale context module, can reliably increases accuracy when plugged into existing semantic segmentation systems.</li>
  <li>Replace pooling layer (designed for classification task) with dilated convolution (designed for segmentation task) can increase accuracy.</li>
</ol>

<h4 id="model">Model</h4>
<ol>
  <li>dilated convolutions
    <ul>
      <li>$l$-dilated convolution: The familiar discrete convolution is simply the 1-dilated convolution.<br />
 <img src="/img/15887283730533.jpg" width="30%" height="100%" /><br />
 use $âˆ—_l$ to represent an $l$-dilated convolution</li>
      <li>Let $F_0, F_1, \cdots , F_{nâˆ’1} : Z^2 \rightarrow R$ be discrete functions and let $k_0, k_1, \cdots, k_{nâˆ’2} : â„¦_1 \rightarrow R$ ($â„¦_r = [âˆ’r, r]^2 âˆ© Z^2$) be discrete 3Ã—3 filter. Consider applying the filters with exponentially increasing dilation: 
  $$F_{i+1} = F_i âˆ—<em>{2^i}k_i for $i = 0, 1, \cdots , n âˆ’ 2$, then the size of the receptive field of each element in $F</em>{i+1}$ is $(2^{i+2} âˆ’ 1)Ã—(2^{i+2} âˆ’ 1)$.
<img src="/img/15887296456656.jpg" alt="-w709" /></li>
    </ul>
  </li>
  <li>context module
    <blockquote>
      <p>The context module is designed to increase the performance of dense prediction architectures by aggregating multi-scale contextual information. The module takes C feature maps as input and produces C feature maps as output. The input and output have the same form, thus the module can be plugged into existing dense prediction architectures.</p>
    </blockquote>

    <ul>
      <li>
        <p>module architecture</p>

        <p>here truncation is a ReLU function $f(\cdot)=max(\cdot, 0)$.<br />
<img src="/img/15887338289922.jpg" alt="-w656" /></p>
      </li>
      <li>
        <p>initialization</p>

        <p>Experiments revealed that standard initialization procedures (random initialization) do not readily support the training of the module.</p>
        <ul>
          <li>Basic: identity initialization<br />
  <img src="/img/15887343089814.jpg" width="30%" height="100%" /></li>
          <li>Large:<br />
  <img src="/img/15887343601296.jpg" width="50%" height="100%" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>front end
    <ul>
      <li>adapted the VGG-16 network for dense prediction and removed the last two pooling and striding layers. Specifically, each of these pooling and striding layers was removed and convolutions in all subsequent layers were dilated by a factor of 2 for each pooling layer that was ablated.</li>
      <li>use reflection padding: the buffer zone is filled by reflecting the image about each edge.</li>
    </ul>
  </li>
</ol>

<h4 id="result">Result</h4>
<ol>
  <li>
    <p>Front end
<img src="/img/15887346292420.jpg" alt="-w599" />
<img src="/img/15887346668224.jpg" alt="-w609" /></p>
  </li>
  <li>
    <p>context module
<img src="/img/15887347716594.jpg" alt="-w599" />
<img src="/img/15887348908756.jpg" alt="-w606" /></p>
  </li>
</ol>
:ET