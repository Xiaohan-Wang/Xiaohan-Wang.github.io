I"y
<ul>
  <li><code class="highlighter-rouge">data.cuda()</code>: the old, pre-0.4 way</li>
  <li><code class="highlighter-rouge">data.to(device)</code>: more flexible. For example, <code class="highlighter-rouge">data.to('cuda:1')</code> put data to GPU1</li>
</ul>

<h4 id="单机单卡">单机单卡</h4>
<p>如果不设置Dataparallel，即使一台机器上有多个GPU，pytorch也只会占用编号为0的GPU</p>

<h4 id="单机多卡">单机多卡</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># have to put model and data to GPU0
</span>
<span class="c1"># model
</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:0'</span><span class="p">)</span>
<span class="c1"># data
</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:0'</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="多机多卡">多机多卡</h4>
<p>待填</p>
:ET