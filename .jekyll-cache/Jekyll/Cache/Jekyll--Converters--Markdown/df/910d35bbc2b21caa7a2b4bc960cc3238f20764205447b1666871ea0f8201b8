I"2<p><a href="https://arxiv.org/pdf/1704.04861.pdf">Paper link</a></p>

<h4 id="take-away-message">Take away message</h4>
<ol>
  <li>MobileNets: light weight deep neural networks that use depth-wise separable convolutions</li>
  <li>two simple global hyper-parameters that efficiently trade off between latency and accuracy</li>
</ol>

<h4 id="model">Model</h4>
<h5 id="depth-separable-convolution">Depth separable convolution</h5>
<p><img src="/img/15895586236484.jpg" width="50%" height="100%" /></p>
<ol>
  <li>depthwise convolution: apply a single filter per each input channel</li>
  <li>pointwise convolution: a simple 1×1 convolution to create a linear combination of the output of the depthwise layer</li>
</ol>

<ul>
  <li>MobileNets use both batchnorm and ReLU nonlinearities for both layers.
<img src="/img/15895586917635.jpg" width="50%" height="100%" /></li>
  <li>computation cost
    <ul>
      <li>symbols
        <ul>
          <li>$D_F$: input feature size / output feature size</li>
          <li>$D_K$: kernel size</li>
          <li>$M$: input channels</li>
          <li>$N$: outout channels</li>
        </ul>
      </li>
      <li>standard convolutions<br />
  <img src="/img/15895585271744.jpg" width="33%" height="100%" /></li>
      <li>depthwise separable convolutions<br />
  <img src="/img/15895588519425.jpg" width="50%" height="100%" /></li>
      <li>reduction in computation: MobileNet uses 3 × 3 depthwise separable convolutions, which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy<br />
  <img src="/img/15895588852425.jpg" width="50%" height="100%" /></li>
    </ul>
  </li>
</ul>

<h5 id="network-structure">Network structure</h5>
<p><img src="/img/15895692454944.jpg" width="50%" height="100%" /></p>
<ol>
  <li>The first layer is a full convolution.</li>
  <li>All layers are followed by a batchnorm and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification</li>
  <li>Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer.</li>
  <li>A final average pooling reduces the spatial resolution to 1 before the fully connected layer.</li>
  <li>Counting depthwise and pointwise convo- lutions as separate layers, MobileNet has 28 layers.</li>
</ol>

<ul>
  <li>MobileNets structure puts nearly all of the computation into dense 1×1 convolutions. This can be implemented with highly optimized general matrix multiply (GEMM) functions.</li>
</ul>

<h5 id="tricks">Tricks</h5>
<ol>
  <li>contrary to training large models we use less regularization and data augmentation techniques because small models have less trouble with overfitting.</li>
  <li>it was important to put very little or no weight decay (L2 regularization) on the depthwise filters since their are so few parameters in them.</li>
</ol>

<h4 id="hyperparameter">Hyperparameter</h4>
<h5 id="width-multiplier-thinner-models">Width Multiplier: Thinner Models</h5>
<ul>
  <li>The role of the width multiplier $\alpha$ is to thin a network uniformly at each layer (the number of input channels $M$ becomes $\alpha M$,  and the number of output channels $N$ becomes $\alpha N$)</li>
  <li>The computational cost of a depthwise separable convolution with width multiplier $\alpha$:
  <img src="/img/15895698351332.jpg" width="50%" height="100%" /></li>
  <li>$ \alpha \in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25.</li>
  <li>Width multiplier has the effect of reducing <strong>computational cost</strong> and <strong>the number of parameters</strong> quadratically by roughly $\alpha^2$.</li>
</ul>

<h5 id="resolution-multiplier-reduced-representation">Resolution Multiplier: Reduced Representation</h5>
<ul>
  <li>resolution multiplier $\rho$ is applied to the input image and the internal representation of every layer. In practice we implicitly set $\rho$ by setting the input resolution.</li>
  <li>the computational cost for the core layers with width multiplier $\alpha$ and resolution multiplier $\alpha$:
  <img src="/img/15895703362678.jpg" width="50%" height="100%" /></li>
  <li>$\rho \in (0, 1]$, which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128.</li>
  <li>Resolution multiplier has the effect of reducing **computational cost **by $\rho^2$.</li>
</ul>

<h4 id="result">Result</h4>
<p><img src="/img/15895706506529.jpg" width="50%" height="100%" /></p>

<h4 id="further-reference">Further reference</h4>
<ol>
  <li>Distilling the knowledge in a neural network.</li>
</ol>
:ET