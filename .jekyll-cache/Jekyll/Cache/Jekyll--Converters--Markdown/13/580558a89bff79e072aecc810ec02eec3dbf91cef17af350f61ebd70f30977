I"y<p><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Moment_Matching_for_Multi-Source_Domain_Adaptation_ICCV_2019_paper.pdf">Paper link</a> and <a href="http://ai.bu.edu/M3SDA/">code</a></p>

<h4 id="take-away-message">Take away message</h4>
<ol>
  <li>the upper bound on the target error of the learned hypothesis depends on the pairwise moment divergence $d_{CM^k}(D_S, D_T)$ between the target domain and each source domain. (This provides a direct motivation for moment matching approaches)</li>
  <li>The last term target error is lower bounded by the pairwise divergences between source domains (implies we might also need to minimize divergence between source domains)</li>
  <li>DomainNet: multi-souce domain adaptation dataset</li>
</ol>

<h4 id="model">Model</h4>
<p><img src="/img/15857182767863.jpg" alt="-w909" /></p>

<ol>
  <li>moment distance based loss:<br />
<img src="/img/15857184320957.jpg" width="60%" height="100%" /></li>
  <li>overall training objective:<br />
<img src="/img/15857185193833.jpg" width="50%" height="100%" /></li>
  <li>In order to align $p(y;x)$ and $p(x)$ at the same time, they follow the training paradigm proposed in [1].
    <ul>
      <li>leverage two classifiers per domain to form N pairs of classifiers $C′ = {(C1, C1′), (C2, C2′), …, (CN, CN ′)}$. The training procedure includes three steps.
        <ol>
          <li>train $G$ and $C′$ to classify the multi-source samples correctly based on the overall training objective.</li>
          <li>train the classifier pairs for a fixed $G$. The goal is to make the discrepancy of each pair of classifiers as large as possible on the target domain.</li>
          <li>fix $C′$ and train $G$ to minimize the discrepancy of each classifier pair on the target domain.</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<h4 id="result">Result</h4>
<p><img src="/img/15857191596055.jpg" alt="-w781" /></p>

<p><img src="/img/15857190392968.jpg" alt="-w851" />个人感觉主要是$M^3SDA-\beta$中的training paradigm在发挥作用。</p>

<h4 id="further-reference">Further reference</h4>
<ol>
  <li>Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), June 2018.</li>
  <li>Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnab´as P´oczos. Mmd gan: Towards deeper understanding of moment matching network. In Advances in Neural Information Processing Systems, pages 2203–2213, 2017.</li>
  <li>Yujia Li, Kevin Swersky, and Rich Zemel. Generative mo- ment matching networks. In International Conference on Machine Learning, pages 1718–1727, 2015.</li>
  <li>Youssef Mroueh, Tom Sercu, and Vaibhava Goel. McGan: Mean and covariance feature matching GAN. In Doina Pre- cup and Yee Whye Teh, editors, Proceedings ofthe 34th In- ternational Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2527– 2535, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.</li>
</ol>

<p>2，3，4是Gan based moment matching</p>

:ET