<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 24 Mar 2020 02:52:49 -0400</pubDate>
    <lastBuildDate>Tue, 24 Mar 2020 02:52:49 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title></title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.12833.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/valeoai/ADVENT&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;观察到source domain prediction是over-confident / low entropy，而target domain prediction是under-confident / high entropy&lt;/li&gt;
  &lt;li&gt;direct entropy minimization&lt;/li&gt;
  &lt;li&gt;structure adaptation: adversarial learning&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15850310997747.jpg&quot; alt=&quot;-w930&quot; /&gt;&lt;/p&gt;

&lt;p&gt;class ratio prior:&lt;br /&gt;
Entropy minimization can get biased towards some easy classes. Therefore, sometimes it is beneficial to guide the learning with some prior. 此处根据class ratio prior设计了另一个Loss来guide learning.&lt;/p&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;In general, AdvEnt works better than MinEnt. By combining results of the two models MinEnt and AdvEnt, we observe a decent boost in performance, compared to results of single models. 
&lt;img src=&quot;/img/15850315105452.jpg&quot; alt=&quot;-w923&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;oracle performance (trained on source and target)
&lt;img src=&quot;/img/15850315456104.jpg&quot; alt=&quot;-w180&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Training on specific entropy ranges.&lt;/li&gt;
  &lt;li&gt;Using class-ratio prior.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;insight&quot;&gt;Insight&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;找source domain结果/中间feature map具备、而target domain不具备的性质，用adversarial learning (e.g. cross entropy)&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 24 Mar 2020 02:52:49 -0400</pubDate>
        <link>http://localhost:4000/2020/03/24/2020-03-23-ADVENT/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/24/2020-03-23-ADVENT/</guid>
        
        
      </item>
    
      <item>
        <title>Unsupervised Domain Adaptation using Feature-Whitening and Consensus Loss</title>
        <description>&lt;h3 id=&quot;take-away-message&quot;&gt;Take Away Message&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Domain alignment layers which implement feature whitening for the purpose of matching source and target feature distributions and increases the smoothness of the loss landscape.&lt;/li&gt;
  &lt;li&gt;Min-Entropy Consensus loss regularizes training while avoiding the adoption of many user-defined hyper-parameters&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15843077382687.jpg&quot; alt=&quot;-w752&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DA的四种方法：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Correlation Alignment paradigm&lt;/li&gt;
  &lt;li&gt;domain-specific alignment layers&lt;/li&gt;
  &lt;li&gt;entropy minimization distribution&lt;/li&gt;
  &lt;li&gt;consistency-enforcing paradigm&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;由 12 -&amp;gt; DWTL layer&lt;br /&gt;
由 34 -&amp;gt; MEC loss&lt;/p&gt;

&lt;p&gt;source: cross-entropy loss
&lt;img src=&quot;/img/15843177084504.jpg&quot; alt=&quot;-w200&quot; /&gt;
target: MEC loss
&lt;img src=&quot;/img/15843177196984.jpg&quot; alt=&quot;-w300&quot; /&gt;
final loss:
&lt;img src=&quot;/img/15843177315198.jpg&quot; alt=&quot;-w300&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;要求在source上表现好&lt;/li&gt;
  &lt;li&gt;要求在target上对perturbation具有robustness，同时对结果high confidence&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;digit classification&lt;/li&gt;
  &lt;li&gt;object recognition&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;potential-improvement&quot;&gt;Potential Improvement&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;try full “coloring”&lt;/li&gt;
  &lt;li&gt;MEC loss中pseudo label的选择方式？&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 14 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/14/Unsupervised-Domain-Adaptation-using-Feature-Whitening-and-Consensus-Loss/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/14/Unsupervised-Domain-Adaptation-using-Feature-Whitening-and-Consensus-Loss/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Domain Adaptation Concept Summary</title>
        <description>&lt;h3 id=&quot;transfer-learning&quot;&gt;Transfer learning&lt;/h3&gt;
&lt;h4 id=&quot;domain-and-task&quot;&gt;domain and task&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;domain = feature space + marginal distribution&lt;/li&gt;
  &lt;li&gt;task = label space + conditional distribution (predictive function)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;transfer-learning定义&quot;&gt;transfer learning定义&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15842471547628.jpg&quot; alt=&quot;-w914&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;正常的machine learning: 只有一个domain，通过自己的X、Y学习predictive function&lt;/li&gt;
  &lt;li&gt;transfer learning：利用source的知识提高target的predictive function的表现
    &lt;ul&gt;
      &lt;li&gt;source和target是不同的
        &lt;ul&gt;
          &lt;li&gt;feature space不同&lt;/li&gt;
          &lt;li&gt;marginal distribution不同&lt;/li&gt;
          &lt;li&gt;label space不同&lt;/li&gt;
          &lt;li&gt;conditional distribution不同  （？？）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;domain adaptation&lt;/em&gt;: transfer learning的子类，source和target只有marginal distribution不同，其余三点都相同&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sat, 14 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/14/Domain-Adaptation-Concept-Summary/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/14/Domain-Adaptation-Concept-Summary/</guid>
        
        <category>Domain Adaptation</category>
        
        <category>Concept</category>
        
        
      </item>
    
      <item>
        <title>A Few Useful Things to Know About Machine Learning</title>
        <description>&lt;h2 id=&quot;12-trick-lessons-in-ml&quot;&gt;12 trick lessons in ML&lt;/h2&gt;

&lt;h3 id=&quot;1-learning-algorithm--representation--evaluation--optimization&quot;&gt;1. learning algorithm = representation + evaluation + optimization&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;representation&lt;/strong&gt;:  choosing a representation for a learner is tantamount to choosing the hypothesis space&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;evaluation&lt;/strong&gt;: evaluation function&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;optimization&lt;/strong&gt;: the method to search among the hypothesis space&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15817081275431.jpg&quot; alt=&quot;-w801&quot; /&gt;
三者同样重要&lt;/p&gt;

&lt;h3 id=&quot;2-generalization-counts&quot;&gt;2. generalization counts&lt;/h3&gt;
&lt;p&gt;strict separation between training and test set.&lt;/p&gt;

&lt;h3 id=&quot;3-data-alone-is-not-enough&quot;&gt;3. data alone is not enough&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;data alone is not enough, no matter how much of it you have. (e.g. Consider learning a Boolean function of (say) 100 variables from a million examples. There are 2^100 − 10^6 examples whose classes you don’t know.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;every learner must embody some &lt;strong&gt;knowledge or assumption&lt;/strong&gt; beyond the data it’s given in order to generalize beyond it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;learners are like lever: less knowledge/assumption + (data) -&amp;gt; useful results. But it still need more than zero input knowledge to work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;corollary: one of the key criteria for choosing a representation is which kinds of knowledge are easily expressed in it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;4-overfitting&quot;&gt;4. overfitting&lt;/h3&gt;
&lt;p&gt;if the knowledge and data we have are not sufficient to completely determine the correct classifier -&amp;gt; risk of overfitting&lt;/p&gt;

&lt;p&gt;Note: A common misconception about overfitting is that it is caused by noise, but severe overfitting can occur even in the absence of noise.&lt;/p&gt;

&lt;h3 id=&quot;5-high-dimensions&quot;&gt;5. high dimensions&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;curse of dimensionality:  our intuition, which come from a three-dimensional world, often do not apply in high-dimensional ones. Naively, one might think that gathering more features never hurts, since at worst they provide no new information about the class. But in fact their benefits may be outweighed by the curse of dimensionality.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;blessing of non-uniformity: In most applications examples are not spread uniformly throughout the instance space, but are concentrated on or near a lower-dimensional manifold.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;6-theoretical-guarantees&quot;&gt;6. theoretical guarantees&lt;/h3&gt;
&lt;p&gt;The main role of theoretical guarantees in machine learning
is not as a criterion for practical decisions, but as a source of
understanding and driving force for algorithm design. In this
capacity, they are quite useful; indeed, the close interplay
of theory and practice is one of the main reasons machine
learning has made so much progress over the years. But
caveat emptor: learning is a complex phenomenon, and just
because a learner has a theoretical justification and works in
practice doesn’t mean the former is the reason for the latter.&lt;/p&gt;

&lt;h3 id=&quot;7-feature-engineering&quot;&gt;7. feature engineering&lt;/h3&gt;
&lt;p&gt;Often, the raw data is not in a form that is amenable to learning, but you can construct features from it that are.&lt;/p&gt;

&lt;p&gt;Note: features that look irrelevant in isolation may be relevant in combination. For example, if the class is an XOR of k input features, each of them by itself carries no information about the class.&lt;/p&gt;

&lt;h3 id=&quot;8-more-data-beats-a-cleverer-algorithm&quot;&gt;8. MORE DATA BEATS A CLEVERER ALGORITHM&lt;/h3&gt;
&lt;p&gt;As a rule of thumb, a dumb algorithm with lots and lots of data beats a clever one with modest amounts of it.&lt;/p&gt;

&lt;h3 id=&quot;9-learn-many-models-not-just-one&quot;&gt;9. LEARN MANY MODELS, NOT JUST ONE&lt;/h3&gt;
&lt;p&gt;model ensembles (researchers noticed that, if instead
of selecting the best variation found, we combine many variations, the results are better—often much better—and at
little extra effort for the user)&lt;/p&gt;

&lt;h3 id=&quot;10--simplicity-does-not-imply-accuracy&quot;&gt;10.  SIMPLICITY DOES NOT IMPLY ACCURACY&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;contrary to intuition, there is no necessary connection between the number of parameters of a model and its tendency to overfit.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the size of the hypothesis space is only a rough guide to what really matters for relating training and test error: the procedure by which a hypothesis is chosen.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;11-representable-does-not-imply-learnable&quot;&gt;11. REPRESENTABLE DOES NOT IMPLY LEARNABLE&lt;/h3&gt;
&lt;p&gt;Just because a function can be represented does not mean it can be learned (e.g.  if the hypothesis space has many local optima of the evaluation function, as is often the case, the learner may not find the true function even if it is representable.). Therefore the key question is not “Can it be represented?”, to which the answer is often trivial, but “Can it be learned?”&lt;/p&gt;

&lt;h3 id=&quot;12-correlation-does-not-imply-causation&quot;&gt;12. CORRELATION DOES NOT IMPLY CAUSATION&lt;/h3&gt;
</description>
        <pubDate>Tue, 11 Feb 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/02/11/A-Few-Useful-Things-to-Know-About-Machine-Learning/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/11/A-Few-Useful-Things-to-Know-About-Machine-Learning/</guid>
        
        <category>ML</category>
        
        
      </item>
    
      <item>
        <title>python各类图像库</title>
        <description>&lt;h3 id=&quot;short-version&quot;&gt;short version&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;类型&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;顺序&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;size&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;scale&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;cv2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;numpy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;BGR&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;(h,w,c)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;255(uint8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;PIL&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;Image/numpy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;RGB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;(w,h) / (h,w,c)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;- / 255(uint8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;skimage&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;numpy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;RGB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;(h,w,c)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;rgb:255(uint8) / gray:1(float)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;cv2&quot;&gt;cv2&lt;/h3&gt;
&lt;h4 id=&quot;ioshow&quot;&gt;i/o/show&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#读入图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
  
&lt;span class=&quot;c1&quot;&gt;#显示图像    
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'src'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitkey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;#保存图像   
&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imwrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'test.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  

&lt;span class=&quot;c1&quot;&gt;#归一化  
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;float&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;255.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;font size=&quot;3&quot;&gt;`imshow` should be followed by `waitKey` function which displays the image for specified milliseconds. **Otherwise, it won’t display the image**. For example, `waitKey(0)` will display the window infinitely until any keypress (it is suitable for image display). waitKey(25) will display a frame for 25 ms, after which display will be automatically closed. (If you put it in a loop to read videos, it will display the video frame-by-frame)&lt;/font&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;attribute&quot;&gt;attribute&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;numpy array&lt;/li&gt;
  &lt;li&gt;默认读入彩色图像
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gray = cv2.imread('1.jpg',cv2.IMREAD_GRAYSCALE)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;channel顺序：BGR
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;im2 = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;img.shape: (h,w,c)&lt;/li&gt;
  &lt;li&gt;img.dtype: uint8(255)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pil&quot;&gt;PIL&lt;/h3&gt;
&lt;h4 id=&quot;ioshow-1&quot;&gt;i/o/show&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;PIL&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#读入图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#显示图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#保存图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'2.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;image对象&quot;&gt;Image对象&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;PIL读入图片后生成Image对象&lt;/li&gt;
  &lt;li&gt;img.size: (w, h)&lt;/li&gt;
  &lt;li&gt;img.mode: 灰度L / 真彩色RGB&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;image转numpy矩阵&quot;&gt;Image转numpy矩阵&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;arr = np.array(img) &lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;channel顺序：RGB&lt;/li&gt;
  &lt;li&gt;arr.shape: (h,w,c)&lt;/li&gt;
  &lt;li&gt;arr.dtype: uint8 (255)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;new_img = Image.fromarray(arr)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;skimage&quot;&gt;skimage&lt;/h3&gt;
&lt;h4 id=&quot;ioshow-2&quot;&gt;i/o/show&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;skimage&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#读入图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#显示图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#保存图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imsave&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;attribute-1&quot;&gt;attribute&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;numpy array&lt;/li&gt;
  &lt;li&gt;channel顺序：RGB&lt;/li&gt;
  &lt;li&gt;img.shape: (h,w,c)&lt;/li&gt;
  &lt;li&gt;img.dtype: RGB uint8 (255) / gray float (1)&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 08 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/08/python%E5%90%84%E7%B1%BB%E5%9B%BE%E5%83%8F%E5%BA%93/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/08/python%E5%90%84%E7%B1%BB%E5%9B%BE%E5%83%8F%E5%BA%93/</guid>
        
        <category>python</category>
        
        
      </item>
    
      <item>
        <title>Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks</title>
        <description>&lt;h3 id=&quot;idea&quot;&gt;Idea&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;适用于domain间存在low-level difference的domain adaptation&lt;/li&gt;
  &lt;li&gt;不需要domain间的corresponding pairs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15785106924617.jpg&quot; alt=&quot;-w857&quot; /&gt;
&lt;img src=&quot;/img/15785107411179.jpg&quot; alt=&quot;-w50&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;主要由discriminator(D)来学习，classifier(T: task)和content-similarity loss主要用来 stabalize GAN的训练&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context-similarity loss: PMSE (pairwise mean squared error)
    &lt;blockquote&gt;
      &lt;p&gt;This loss allows the model to learn to reproduce the overall shape of the objects being modeled without wasting modeling power on the absolute color or intensity of the inputs, while allowing our adversarial training to change the object in a consistent way.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reflection&quot;&gt;Reflection&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;T的参与感觉让domain adaptation变成了迎合任务的domain adaptation, 而不是单纯的pixel-level domain adaptation. (也有优势，在对应任务上应该有更好的表现)&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 08 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/08/Unsupervised-Pixel-Level-Domain-Adaptation-with-GAN/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/08/Unsupervised-Pixel-Level-Domain-Adaptation-with-GAN/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Pixel-Level Domain Transfer</title>
        <description>&lt;h3 id=&quot;idea&quot;&gt;Idea&lt;/h3&gt;
&lt;p&gt;transfer images in one domain into another domain&lt;/p&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15784545847019.jpg&quot; alt=&quot;-w744&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;real/fake discriminator: 判断图像是否来自target domain
&lt;img src=&quot;/img/15784548684906.jpg&quot; alt=&quot;-w727&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;domain (semantic) discriminator: 判断是否和source domain图像具有相同semantic information
&lt;img src=&quot;/img/15784548946537.jpg&quot; alt=&quot;-w729&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;converter
 &lt;img src=&quot;/img/15784556404369.jpg&quot; alt=&quot;-w709&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15784549145475.jpg&quot; alt=&quot;-w771&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reflection&quot;&gt;Reflection&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;作者构建的dataset将source domain和target domain中的图像make pair，从而学习image transfer。但general的DA任务并不会有source和target的image pair，因此很难直接使用&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MSE和discriminator：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;MSE容易产生blurry images, 因为其假设pixels服从Gaussian distribution&lt;/li&gt;
      &lt;li&gt;MSE never allow a small geometric miss-alignment&lt;/li&gt;
      &lt;li&gt;当有关联，但又不容易用公式描述时，可以尝试discriminator&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 07 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/07/Pixel-Level-Domain-Transfer/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/07/Pixel-Level-Domain-Transfer/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Unbiased Look at Dataset Bias</title>
        <description>&lt;h3 id=&quot;promise-and-perils-of-visual-dataset&quot;&gt;Promise and Perils of Visual Dataset&lt;/h3&gt;
&lt;h4 id=&quot;promise&quot;&gt;Promise&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Dataset是CV取得progress的重要因素（甚至超过算法的影响）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;让CV更像一门实验科学（rather than a black art）&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;perils&quot;&gt;Perils&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;(good finetune + bad approach) 表现好于 (bad finetune + good approach) =&amp;gt; 难以决定new approach的真正表现&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;pay too much attention on “winning” a competition, 但其实提高可能很微小 =&amp;gt; 我们应该将performance number作为一个check，而不是competiton，从而给new approach留出发展的机会&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;the-rise-of-modern-dataset&quot;&gt;The rise of modern dataset&lt;/h3&gt;
&lt;p&gt;repeat: one dataset -&amp;gt; being rejected due to its perceived biases -&amp;gt; new dataset -&amp;gt; being rejected due to its perceived biases -&amp;gt; …&lt;/p&gt;

&lt;h3 id=&quot;dataset-biases&quot;&gt;Dataset Biases&lt;/h3&gt;
&lt;p&gt;Instead of representing the visual world, datasets have become closed world onto themselves.&lt;/p&gt;
&lt;h4 id=&quot;selection-bias&quot;&gt;selection bias&lt;/h4&gt;
&lt;p&gt;datasets often prefer particular kinds of images (e.g. street scenes, or nature scenes, or images retrieved via Internet keyword searches)&lt;/p&gt;
&lt;h4 id=&quot;capture-bias&quot;&gt;capture bias&lt;/h4&gt;
&lt;p&gt;photographers tending to take pictures of objects in similar ways (although this bias might be similar across the different datasets)&lt;/p&gt;
&lt;h4 id=&quot;category-or-label-bias&quot;&gt;category or label bias&lt;/h4&gt;
&lt;p&gt;This comes from the fact that semantic categories are often poorly defined, and different labellers may assign differing labels to the same type of object2.&lt;/p&gt;
&lt;h4 id=&quot;negative-set-bias&quot;&gt;&lt;font color=&quot;#0000dd&quot;&gt;negative set bias&lt;/font&gt;&lt;/h4&gt;
&lt;p&gt;Datasets define a visual phenomenon (e.g. object, scene, event) not just by what it is (positive instances), but also by what it is not (negative instances). The negative set defines what the dataset considers to be “the rest of the world”. If that set is not representative, or unbalanced, that could produce classifiers that are overconfident and not very discriminative.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;font size=&quot;3&quot;&gt;This is particularly important for classification tasks, where the number of negatives is only a few orders of magnitude larger than the number of positives for each class. For example, if we want to find all images of “boats” in a PASCAL VOC-like classification task setting, how can we make sure that the classifier focuses on the boat itself, and not on the water below, or shore in the distance (after all, all boats are depicted in water)? This is where a large negative set (including rivers, lakes, sea, etc, without boats) is imperative to “push” the lazy classifier into doing the right thing.&lt;/font&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;dataset-value&quot;&gt;Dataset value&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15783472481154.jpg&quot; alt=&quot;-w842&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ways-to-avoid-biases-during-dataset-construction&quot;&gt;Ways to avoid biases during dataset construction&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=5995347&amp;amp;tag=1&quot;&gt;paper link&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 05 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/05/Unbiased-Look-at-Dataset-Bias/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/05/Unbiased-Look-at-Dataset-Bias/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>SPP-Net (spatial pyramid pooling)</title>
        <description>&lt;h4 id=&quot;goal&quot;&gt;Goal&lt;/h4&gt;
&lt;p&gt;消除CNN对图片size/scale的限制&lt;/p&gt;

&lt;h4 id=&quot;motivation&quot;&gt;Motivation&lt;/h4&gt;
&lt;p&gt;CNN = convolutional layers + fully-connected layers&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;convolutional layers不要求图片size&lt;/li&gt;
  &lt;li&gt;fully-connected layers对size有要求&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;思路：在fully-connected layers前面加入SPP layer, 其能对不同size的feature产生fixed-length output，然后送入fully-connected layers&lt;/p&gt;

&lt;p&gt;在image detection上的应用
&lt;img src=&quot;/img/15849366399167.jpg&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15849365194047.jpg&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In each spatial bin, pool (max pooling in the paper) the response of each filter. &lt;br /&gt;
The output of SPP: k*M (M: number  of bins, e.g. 16+4+1; k: number of channels, e.g. 256)&lt;/p&gt;

</description>
        <pubDate>Mon, 02 Dec 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2019/12/02/SPP-Net/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/12/02/SPP-Net/</guid>
        
        <category>CV paperlist</category>
        
        
      </item>
    
  </channel>
</rss>
