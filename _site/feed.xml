<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 10 May 2020 15:57:00 -0400</pubDate>
    <lastBuildDate>Sun, 10 May 2020 15:57:00 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>为什么现在的CNN模型都是在VGGNet或者ResNet上调整的？</title>
        <description>&lt;p&gt;答案来自&lt;a href=&quot;https://www.zhihu.com/question/43370067/answer/128881262&quot;&gt;知乎&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Q: 为什么现在的CNN模型都是在VGGNet或者ResNet上调整的？&lt;/p&gt;

&lt;p&gt;A:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;公开的论文需要一个标准的baseline及在baseline上改进的比较，因此大家会基于一个公认的baseline开始做实验，这样大家才比较信服。&lt;/li&gt;
  &lt;li&gt;视觉、自然语言等更专注于本领域的内部知识，而非baseline本身。因此常常是在一个base网络的基础之上进行修改，以验证自己方法的有效性。&lt;/li&gt;
  &lt;li&gt;进行基本模型的改进需要大量的实验和尝试，很有可能投入产出比比较小。对于深度学习，很大部分可以提升性能的点在于一些对于细节的精确把握。因此可以看到许多排名靠前的队伍最后讲的关键技术点似乎都是tricks。而这样精确细节的把握是需要大量的时间和计算资源的，往往在学校不可行。&lt;/li&gt;
  &lt;li&gt;AlexNet, Network in Network, VGG, GoogLeNet, Resnet等CNN网络都是图片分类网络, 都是在imagenet上1.2 million数据训练出来的。一般来说，某CNN网络在imagenet上面的分类结果越好，其deep feature的generalization能力越强，可以应用到各种CV问题。&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 10 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/10/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%8E%B0%E5%9C%A8%E7%9A%84CNN%E6%A8%A1%E5%9E%8B%E9%83%BD%E6%98%AF%E5%9C%A8VGGNet%E6%88%96%E8%80%85ResNet%E4%B8%8A%E8%B0%83%E6%95%B4%E7%9A%84/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/10/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%8E%B0%E5%9C%A8%E7%9A%84CNN%E6%A8%A1%E5%9E%8B%E9%83%BD%E6%98%AF%E5%9C%A8VGGNet%E6%88%96%E8%80%85ResNet%E4%B8%8A%E8%B0%83%E6%95%B4%E7%9A%84/</guid>
        
        <category>Q &amp; A</category>
        
        
      </item>
    
      <item>
        <title>RefineNet:_Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1611.06612.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;设计RefineNet，结合高层语义信息和低层细节信息，得到高分辨率的分割图&lt;/li&gt;
  &lt;li&gt;RefineNet中大量使用了ResNet中Identity mapping (both short range and long range) 的思想，保证了有效的端到端训练&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;introduction&quot;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;网络中pooling操作会降低分割图的分辨率，目前有三种解决方式：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;learn deconvolutional filters as an up-sampling operation: The deconvolution operations are not able to recover the low-level visual features which are lost after the downsampling operation in the convolution forward stage.&lt;/li&gt;
  &lt;li&gt;Deeplab系列中的atrous convolution: &lt;strong&gt;a significant cost in memory, because unlike the image subsampling methods, one must retain very large numbers of feature maps at higher resolution.&lt;/strong&gt; In practice, therefore, dilation convolution methods usually have a resolution prediction of no more than 1/8 size of the original rather than 1/4, when using a deep network.&lt;/li&gt;
  &lt;li&gt;exploits features from intermediate layers for generating high-resolution prediction (本文基于的方法)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15890589297975.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15890589494152.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15890590750133.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15890590946983.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15890591146152.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;reflection&quot;&gt;Reflection&lt;/h4&gt;
&lt;p&gt;感觉整个网络设计并没有太新颖的模块或思路，但可能是通过大量的identity mapping达成了有效的端对端训练，最后得到了非常棒的结果。如果是的话，说明设计网络时，网络各个模块能否得到充分的训练非常重要。&lt;/p&gt;
</description>
        <pubDate>Sat, 09 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/09/RefineNet-Multi-Path-Refinement-Networks-for-High-Resolution-Semantic-Segmentation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/09/RefineNet-Multi-Path-Refinement-Networks-for-High-Resolution-Semantic-Segmentation/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>DeepLab v2</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.00915.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;相比于DeepLab v1:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;atrous spatial pyramid pooling (ASPP) to deal with multiscale context and objects.&lt;/li&gt;
  &lt;li&gt;deeper convolutional neural networks (from VGG-16 to ResNet-101).&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;h5 id=&quot;atrous-convolution-for-dense-feature-extraction&quot;&gt;atrous convolution for dense feature extraction&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15889713549783.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;the number of filter parameters and the number of operations per position stay constant, but the effective filter size increases.&lt;/li&gt;
  &lt;li&gt;Pushing this approach all the way through the network could allow us to compute feature responses at the original image resolution, but this ends up being too costly. Thus the authors have adopted instead a hybrid approach that strikes a good efficiency/accuracy trade-off, using atrous convolution to increase by a factor of 4 the density of computed feature maps, followed by fast bilinear interpolation by an additional factor of 8 to recover feature maps at the original image resolution. Bilinear interpolation is sufficient in this setting because the class score maps are quite smooth.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;multiscale-image-representation&quot;&gt;multiscale image representation&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;standard multiscale processing
    &lt;ul&gt;
      &lt;li&gt;extract DCNN score maps from multiplerescaled versions of the original image using parallel DCNN branches that share the same parameters. To produce the final result, the authors bilinearly interpolate the feature maps from the parallel DCNN branches to the original image resolution and fuse them, by taking at each position the maximum response across the different scales.&lt;/li&gt;
      &lt;li&gt;do this both during training and testing.&lt;/li&gt;
      &lt;li&gt;Multiscale processing significantly improves performance, but at the cost of computing feature responses at all DCNN layers for multiple scales of input.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASPP
&lt;img src=&quot;/img/15889716579187.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15889716791787.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;fully-connected CRF&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;three main improvements compared to DeepLab v1:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;different learning policy during training&lt;/li&gt;
  &lt;li&gt;atrous spatial pyramid pooling&lt;/li&gt;
  &lt;li&gt;multi-scale processing and other factors&lt;/li&gt;
  &lt;li&gt;employment of deeper networks&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;different learning policy during training&lt;br /&gt;
employing a “poly” learning rate iter policy (the learning rate is multiplied by $(1−
\frac{\text{iter}}{\text{max_iter}} )^{\text{power}}$) is more effective than “step” learning rate (reduce the learning rate at a fixed step size).&lt;br /&gt;
 &lt;img src=&quot;/img/15889722172052.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASPP
&lt;img src=&quot;/img/15889726217656.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15889726447069.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;multi-scale processing and other factors (on ResNet 101)
&lt;img src=&quot;/img/15889728730154.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;VGG-16 vs. ResNet-101&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;DeepLab based on ResNet-101 delivers better segmentation results along object boundaries than employing VGG- 16. &lt;br /&gt;
 &lt;img src=&quot;/img/15889733334375.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
 &lt;img src=&quot;/img/15889732341678.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;Post-processing the ResNet-101 result with a CRF further improves the segmentation result. (有提高，但不大)&lt;br /&gt;
 &lt;img src=&quot;/img/15889730892561.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;VGG-16. K. Simonyan and A. Zisserman, “Very deep convolutional net- works for large-scale image recognition,” in ICLR, 2015.&lt;/li&gt;
  &lt;li&gt;ResNet. K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” arXiv:1512.03385, 2015.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 08 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/08/DeepLab-v2/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/08/DeepLab-v2/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>DeepLab v1</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1412.7062.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;two technical hurdles in the application of DCNNs to image labeling tasks: signal downsampling, and spatial ‘insensitivity’ (invariance).&lt;/li&gt;
  &lt;li&gt;signal downsampling: reduction of signal resolution incurred by the repeated max-pooling -&amp;gt; employ the atrous algorithm (就是dilated convolution)&lt;/li&gt;
  &lt;li&gt;spatial insensitivity: object-centric decisions from a classifier requires invariance to spatial transformations, inherently limiting the spatial accuracy of the DCNN model -&amp;gt; capture fine details by employing a fully-connected Conditional Random Field (CRF) + multi-scale prediction&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;修改自VGG-16&lt;/li&gt;
  &lt;li&gt;decouple the DCNN and CRF training stages, assuming the DCNN unary terms are fixed when setting the CRF parameters&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/img/15888811976149.jpg&quot; alt=&quot;-w724&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;efficient-feature-extraction-atrous-algorithm--reducing-model&quot;&gt;efficient feature extraction: atrous algorithm + reducing model&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;To compute scores more densely at target stride of 8 pixels:&lt;br /&gt;
 &lt;img src=&quot;/img/15888799732428.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;
    &lt;ol&gt;
      &lt;li&gt;convert the fully-connected layers of VGG-16 into convolutional ones and run the network in a convolutional fashion on the image at its original resolution&lt;/li&gt;
      &lt;li&gt;instead of subsampling after the last two max-pooling layers in the network, use the convolutional filters with an input stride of 2 or 4 pixels, respectively.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;After converting the network to a fully convolutional one, the first fully connected layer has 4,096 filters of large 7×7 spatial size and becomes the computational bottleneck.
    &lt;ol&gt;
      &lt;li&gt;Use filters with 4×4 (or 3×3) spatial size but with the same receptive fields.&lt;/li&gt;
      &lt;li&gt;reducing the number of channels at the fully connected layers from 4,096 down to 1,024.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;detail-boundary-recovery-fully-connected-crf--multi-scale-prediction&quot;&gt;detail boundary recovery: fully-connected CRF + multi-scale prediction&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;fully-connected CRF 
 &lt;img src=&quot;/img/15888811265917.jpg&quot; alt=&quot;-w723&quot; /&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Energy function:&lt;br /&gt;
 &lt;img src=&quot;/img/15888817215422.jpg&quot; width=&quot;40%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;first term:&lt;br /&gt;
 &lt;img src=&quot;/img/15888818741237.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;second term:&lt;br /&gt;
 &lt;img src=&quot;/img/15888819146749.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
 &lt;img src=&quot;/img/15888820310003.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
 the kernels are:&lt;br /&gt;
 &lt;img src=&quot;/img/15888821802918.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
 The first kernel forces pixels with similar color and position to have similar labels, while the second kernel only considers spatial proximity when enforcing smoothness.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;multi-scale prediction
 attach to the input image and the output of each of the first four max pooling layers a two-layer MLP (first layer: 128 3x3 convolutional filters, second layer: 128 1x1 convolutional fil- ters) whose feature map is concatenated to the main network’s last layer feature map. The aggregate feature map fed into the softmax layer is thus enhanced by 5 * 128 = 640 channels.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15888828891435.jpg&quot; alt=&quot;-w794&quot; /&gt;
&lt;img src=&quot;/img/15888829216375.jpg&quot; alt=&quot;-w739&quot; /&gt;
&lt;img src=&quot;/img/15888829687129.jpg&quot; alt=&quot;-w702&quot; /&gt;
&lt;img src=&quot;/img/15888830739237.jpg&quot; alt=&quot;-w725&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://hellodfan.com/2018/01/22/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E8%AE%BA%E6%96%87-DeepLab%E7%B3%BB%E5%88%97/&quot;&gt;部分翻译&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Krahenbuhl, P. and Koltun, V. Efficient inference in fully connected crfs with gaussian edge poten- tials. In NIPS, 2011.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 07 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/07/Deeplab-v1/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/07/Deeplab-v1/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>Dilated convolutions</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.07122.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Dilated convolution operator, which can expend the receptive field without losing resolution or coverage, is very suitable for dense prediction task.&lt;/li&gt;
  &lt;li&gt;multi-scale context module, can reliably increases accuracy when plugged into existing semantic segmentation systems.&lt;/li&gt;
  &lt;li&gt;Replace pooling layer (designed for classification task) with dilated convolution (designed for segmentation task) can increase accuracy.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;dilated convolutions
    &lt;ul&gt;
      &lt;li&gt;$l$-dilated convolution: The familiar discrete convolution is simply the 1-dilated convolution.&lt;br /&gt;
 &lt;img src=&quot;/img/15887283730533.jpg&quot; width=&quot;35%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
 use $∗_l$ to represent an $l$-dilated convolution&lt;/li&gt;
      &lt;li&gt;Let $F_0, F_1, \cdots , F_{n−1} : Z^2 \rightarrow R$ be discrete functions and let $k_0, k_1, \cdots, k_{n−2} : Ω_1 \rightarrow R$ ($Ω_r = [−r, r]^2 ∩ Z^2$) be discrete 3×3 filter. Consider applying the filters with exponentially increasing dilation: 
  &lt;script type=&quot;math/tex&quot;&gt;F_{i+1} = F_i ∗_{2^i}k_i\text{   for   } i = 0, 1, \cdots , n − 2&lt;/script&gt;, then the size of the receptive field of each element in $F_{i+1}$ is $(2^{i+2} − 1)×(2^{i+2} − 1)$.
&lt;img src=&quot;/img/15887296456656.jpg&quot; alt=&quot;-w709&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;context module
    &lt;blockquote&gt;
      &lt;p&gt;The context module is designed to increase the performance of dense prediction architectures by aggregating multi-scale contextual information. The module takes C feature maps as input and produces C feature maps as output. The input and output have the same form, thus the module can be plugged into existing dense prediction architectures.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;module architecture&lt;/p&gt;

        &lt;p&gt;here truncation is a ReLU function $f(\cdot)=max(\cdot, 0)$.&lt;br /&gt;
&lt;img src=&quot;/img/15887338289922.jpg&quot; alt=&quot;-w656&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;initialization&lt;/p&gt;

        &lt;p&gt;Experiments revealed that standard initialization procedures (random initialization) do not readily support the training of the module.&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;Basic: identity initialization&lt;br /&gt;
  &lt;img src=&quot;/img/15887343089814.jpg&quot; width=&quot;30%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;Large:&lt;br /&gt;
  &lt;img src=&quot;/img/15887343601296.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;front end
    &lt;ul&gt;
      &lt;li&gt;adapted the VGG-16 network for dense prediction and removed the last two pooling and striding layers. Specifically, each of these pooling and striding layers was removed and convolutions in all subsequent layers were dilated by a factor of 2 for each pooling layer that was ablated.&lt;/li&gt;
      &lt;li&gt;use reflection padding: the buffer zone is filled by reflecting the image about each edge.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Front end
&lt;img src=&quot;/img/15887346292420.jpg&quot; alt=&quot;-w599&quot; /&gt;
&lt;img src=&quot;/img/15887346668224.jpg&quot; alt=&quot;-w609&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;context module
&lt;img src=&quot;/img/15887347716594.jpg&quot; alt=&quot;-w599&quot; /&gt;
&lt;img src=&quot;/img/15887348908756.jpg&quot; alt=&quot;-w606&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 05 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/05/Dilated-convolutions/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/05/Dilated-convolutions/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>U-Net</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1505.04597.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/milesial/Pytorch-UNet&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;contracting path + expending path + skip connection&lt;/li&gt;
  &lt;li&gt;strong data augmentation, works well for very few images (it was designed for biomedical images)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15882170880287.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15882176961707.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It use valid convolution, thus the resolution gets lower and lower in the contracting path and expending path.&lt;/li&gt;
  &lt;li&gt;Because of the high resolution of biomedical images, overlap-tile strategy is used.&lt;/li&gt;
  &lt;li&gt;To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;contracting path:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels.&lt;/li&gt;
  &lt;li&gt;an upsampling of the feature map by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU.&lt;/li&gt;
  &lt;li&gt;At the final layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Training objective:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;softmax + cross entropy&lt;/li&gt;
  &lt;li&gt;class balancing (it’s important because of the large background)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;/h4&gt;
&lt;p&gt;Teach the network the desired invariance and robustness properties, when only few training samples are available:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;shift&lt;/li&gt;
  &lt;li&gt;rotation&lt;/li&gt;
  &lt;li&gt;gray value deformations&lt;/li&gt;
  &lt;li&gt;random elastic deformation&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size.&lt;/li&gt;
  &lt;li&gt;favor large input tiles over a large batch size and hence reduce the batch to a single image.&lt;/li&gt;
  &lt;li&gt;use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step.&lt;/li&gt;
  &lt;li&gt;Xavier initialization.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;time&quot;&gt;Time&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Training time: 10 hours on a NVidia Titan GPU (6 GB)&lt;/li&gt;
  &lt;li&gt;Inference time: Segmentation of a 512x512 image takes less than a second on a recent GPU&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 29 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/29/U-Net/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/29/U-Net/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>Conditional Generative Adversarial Network for Structured Domain Adaptation</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;We shouldn’t assume that the source and target domains share same intermediate feature space(s), i.e. using loss to obtain domain-invariant features.&lt;/li&gt;
  &lt;li&gt;Use a conditional generator to transform source feature maps into target-like, and trained on the target-like feature maps and original labels (和pixel level domain adaptation类似，但pixel level DA将source images转化为target-like，而本文将source features转化为target-like).&lt;/li&gt;
  &lt;li&gt;文章中提到 without relying on the assumption that the source and target domains share a same prediction function in a domain-invariant feature space。 实际只完成了not in a domain-invariant feature space， 本质上还是same prediction function (source和target domain的encoder和decoder都相同)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15881099962969.jpg&quot; alt=&quot;-w913&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;a conditional generator to generate residual features, to transform features of synthetic images to real-image like features.
    &lt;ul&gt;
      &lt;li&gt;a noise map $z$ is addd for randomness to create an unlimited number of training samples.&lt;/li&gt;
      &lt;li&gt;expect that $x_f$ preserves the semantic of the source feature map $x_s$, meanwhile appears as if it were extracted from a target domain image, i.e., a real image.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;a discriminator to distinguish adapted source features and real target features.&lt;/li&gt;
  &lt;li&gt;task-specific loss (decoder part)
    &lt;ul&gt;
      &lt;li&gt;train T with both adapted and non-adapted source feature maps (Training T solely on adapted feature maps leads to similar performance, but requires many runs with different initializations and learning rates due to the instability of the GAN).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;overall minimax objective:&lt;br /&gt;
&lt;img src=&quot;/img/15881108285954.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;overall performance: 涨幅非常大&lt;br /&gt;
&lt;img src=&quot;/img/15881108796823.jpg&quot; alt=&quot;-w905&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;amount of synthetic data
 &lt;img src=&quot;/img/15881109617007.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ablation Studies&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The effectiveness of conditional generator&lt;br /&gt;
 &lt;img src=&quot;/img/15881110275006.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Different lower layers for learning generator
 &lt;img src=&quot;/img/15881111038400.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;On the number of residual blocks （上图）&lt;/li&gt;
      &lt;li&gt;How much does the noise channel contribute?
 &lt;img src=&quot;/img/15881111498987.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 28 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/28/Conditional-Generative-Adversarial-Network-for-Structured-Domain-Adaptation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/28/Conditional-Generative-Adversarial-Network-for-Structured-Domain-Adaptation/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>SegNet:_A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.00561.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;In practice, we have to consider the cost of memory and computational time.&lt;/li&gt;
  &lt;li&gt;Most recent deep architectures for segmentation have identical encoder networks, i.e VGG16, but differ in the form of the decoder network, training and inference.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15880309668474.jpg&quot; alt=&quot;-w901&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Encoder:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The encoder network consists of 13 convolutional layers which correspond to the first 13 convolutional layers in the VGG16 network.&lt;/li&gt;
  &lt;li&gt;Initialize the training process from weights trained for classification on large datasets.&lt;/li&gt;
  &lt;li&gt;Benifit: Removing the fully connected layers of VGG16 makes the SegNet encoder network significantly smaller [reduces the number of parameters in the SegNet encoder network significantly (from 134M to 14.7M)] and easier to train.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Decoder:
&lt;img src=&quot;/img/15880312579950.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The appropriate decoder in the decoder network upsamples its input feature map(s) using the memorized max-pooling indices from the corresponding encoder feature map(s). This step produces sparse feature map(s).&lt;/li&gt;
  &lt;li&gt;These feature maps are then convolved with a trainable decoder filter bank to produce dense feature maps.&lt;/li&gt;
  &lt;li&gt;Except for the last decoder (which corresponds to the first encoder), the other decoders in the network produce feature maps with the same number of size and channels as their encoder inputs.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;p&gt;class balancing&lt;/p&gt;

</description>
        <pubDate>Mon, 27 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/27/SegNet-A-Deep-Convolutional-Encoder-Decoder-Architecture-for-Image-Segmentation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/27/SegNet-A-Deep-Convolutional-Encoder-Decoder-Architecture-for-Image-Segmentation/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>Fully Convolutional Networks for Semantic Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1411.4038.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/wkentaro/pytorch-fcn&quot;&gt;code (pytorch)&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;将分类网络中的全连接层变为对应的卷积层，从而得到全卷积网络。全卷积网络可用于实现如语义分割的稠密预测任务。添加skip-connecting结合deep, coarse, semantic information和shallow, fine, appearance information。&lt;/p&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;全连接层变为卷积层
&lt;img src=&quot;/img/15876140745604.jpg&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;view fully connected layers as convolutions with kernels that cover their entire input regions.&lt;/li&gt;
      &lt;li&gt;优点：When receptive fields overlap significantly, both feedforward computation and back propagation are much more efficient when computed layer-by-layer over an entire image instead of independently patch-by-patch. （receptive field: locations in the image that locations in higher layers are path-connected to.）&lt;/li&gt;
      &lt;li&gt;问题：由于subsampling, output的分辨率远低于input&lt;br /&gt;
 &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;转置卷积增大分辨率(what)+skip connection结合低层位置信息(where)
&lt;img src=&quot;/img/15876162869041.jpg&quot; alt=&quot;-w870&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;转置卷积能实现最好的上采样效果，通过learning可以实现非线性的上采样&lt;/li&gt;
      &lt;li&gt;将upsampled结果 (deep, coarse, semantic information) 和skip connection结果 (shallow, fine, appearance information) 加起来，得到combine what and where的结果&lt;/li&gt;
      &lt;li&gt;Initialize the 2× upsampling to bilinear interpolation, but allow the parameters to be learned）。 但是，在FCN-8s后，继续融合低层信息取得的结果基本不变。因此，最后几层直接使用bilinear interpolation。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;h5 id=&quot;评价指标&quot;&gt;评价指标&lt;/h5&gt;
&lt;p&gt;Let $n_{ij}$ be the number of pixels of class $i$ predicted to belong to class $j$, where there are $n_{cl}$ different classes, and let $t_i=\sum_jn_{ij}$ be the total number of pixels of class $i$.
&lt;img src=&quot;/img/15876175187934.jpg&quot; width=&quot;40%&quot; height=&quot;50%&quot; /&gt;&lt;/p&gt;
&lt;h5 id=&quot;结果&quot;&gt;结果&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15876184066437.jpg&quot; width=&quot;50%&quot; height=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;使用classification的网络参数作为预训练模型，在此基础上fine-tune网络以用于semantic segmentation任务。&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 22 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/22/Fully-Convolutional-Networks-for-Semantic-Segmentation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/22/Fully-Convolutional-Networks-for-Semantic-Segmentation/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>semantic segmentation dataset summary</title>
        <description>&lt;p&gt;部分内容来自于 https://blog.csdn.net/MOU_IT/article/details/82225505&lt;/p&gt;

&lt;h4 id=&quot;总述&quot;&gt;总述&lt;/h4&gt;
&lt;p&gt;目前学术界主要有三个benchmark（数据集）用于模型训练和测试。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Pascal VOC系列
    &lt;ul&gt;
      &lt;li&gt;VOC 2012 (最常见)&lt;/li&gt;
      &lt;li&gt;Pascal Context (偶尔)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Microsoft COCO&lt;br /&gt;
 COCO一共有80个类别。虽然有很详细的像素级别的标注，但由于官方没有专门对语义分割的测评，因此COCO数据集往往被当成是额外的训练数据集用于模型的训练（预训练？）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Cityscapes
    &lt;ul&gt;
      &lt;li&gt;辅助驾驶（自动驾驶）环境&lt;/li&gt;
      &lt;li&gt;使用比较常见的19个类别用于评测&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;voc-2012&quot;&gt;VOC 2012&lt;/h4&gt;

&lt;p&gt;标准的VOC2012数据集有21个类别(包括背景)，包含:{ 0=background，1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car , 8=cat, 9=chair, 10=cow, 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor，255= ’void’ or unlabelled }这些比较常见的类别。&lt;/p&gt;

&lt;p&gt;对于分割任务：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;VOC 2012用于分割的数据中train+val包含 2007-2011年间的所有数据，test包含2008-2011年间的数据，没有包含07年的是因为07年的test数据已经公开了。&lt;/li&gt;
  &lt;li&gt;trainval：2913张图片，共6929个物体&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;补充：&lt;a href=&quot;https://arleyzhang.github.io/articles/1dc20586/&quot;&gt;PASCAL VOC详细介绍&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;ms-coco&quot;&gt;MS COCO&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;官方没有semantic segmentation测评，因此该数据集常常只用于预训练&lt;/li&gt;
  &lt;li&gt;数据集大小？&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;cityscapes&quot;&gt;Cityscapes&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;由奔驰主推，提供无人驾驶环境下的图像分割数据集，用于评估视觉算法在城区场景语义理解方面的性能。&lt;/li&gt;
  &lt;li&gt;主要包含来自50个不同城市的街道场景，拥有5000张fine annotated images，20000张coarse annotated images。5000张fine annotated images中，2975张训练图，500张验证图和1525张测试图，每张图片大小都是1024x2048。（一般用5000张fine annotated images来训练）&lt;/li&gt;
  &lt;li&gt;分为category(大类)和class(小类)。evaluation过程中，会忽略太不常见的classes，即剩余19个classes。
&lt;img src=&quot;/img/15875266001958.jpg&quot; alt=&quot;-w721&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;补充: &lt;a href=&quot;https://niecongchong.github.io/2019/08/10/CityScapes%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AE%80%E4%BB%8B%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E7%B2%BE%E5%BA%A6%E6%8C%87%E6%A0%87/&quot;&gt;Cityscapes详细介绍&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 21 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/21/semantic-segmentation-datasets/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/21/semantic-segmentation-datasets/</guid>
        
        <category>Datasets</category>
        
        
      </item>
    
  </channel>
</rss>
