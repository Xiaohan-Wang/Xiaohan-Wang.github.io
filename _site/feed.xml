<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 23 Apr 2020 01:10:30 -0400</pubDate>
    <lastBuildDate>Thu, 23 Apr 2020 01:10:30 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>semantic segmentation dataset summary</title>
        <description>&lt;p&gt;部分内容来自于 https://blog.csdn.net/MOU_IT/article/details/82225505&lt;/p&gt;

&lt;h4 id=&quot;总述&quot;&gt;总述&lt;/h4&gt;
&lt;p&gt;目前学术界主要有三个benchmark（数据集）用于模型训练和测试。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Pascal VOC系列
    &lt;ul&gt;
      &lt;li&gt;VOC 2012 (最常见)&lt;/li&gt;
      &lt;li&gt;Pascal Context (偶尔)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Microsoft COCO&lt;br /&gt;
 COCO一共有80个类别。虽然有很详细的像素级别的标注，但由于官方没有专门对语义分割的测评，因此COCO数据集往往被当成是额外的训练数据集用于模型的训练（预训练？）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Cityscapes
    &lt;ul&gt;
      &lt;li&gt;辅助驾驶（自动驾驶）环境&lt;/li&gt;
      &lt;li&gt;使用比较常见的19个类别用于评测&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;voc-2012&quot;&gt;VOC 2012&lt;/h4&gt;

&lt;p&gt;标准的VOC2012数据集有21个类别(包括背景)，包含:{ 0=background，1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car , 8=cat, 9=chair, 10=cow, 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor，255= ’void’ or unlabelled }这些比较常见的类别。&lt;/p&gt;

&lt;p&gt;对于分割任务：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;VOC 2012用于分割的数据中train+val包含 2007-2011年间的所有数据，test包含2008-2011年间的数据，没有包含07年的是因为07年的test数据已经公开了。&lt;/li&gt;
  &lt;li&gt;trainval：2913张图片，共6929个物体&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;补充：&lt;a href=&quot;https://arleyzhang.github.io/articles/1dc20586/&quot;&gt;PASCAL VOC详细介绍&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;ms-coco&quot;&gt;MS COCO&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;官方没有semantic segmentation测评，因此该数据集常常只用于预训练&lt;/li&gt;
  &lt;li&gt;数据集大小？&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;cityscapes&quot;&gt;Cityscapes&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;由奔驰主推，提供无人驾驶环境下的图像分割数据集，用于评估视觉算法在城区场景语义理解方面的性能。&lt;/li&gt;
  &lt;li&gt;主要包含来自50个不同城市的街道场景，拥有5000张fine annotated images，20000张coarse annotated images。5000张fine annotated images中，2975张训练图，500张验证图和1525张测试图，每张图片大小都是1024x2048。（一般用5000张coarse annotated images来训练）&lt;/li&gt;
  &lt;li&gt;分为category(大类)和class(小类)。evaluation过程中，会忽略太不常见的classes，即剩余19个classes。
&lt;img src=&quot;/img/15875266001958.jpg&quot; alt=&quot;-w721&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;补充: &lt;a href=&quot;https://niecongchong.github.io/2019/08/10/CityScapes%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AE%80%E4%BB%8B%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E7%B2%BE%E5%BA%A6%E6%8C%87%E6%A0%87/&quot;&gt;Cityscapes详细介绍&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 21 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/21/semantic-segmentation-datasets/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/21/semantic-segmentation-datasets/</guid>
        
        <category>Datasets</category>
        
        
      </item>
    
      <item>
        <title>AutoDIAL:Automatic DomaIn Alignment Layers</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.08082.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/ducksoup/autodial&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;limitation of AdaBN: the target samples have no influence on the network parameters, as they are not observed during training.&lt;/li&gt;
  &lt;li&gt;对训练过程描述的很详细，可以参考&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15873547625687.jpg&quot; alt=&quot;-w821&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15873551722703.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;img src=&quot;/img/15873552325041.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;img src=&quot;/img/15873552520236.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
其中， $\alpha$ 在[0.5, 1]中：1对应AdaBN (full degree of DA)；0.5对应没有DA，因为此时$q_\alpha^{st}$和$q_\alpha^{ts}$相同，即source和target domain会做相同的transformation。通过让网络自己学习参数$\alpha$，实现了automatically learn the degree of alignment that should be pursued at different levels of the network.&lt;/p&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;p&gt;Bayes based training process:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;objective:&lt;br /&gt;
&lt;img src=&quot;/img/15873556873967.jpg&quot; width=&quot;45%&quot; height=&quot;100%&quot; /&gt;&lt;img src=&quot;/img/15873557363954.jpg&quot; width=&quot;30%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;sampling distribution&lt;br /&gt;
&lt;img src=&quot;/img/15873558553822.jpg&quot; width=&quot;35%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;prior
&lt;img src=&quot;/img/15873559027282.jpg&quot; width=&quot;45%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/15873559171140.jpg&quot; width=&quot;45%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;转化为training objective：
&lt;img src=&quot;/img/15873559673684.jpg&quot; width=&quot;45%&quot; height=&quot;100%&quot; /&gt;  &lt;br /&gt;
source domain用standard cross entropy loss， target domain用entropy minimization.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;$\alpha$ 大部分接近1.&lt;br /&gt;
&lt;img src=&quot;/img/15873565357251.jpg&quot; alt=&quot;-w907&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;others:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;the entropy regularization term is especially beneficial when source and target data representations are aligned. (?)&lt;/li&gt;
  &lt;li&gt;lower layers in a network are subject to domain shift even more than the very last layers.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;reflection&quot;&gt;Reflection&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;同时结合了AdaBN和entropy minimization， 不确定结果的提高是否来源于提出的DA layer.&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 15 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/15/AutoDIAL-Automatic-DomaIn-Alignment-Layers/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/15/AutoDIAL-Automatic-DomaIn-Alignment-Layers/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1712.02560.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/mil-tokyo/MCD_DA&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;if only consider distribution matching (based on statistics), a trained generator can generate ambiguous features near class boundaries (在class boundary附近，由于缺少训练数据，相当于随机分类)
&lt;img src=&quot;/img/15873584186697.jpg&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For better distribution alignment:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;maximize the discrepancy between two classifiers’ outputs to detect target samples that are far from the support of the source.&lt;/li&gt;
  &lt;li&gt;A feature generator learns to generate target features near the support to minimize the discrepancy, in order to avoid generating target features near class boundaries.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15873586517441.jpg&quot; alt=&quot;-w775&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;discrepancy-loss&quot;&gt;discrepancy loss&lt;/h5&gt;
&lt;p&gt;use the absolute values of the difference between the two classifiers’ probabilistic outputs as discrepancy loss.
&lt;img src=&quot;/img/15873624804347.jpg&quot; width=&quot;30%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;training-step&quot;&gt;Training step&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;task-specific loss&lt;br /&gt;
train both classifiers and generator to
classify the source samples correctly. In order to make classifiers and generator obtain task-specific discriminative features, this step is crucial.&lt;br /&gt;
&lt;img src=&quot;/img/15873636378997.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15873636851548.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;train classifier&lt;br /&gt;
A classification loss on the source samples is added here. Without this loss, the authors experimentally found that our algorithm’s performance drops significantly.
&lt;img src=&quot;/img/15873638777096.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;train generator
&lt;img src=&quot;/img/15873639472583.jpg&quot; width=&quot;15%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;reflection&quot;&gt;Reflection&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;distribution alignment也可以用adversarial learning来实现。adversarial learning自由度更大，相比于传统的基于参数的hand-crafted transformation， 也许更能实现理想的distribution alignment.&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Fri, 10 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/10/Maximum-classifier-discrepancy-for-unsupervised-domain-adaptation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/10/Maximum-classifier-discrepancy-for-unsupervised-domain-adaptation/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>t-SNE dimension reduction</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;t-SNE was proposed in 2008, which is s a very powerful and state-of-the-art dimension reduction algorithm, and is especially good at data visualization.&lt;/p&gt;

&lt;p&gt;PCA: perseve global structure (principle component)
t-SNE: preverse local structure or global structure&lt;/p&gt;

&lt;p&gt;https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2899/neighborhood-of-a-point-embedding/2/module-2-data-science-exploratory-data-analysis-and-data-visualization&lt;/p&gt;

</description>
        <pubDate>Fri, 03 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/03/t-SNE%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/03/t-SNE%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/</guid>
        
        <category>ML</category>
        
        
      </item>
    
      <item>
        <title>Backbones</title>
        <description>&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;paper&lt;/th&gt;
      &lt;th&gt;code&lt;/th&gt;
      &lt;th&gt;comment&lt;/th&gt;
      &lt;th&gt;experiment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;AlexNet&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://xiaohan-wang.github.io/2020/03/27/AlexNet/&quot;&gt;link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Inception-BN&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VGG-16&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Thu, 02 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/02/Backbones/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/02/Backbones/</guid>
        
        <category>Backbones</category>
        
        <category>CV paperlist</category>
        
        
      </item>
    
      <item>
        <title>DA dataset summary</title>
        <description>&lt;h3 id=&quot;image-classification&quot;&gt;Image classification&lt;/h3&gt;
&lt;h4 id=&quot;numbers&quot;&gt;Numbers&lt;/h4&gt;
&lt;h5 id=&quot;mnist&quot;&gt;Mnist&lt;/h5&gt;
&lt;h5 id=&quot;usps&quot;&gt;USPS&lt;/h5&gt;
&lt;h5 id=&quot;svhn-street-view-house-numbers&quot;&gt;SVHN (street view house numbers)&lt;/h5&gt;

&lt;h4 id=&quot;office&quot;&gt;Office&lt;/h4&gt;
&lt;h4 id=&quot;office-31&quot;&gt;Office-31&lt;/h4&gt;
&lt;p&gt;It contains 4652 images organized in 31 classes from three different domains: Amazon (A), DSRL (D) and Webcam (W).&lt;/p&gt;
&lt;h4 id=&quot;office-caltech&quot;&gt;Office-Caltech&lt;/h4&gt;
&lt;p&gt;It selects the subset of 10 common categories in the Office31 and the Caltech256 datasets. It contains 2533 images of which about half belong to Caltech256. Each of Amazon (A), DSLR (D), Webcam (W) and Caltech256 (C) are regarded as separate domains.&lt;/p&gt;

&lt;h3 id=&quot;image-segmentation&quot;&gt;Image segmentation&lt;/h3&gt;
&lt;h4 id=&quot;gta5-synthetic-dataset&quot;&gt;GTA5 (synthetic dataset)&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://download.visinf.tu-darmstadt.de/data/from_games/&quot;&gt;homepage&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;synthia-synthetic-data&quot;&gt;synthia (synthetic data)&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;http://synthia-dataset.net/&quot;&gt;homepage&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;cityscapes&quot;&gt;Cityscapes&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cityscapes-dataset.com/&quot;&gt;homepage&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;multi-source-domain-adaptation&quot;&gt;multi-source domain adaptation&lt;/h3&gt;
&lt;h4 id=&quot;domainnet&quot;&gt;DomainNet&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://ai.bu.edu/M3SDA/&quot;&gt;homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Challenge: &lt;a href=&quot;http://ai.bu.edu/visda-2019/&quot;&gt;VisDA-2019&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 02 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/02/DA-dataset-summary/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/02/DA-dataset-summary/</guid>
        
        <category>Datasets</category>
        
        
      </item>
    
      <item>
        <title>Moment Matching for Multi-Source Domain Adaptation</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Moment_Matching_for_Multi-Source_Domain_Adaptation_ICCV_2019_paper.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;http://ai.bu.edu/M3SDA/&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;the upper bound on the target error of the learned hypothesis depends on the pairwise moment divergence $d_{CM^k}(D_S, D_T)$ between the target domain and each source domain. (This provides a direct motivation for moment matching approaches)&lt;/li&gt;
  &lt;li&gt;The last term target error is lower bounded by the pairwise divergences between source domains (implies we might also need to minimize divergence between source domains)&lt;/li&gt;
  &lt;li&gt;DomainNet: multi-souce domain adaptation dataset&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15857182767863.jpg&quot; alt=&quot;-w909&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;moment distance based loss:&lt;br /&gt;
&lt;img src=&quot;/img/15857184320957.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;overall training objective:&lt;br /&gt;
&lt;img src=&quot;/img/15857185193833.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;In order to align $p(y;x)$ and $p(x)$ at the same time, they follow the training paradigm proposed in [1]. They leverage two classifiers per domain to form N pairs of classifiers $C′ = {(C1, C1′), (C2, C2′), …, (CN, CN ′)}$. The training procedure includes three steps.
    &lt;ol&gt;
      &lt;li&gt;train $G$ and $C′$ to classify the multi-source samples correctly based on the overall training objective.&lt;/li&gt;
      &lt;li&gt;train the classifier pairs for a fixed $G$. The goal is to make the discrepancy of each pair of classifiers as large as possible on the target domain.&lt;/li&gt;
      &lt;li&gt;fix $C′$ and train $G$ to minimize the discrepancy of each classifier pair on the target domain.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15857191596055.jpg&quot; alt=&quot;-w781&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15857190392968.jpg&quot; alt=&quot;-w851&quot; /&gt;个人感觉主要是$M^3SDA-\beta$中的training paradigm在发挥作用。&lt;/p&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), June 2018.&lt;/li&gt;
  &lt;li&gt;Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnab´as P´oczos. Mmd gan: Towards deeper understanding of moment matching network. In Advances in Neural Information Processing Systems, pages 2203–2213, 2017.&lt;/li&gt;
  &lt;li&gt;Yujia Li, Kevin Swersky, and Rich Zemel. Generative mo- ment matching networks. In International Conference on Machine Learning, pages 1718–1727, 2015.&lt;/li&gt;
  &lt;li&gt;Youssef Mroueh, Tom Sercu, and Vaibhava Goel. McGan: Mean and covariance feature matching GAN. In Doina Pre- cup and Yee Whye Teh, editors, Proceedings ofthe 34th In- ternational Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2527– 2535, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2，3，4是Gan based moment matching&lt;/p&gt;

</description>
        <pubDate>Tue, 31 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/31/Moment-Matching-for-Multi-Source-Domain-Adaptation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/31/Moment-Matching-for-Multi-Source-Domain-Adaptation/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Short Summay for Some Papers 1</title>
        <description>&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unsupervised Domain Adaptation with Similarity Learning (CVPR 2018)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/15857245561079.jpg&quot; width=&quot;60%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;a similarity-based classifier in which each image (from either the source or target domain) is compared to a set of prototypes (or centroides). The label associated to the prototype that best matches the query image is given to it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 30 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/30/week-summary/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/30/week-summary/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Boosting Domain Adaptation by Discovering Latent Domains</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://research.mapillary.com/img/publications/CVPR18b.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/mancinimassimiliano/latent_domains_DA&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15854715937974.jpg&quot; width=&quot;40%&quot; height=&quot;60%&quot; /&gt;
第一篇针对classification task的deep learning based multi-source domain adaptation. 本质上是multi-source的AdaBN.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15857203223787.jpg&quot; alt=&quot;-w763&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;a side-output branch is empolyed to predict domain assignment probabilities for each input sample.&lt;/li&gt;
  &lt;li&gt;mDA-layer to renormalize the multi-modal feature distributions&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;domain-discovery&quot;&gt;domain discovery&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;domain prediction branch: sample points at different mDA-layers corresponding to a single input element to the network should share the same probabilities&lt;/li&gt;
  &lt;li&gt;split the network into a domain prediction branch and classification branch at some low level layer: features tend to become increasingly more domain invariant going deeper into the network, meaning that it becomes increasingly harder to compute a sample’s domain as a function of deeper features.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;multi-source-domain-adaptation-layer&quot;&gt;multi-source domain adaptation layer&lt;/h5&gt;
&lt;blockquote&gt;
  &lt;p&gt;Domain Alignment layers: aim to reduce internal domain shift at different levels within the network by re-normalizing features in a domain-dependent way, matching their distributions to a pre-determined one.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$q_{d|x}(d | x_i)$ is the conditional probability of $x_i$ belonging to $d$, given $x_i$&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;compute $\mu$ and $\sigma^2$&lt;br /&gt;
 &lt;img src=&quot;/img/15857223593758.jpg&quot; width=&quot;30%&quot; height=&quot;40%&quot; /&gt;&lt;br /&gt;
 &lt;img src=&quot;/img/15857224509940.jpg&quot; width=&quot;45%&quot; height=&quot;40%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;normalize each sample&lt;br /&gt;
By substituting $w_{i,d}$ for $q_{d|x}(d | x_i)$&lt;br /&gt;
&lt;img src=&quot;/img/15857225154947.jpg&quot; width=&quot;45%&quot; height=&quot;40%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;overall-training-objective&quot;&gt;overall training objective&lt;/h5&gt;
&lt;p&gt;only use source dataset now
&lt;img src=&quot;/img/15857216573544.jpg&quot; width=&quot;45%&quot; height=&quot;40%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;labeled data的分类loss (cross entropy)&lt;/li&gt;
  &lt;li&gt;data with known domain的预测loss (cross entropy)&lt;/li&gt;
  &lt;li&gt;unlabeled data的分类 (minimizing uncertainty)&lt;/li&gt;
  &lt;li&gt;data without domain label的预测loss (minimizing uncertainty)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15857232316372.jpg&quot; width=&quot;50%&quot; height=&quot;60%&quot; /&gt;
&lt;img src=&quot;/img/15857232406316.jpg&quot; width=&quot;50%&quot; height=&quot;60%&quot; /&gt;
在latent domain明显的时候(digit和PACS), model表现更好。但对于office-31, 可能因为很难直接发现latent domain，所以基本没有提高。&lt;/p&gt;

&lt;h4 id=&quot;reflection&quot;&gt;Reflection&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;multi-source DA可以地方可以借鉴single-source的思想，比如说本文用的domain-alignment layer&lt;/li&gt;
  &lt;li&gt;难点在于 好的latent domain discovery&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;further-reference-for-latent-domain-discovery&quot;&gt;Further reference (for latent domain discovery)&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;J. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discovering latent domains for multisource domain adaptation. In ECCV, 2012.&lt;/li&gt;
  &lt;li&gt;B. Gong, K. Grauman, and F. Sha. Reshaping visual datasets for domain adaptation. In NIPS, 2013.&lt;/li&gt;
  &lt;li&gt;C. Xiong, S. McCloskey, S.-H. Hsieh, and J. J. Corso. Latent domains modeling for visual domain adaptation. In AAAI, 2014.&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 29 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/29/Boosting-Domain-Adaptation-by-Discovering-Latent-Domains/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/29/Boosting-Domain-Adaptation-by-Discovering-Latent-Domains/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>AlexNet</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;60 million parameters; 650,000 neurons&lt;/p&gt;

&lt;h4 id=&quot;why-works&quot;&gt;Why works&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;数据集的改变：
    &lt;ul&gt;
      &lt;li&gt;以前， on the order of tens of thousands of images&lt;/li&gt;
      &lt;li&gt;最近，Imagenet；LabelMe&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CNN
    &lt;ul&gt;
      &lt;li&gt;enough learning capacity&lt;/li&gt;
      &lt;li&gt;prior knowledge (assumptions): stationarity of statistics and locality of pixel dependencies&lt;/li&gt;
      &lt;li&gt;compared to NN, CNN has fewer parameters: easier to train&lt;/li&gt;
      &lt;li&gt;GPU and large dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;alexnet&quot;&gt;AlexNet&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Input:
    &lt;ul&gt;
      &lt;li&gt;256*256, 首先reshape将短边变成256，然后center crop&lt;/li&gt;
      &lt;li&gt;预处理：减去training set的mean，将输入数据变为centered raw RGB value&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;network
    &lt;ul&gt;
      &lt;li&gt;采用ReLU：sigmoid/tanh梯度下降所需的训练时间比ReLU慢很多倍&lt;/li&gt;
      &lt;li&gt;多GPU训练：GTX 580 GPU只有3GB momory，限制了网络size。为了能扩大网络size，采用多GPU训练&lt;/li&gt;
      &lt;li&gt;local response normalization: a form of lateral inhibition inspired by the type found in real neurons&lt;/li&gt;
      &lt;li&gt;creating competition  for big activities amongst neuron outputs computed using different kernels&lt;/li&gt;
      &lt;li&gt;overlapping pooling: slightly more difficult to overfit&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Architecture
&lt;img src=&quot;/img/15854655847916.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/15854656864364.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;减少过拟合
    &lt;ul&gt;
      &lt;li&gt;Data augmentation
        &lt;ul&gt;
          &lt;li&gt;train：random crop (224*224) + horizontal reflections (increase the size of training set by a factor of 2048)&lt;/li&gt;
          &lt;li&gt;test：predict with 10 224*224 patches (4 corners + center and horizontal reflection), then average&lt;/li&gt;
          &lt;li&gt;PCA color augmentation: 相当于改变了the intensity and color of the illumination&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Dropout&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;quantitative evaluation&lt;br /&gt;
&lt;img src=&quot;/img/15854658726746.jpg&quot; width=&quot;50%&quot; height=&quot;30%&quot; /&gt;
&lt;img src=&quot;/img/15854658948549.jpg&quot; width=&quot;60%&quot; height=&quot;40%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;qualitative evaluation
&lt;img src=&quot;/img/15854660821423.jpg&quot; alt=&quot;&quot; /&gt;
kernels learned on the first layer: frequency- and orientation-selective kernels, and various colored blobs&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Fri, 27 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/27/AlexNet/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/27/AlexNet/</guid>
        
        <category>CV paperlist</category>
        
        
      </item>
    
  </channel>
</rss>
