<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 28 May 2020 17:32:07 -0400</pubDate>
    <lastBuildDate>Thu, 28 May 2020 17:32:07 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Learning rate scheduler</title>
        <description>
&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;总体来说，学习率呈现“上升——平稳——下降”的规律。&lt;/p&gt;

&lt;h4 id=&quot;warm-up&quot;&gt;Warm up&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;减缓模型在训练初期过拟合mini-batch  &lt;br /&gt;
 个人猜测：
    &lt;ul&gt;
      &lt;li&gt;在第一轮训练的时候（尤其是训练刚开始），模型只见过部分数据，此时梯度大概率是偏离相对全局真正较优的方向的（此时很大概率是过拟合当前小部分数据的方向），因此需要使用较小的学习率，以免对当前小部分数据过拟合，对后面训练造成影响（影响有两种可能：1）需要后面多轮训练来修正之间的偏差；2）偏差太大，后面多轮训练也没用，最后accuracy下降）。&lt;/li&gt;
      &lt;li&gt;当训练了一段时间（比如两轮、三轮）后，模型见过了全部数据，此时梯度基本符合相对全局真正较优的方向，所以可以适当调大学习率。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;有助于保持模型深层的稳定性&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;learning-rate-decay&quot;&gt;Learning rate decay&lt;/h4&gt;
&lt;p&gt;学习率衰减的基本思想是学习率随着训练的进行逐渐衰减，即在开始的时候使用较大的学习率，加快靠近最小值的速度，在后来些时候用较小的学习率，提高稳定性，避免因学习率太大跳过最小值，保证能够收敛到最小值。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;step(固定步长衰减): 每隔step_size个epoch后，lr衰减为原来的gamma倍&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/15906775729941.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;multistep(多步长衰减): 动态步长控制，lr衰减为原来的gamma倍&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/15906776874073.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;poly(多项式衰减): $lr = \text{base_lr} * (1 - \frac{T_{cur}}{T_{max}}) ^ {power} $
    &lt;ul&gt;
      &lt;li&gt;学习率曲线的形状主要由参数 power 的值来控制。当 power = 1 的时候，学习率曲线为一条直线。当 power &amp;lt; 1 的时候，下降速率由慢到快。当 power &amp;gt; 1 的时候，下降速率由快到慢。
 &lt;img src=&quot;/img/15906784236558.jpg&quot; width=&quot;85%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;cosine(余弦衰减): $lr = \frac{1}{2}*\text{base_lr} * \left(1+ \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)$
    &lt;ul&gt;
      &lt;li&gt;余弦值首先缓慢下降，然后加速下降，之后再次缓慢下降。&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/img/15906782692333.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;warm-restart&quot;&gt;Warm restart&lt;/h4&gt;
&lt;p&gt;TODO&lt;/p&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/338066667&quot;&gt;warm up&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lumingdong.cn/setting-strategy-of-gradient-descent-learning-rate.html&quot;&gt;cosine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/39565465&quot;&gt;poly&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/93624972&quot;&gt;step and multistep&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 28 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/28/learning-rate/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/28/learning-rate/</guid>
        
        <category>Model Training</category>
        
        
      </item>
    
      <item>
        <title>batch size与learning rate</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.02677.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;当满足一定条件下，batch_size增大k倍时，lr可以直接相应增大k倍&lt;/li&gt;
  &lt;li&gt;训练初期不满足1)所需的条件，此时lr需要使用warm up&lt;/li&gt;
  &lt;li&gt;underlying loss function受batch size影响。为了不改变underlying loss function，在增大batch_size时，其实并不是改变per-worker sample size n，而是增大the number of workers k&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;linear-scaling-rule&quot;&gt;Linear Scaling Rule&lt;/h4&gt;
&lt;p&gt;设有 k 个mini-batch $B_0, \cdots, B_{k-1}$:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$lr=\eta$, 使用小batch_size $B_j$训练，共训 k 个iteration（$j=0, \cdots, k-1$）：&lt;br /&gt;
 &lt;img src=&quot;/img/15906983171534.jpg&quot; width=&quot;40%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;$lr = \hat{\eta}$, 使用大batch-size $\cup_j{B_j}$训一个iteration：
 &lt;img src=&quot;/img/15906986727563.jpg&quot; width=&quot;40%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1和2对应不同的更新方式，因此不太可能有$\hat w_{t+1} = w_{t+k}$。但是，&lt;strong&gt;如果我们假设$\nabla l(x,w_t) \approx \nabla l(x,w_{t+j})$ for $j &amp;lt; k$，那么在 $\hat \eta =k\eta$ 条件下能推出 $\hat w_{t+1} \approx  w_{t+k}$，即使用不同batch-size得到相似的更新结果&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;warm-up&quot;&gt;Warm up&lt;/h4&gt;
&lt;p&gt;在训练的开始阶段，模型权重迅速改变，不满足linear scaling rule中的条件。此时使用gradual warmup有助于解决训练中出现的问题。&lt;/p&gt;

&lt;h4 id=&quot;batch-normalization&quot;&gt;Batch normalization&lt;/h4&gt;
&lt;p&gt;如果有BN层，那么每个sample的loss其实并不是独立的，而是与其所在的mini-batch有关（因为会根据每个mini-batch的mean/std做normalization）。由于不同的mini-batch size会导致不同的mean/variance statistics distribution，因此改变mini-batch size会改变underlying loss function。&lt;/p&gt;

&lt;p&gt;为了不改变underlying loss function，根据上述分析，在增大mini-batch size时，我们其实不应该改变per-worker sample size n（batch statistics是在每个worker上单独计算的），而是应该增大the number of workers k。通常n=32在大部分数据集和网络上都表现良好。&lt;/p&gt;

</description>
        <pubDate>Thu, 28 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/28/batch-size%E4%B8%8Elearning-rate/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/28/batch-size%E4%B8%8Elearning-rate/</guid>
        
        <category>Model Training</category>
        
        
      </item>
    
      <item>
        <title>Image Preprocessing</title>
        <description>
&lt;p&gt;原文：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced&lt;/li&gt;
  &lt;li&gt;https://freecontent.manning.com/the-computer-vision-pipeline-part-3-image-preprocessing/&lt;/li&gt;
  &lt;li&gt;https://www.codecademy.com/articles/normalization&lt;/li&gt;
  &lt;li&gt;https://www.infoq.cn/article/kyXx3sRKNsdFgqapv2Gw&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;No free lunch theorem for optimization:
In ML project, it means that there’s no single prescribed recipe that is guaranteed to work well in all situations. We must make certain assumption about the dataset and the problem we are trying to solve.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;why-image-preprocessing&quot;&gt;why image preprocessing&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;The acquired data are usually messy and come from different sources, they need to be standardized and cleaned up.&lt;/li&gt;
  &lt;li&gt;We can’t write a unique algorithm for each of the condition in which an image is taken, thus, when we acquire an image, we tend to convert it into a form that allows a general algorithm to solve it.&lt;/li&gt;
  &lt;li&gt;It can reduce the complexity and increase the accuracy of the applied algorithm.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;彩色图像--灰度图像&quot;&gt;彩色图像 / 灰度图像&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;In many objects, color isn’t necessary to recognize and interpret an image. Grayscale can be good enough for recognizing certain objects. Because color images contain more information than black and white images, they can add unnecessary complexity and take up more space in memory.
 &lt;img src=&quot;/img/15904315018825.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;In other applications, color is important to define certain objects. Like skin cancer detection which relies heavily on the skin colors (red rashes).&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;standardize-image&quot;&gt;Standardize Image&lt;/h4&gt;
&lt;h5 id=&quot;reason&quot;&gt;Reason&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;If we didn’t scale our input training vectors, the ranges of our distributions of feature values would likely be different for each feature, and thus the learning rate would &lt;strong&gt;cause corrections in each dimension that would differ (proportionally speaking) from one another&lt;/strong&gt;. We might be over compensating a correction in one weight dimension while undercompensating in another.&lt;/li&gt;
  &lt;li&gt;In the process of training our network, we’re going to be multiplying (weights) and adding to (biases) these initial inputs in order to cause activations that we then backpropogate with the gradients to train the model. We’d like in this process for each feature to have a similar range so that our &lt;strong&gt;gradients don’t go out of control&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;ways&quot;&gt;ways&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;Min-max normalization $\frac{\text{value}-\text{min}}{\text{max}-\text{min}}$: Guarantees all features will have the exact same scale but does not handle outliers well.&lt;br /&gt;
 &lt;img src=&quot;/img/15904331747160.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
 Normalizing fixed the squishing problem on the y-axis, but the x-axis is still problematic. Now if we were to compare these points, the y-axis would dominate; the y-axis can differ by 1, but the x-axis can only differ by 0.4.&lt;/li&gt;
  &lt;li&gt;Z-score normalization $\frac{value-mean}{std}$: Handles outliers, but does not produce normalized data with the exact same scale.&lt;br /&gt;
 &lt;img src=&quot;/img/15904332447812.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
 While the data still looks squished, notice that the points are now on roughly the same scale for both features — almost all points are between -2 and 2 on both the x-axis and y-axis.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h4&gt;

&lt;h5 id=&quot;分类&quot;&gt;分类&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;有监督的数据增强：采用预设的数据变换规则，在已有数据的基础上进行数据的扩增
    &lt;ul&gt;
      &lt;li&gt;单样本数据增强：增强一个样本的时候，全部围绕着该样本本身进行操作
        &lt;ul&gt;
          &lt;li&gt;几何操作类：对图像进行几何变换，包括翻转，旋转，裁剪，变形，缩放等各类操作
            &lt;ul&gt;
              &lt;li&gt;可能会引入图像边界之外的位置，导致没有图像没有覆盖的黑色区域，此时可以通过常数、边缘、反射、对称等方式填充未知区域&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;颜色变换类：改变图像本身的内容，包括噪声、模糊、颜色变换（HSV，对比度变换）、擦除、填充
            &lt;ul&gt;
              &lt;li&gt;高斯噪声：当神经网络试图学习可能并无用处的高频特征时（即频繁发生的无意义模式），常常会发生过拟合。具有零均值特征的高斯噪声本质上就是在所有频率上都有数据点，能有效使得高频特征失真，减弱它对模型的影响。这也意味着低频成分（通常也是我们关心的数据）也会失真，但神经网络能够通过学习忽略这部分影响。添加正确数量的噪声就能增强神经网络的学习能力。&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;多样本数据增强方法：利用多个样本来产生新的样本
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.jair.org/index.php/jair/article/view/10302/24590&quot;&gt;SMOTE&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1801.02929.pdf&quot;&gt;SamplePairing&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1710.09412.pdf&quot;&gt;mixup&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;SMOTE，SamplePairing，mixup 三者思路上有相同之处，都是试图将离散样本点连续化来拟合真实样本分布，不过所增加的样本点在特征空间中仍位于已知小样本点所围成的区域内。如果能够在给定范围之外适当插值，也许能实现更好的数据增强效果。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;无监督的数据增强方法
    &lt;ul&gt;
      &lt;li&gt;生成新的数据: 通过模型学习数据的分布，随机生成与训练数据集分布一致的图片，代表方法 &lt;a href=&quot;https://arxiv.org/pdf/1406.2661.pdf&quot;&gt;GAN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;学习增强策略: 通过模型，学习出适合当前任务的数据增强方法，代表方法 &lt;a href=&quot;https://arxiv.org/pdf/1805.09501.pdf&quot;&gt;AutoAugment&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;offline--online-augmentation&quot;&gt;Offline / Online augmentation&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;Offline augmentation:
    &lt;ul&gt;
      &lt;li&gt;事先进行所有必需的图像平移工作，基本上就是增加数据集大小&lt;/li&gt;
      &lt;li&gt;当数据集相对较小时，优先选择这种方法，因为会将数据集增大N倍（N=执行的转换数量）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Online augmentation:
    &lt;ul&gt;
      &lt;li&gt;将图像输入机器学习模型之前，以小批量进行图像平移&lt;/li&gt;
      &lt;li&gt;比较适合数据集较大时的情况，因为我们很难应付数据集爆炸性变大。相反，我们可以小批量平移输入到模型中的图像。有些机器学习框架支持在线增强，使用GPU可以加快增强速度。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Mon, 25 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/25/image-preprocessing/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/25/image-preprocessing/</guid>
        
        <category>Model Training</category>
        
        
      </item>
    
      <item>
        <title>Identity mapping</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1603.05027.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;The forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as 1) the skip connections and 2) after-addition activation. This makes training easier and improves generalization.&lt;/p&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;Residual unit:
&lt;img src=&quot;/img/15902038485632.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$x_l$ and $x_{l+1}$ are input and output of the $l$-th unit&lt;/li&gt;
  &lt;li&gt;$F$ is a residual function&lt;/li&gt;
  &lt;li&gt;$h(x_l) $ is an skip connection&lt;/li&gt;
  &lt;li&gt;$f$ is an after-addition activation function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;If (i) skip connection is identity mapping $h(x_l) = x_l$, and (ii) after-addition activation $f$ is an identity mapping, then we have
&lt;img src=&quot;/img/15902041544783.jpg&quot; width=&quot;30%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15902041725422.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;
that is, the signal can be directly propagated from any unit to another, both forward and backward.&lt;/strong&gt;&lt;/p&gt;

&lt;h5 id=&quot;identity-skip-connections&quot;&gt;Identity Skip Connections&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15902046739782.jpg&quot; width=&quot;81%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15902049097732.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As indicated by the grey arrows in Fig. 2, the shortcut connections are the most direct paths for the information to propagate. Multiplicative manipulations (scaling, gating, 1×1 convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems.&lt;/p&gt;

&lt;h5 id=&quot;activation-functions&quot;&gt;Activation Functions&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15902048739817.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15902048935542.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;(a) original residual unit&lt;/li&gt;
  &lt;li&gt;(b) BN after addition
    &lt;ul&gt;
      &lt;li&gt;the BN layer alters the signal that passes through the shortcut and impedes information propagation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(c) ReLU before addition
    &lt;ul&gt;
      &lt;li&gt;a non-negative output from the transform $F$, while intuitively a “residual” function should take values in $(-\infty, +\infty)$. As a result, the forward propagated signal is monotonically increasing, which may impact the representational ability.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;pre-activation: develop an asymmetric form where an activation $f$ only affect the residual path but not the both paths in the next Residual Unit (i.e. move the activation module in a) and b) to the residual path, which become d) and e))
    &lt;ul&gt;
      &lt;li&gt;(d) This ReLU layer is not used in conjunction with a BN layer, and may not enjoy the benefits of BN.&lt;/li&gt;
      &lt;li&gt;(e)  result is good&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Benefit of pre-activation&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Ease of optimization
    &lt;ul&gt;
      &lt;li&gt;the benefit of ease of optimization is more obvious in 1001-layer network than networks with fewer layers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reducing overfitting
    &lt;ul&gt;
      &lt;li&gt;The pre-activation version reaches slightly higher training loss at convergence, but produces lower test error. This phenomenon is observed on ResNet-110, ResNet-110(1-layer), and ResNet-164 on both CIFAR-10 and 100.&lt;/li&gt;
      &lt;li&gt;In the original Residual Unit (Fig. 4(a)), although the BN normalizes the signal, this is soon added to the shortcut and thus the merged signal is not normalized. This unnormalized signal is then used as the input of the next weight layer. On the contrary, in the pre-activation version, the inputs to all weight layers have been normalized.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 22 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/22/Identity-mapping/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/22/Identity-mapping/</guid>
        
        <category>Baseline</category>
        
        
      </item>
    
      <item>
        <title>PyTorch模型检查/可视化</title>
        <description>&lt;h4 id=&quot;print&quot;&gt;print&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;c1&quot;&gt;# 打印所有子模块，但不能显示各个模块间的关系
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_ASPP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backbone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'resnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;named_children&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;':'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/15898472827399.jpg&quot; alt=&quot;-w817&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;模型可视化&quot;&gt;模型可视化&lt;/h4&gt;

&lt;h5 id=&quot;torchviz--graphviz&quot;&gt;torchviz + graphviz&lt;/h5&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchviz&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 加载并运行模型
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_ASPP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backbone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'resnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 构造图对象，保存为pdf
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;torchviz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'aspp'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/15898469921259.jpg&quot; alt=&quot;-w1157&quot; /&gt;&lt;/p&gt;
&lt;h5 id=&quot;netron&quot;&gt;netron&lt;/h5&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lutzroeder/Netron&quot;&gt;github link&lt;/a&gt; ，超有潜力，但目前还不能稳定支持PyTorch&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;netron&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_ASPP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backbone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'resnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;# 保存模型
&lt;/span&gt;   
    &lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;aspp.pth&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 用python server打开
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;netron&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/img/15898517608333.jpg&quot; alt=&quot;-w1008&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;tensorboard&quot;&gt;tensorboard&lt;/h5&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.tensorboard&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SummaryWriter&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_ASPP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backbone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'resnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 保存模型结构，在当前文件夹下生成runs
&lt;/span&gt;    
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SummaryWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'aspp'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;通过&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorboard --logdir=runs&lt;/code&gt;, 可以在浏览器打开。但感觉不太直观，有点看不懂&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15898488137991.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;tensorwatch&quot;&gt;tensorwatch&lt;/h5&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorwatch&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tw&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_ASPP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backbone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'resnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;draw_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;只能在jupyter notebook上运行，但生成的结果是横屏，很乱，就不贴图了&lt;/p&gt;
</description>
        <pubDate>Mon, 18 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/18/pytorch%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/18/pytorch%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96/</guid>
        
        <category>PyTorch</category>
        
        
      </item>
    
      <item>
        <title>代码管理、实验管理</title>
        <description>&lt;p&gt;目前对代码管理以及实验管理的一些小想法，部分来自&lt;a href=&quot;https://www.zhihu.com/question/269707221&quot;&gt;知乎&lt;/a&gt;，随时修改…&lt;/p&gt;

&lt;h4 id=&quot;项目代码&quot;&gt;项目代码&lt;/h4&gt;
&lt;h5 id=&quot;before-starting&quot;&gt;before starting&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;实现前先思考代码的整体框架和结构、各部分功能，可以用xmind做一个简单思维导图&lt;/li&gt;
  &lt;li&gt;尽量解耦代码，从而获得更高的复用性
    &lt;ul&gt;
      &lt;li&gt;实现一个基础整体框架 e.g. &lt;a href=&quot;https://github.com/MrGemy95/Tensorflow-Project-Template&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;单独实现各个模块&lt;/li&gt;
      &lt;li&gt;组合代码，同时添加一些单独的细节部分&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;tricks&quot;&gt;tricks&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;分离代码和数据
    &lt;ul&gt;
      &lt;li&gt;最好假设它们不在一个文件夹&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;试验参数
    &lt;ul&gt;
      &lt;li&gt;尽量使用config文件传入，并且config尽量与log文件同名保存。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;train, val, test set各自的数据最好放在各自的文件夹中&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;实验管理&quot;&gt;实验管理&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;使用&lt;a href=&quot;https://mlflow.org/docs/latest/index.html&quot;&gt;MLflow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;实验记录&quot;&gt;实验记录&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;内容
    &lt;ul&gt;
      &lt;li&gt;记录每个实验的目的、方法、结果&lt;/li&gt;
      &lt;li&gt;尽量附代码 （用代码的 commit id）、数据&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;工具
    &lt;ul&gt;
      &lt;li&gt;最好用overleaf，可以与github repo 或者本地 git repo 同步 【TODO】&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 15 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/15/Patterns-for-Research/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/15/Patterns-for-Research/</guid>
        
        <category>Q &amp; A</category>
        
        
      </item>
    
      <item>
        <title>MobileNets</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.04861.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;MobileNets: light weight deep neural networks that use depth-wise separable convolutions&lt;/li&gt;
  &lt;li&gt;two simple global hyper-parameters that efficiently trade off between latency and accuracy&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;h5 id=&quot;depth-separable-convolution&quot;&gt;Depth separable convolution&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895586236484.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;depthwise convolution: apply a single filter per each input channel&lt;/li&gt;
  &lt;li&gt;pointwise convolution: a simple 1×1 convolution to create a linear combination of the output of the depthwise layer&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;MobileNets use both batchnorm and ReLU nonlinearities for both layers.
&lt;img src=&quot;/img/15895586917635.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;computation cost
    &lt;ul&gt;
      &lt;li&gt;symbols
        &lt;ul&gt;
          &lt;li&gt;$D_F$: input feature size / output feature size&lt;/li&gt;
          &lt;li&gt;$D_K$: kernel size&lt;/li&gt;
          &lt;li&gt;$M$: input channels&lt;/li&gt;
          &lt;li&gt;$N$: outout channels&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;standard convolutions&lt;br /&gt;
  &lt;img src=&quot;/img/15895585271744.jpg&quot; width=&quot;33%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;depthwise separable convolutions&lt;br /&gt;
  &lt;img src=&quot;/img/15895588519425.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;reduction in computation: MobileNet uses 3 × 3 depthwise separable convolutions, which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy&lt;br /&gt;
  &lt;img src=&quot;/img/15895588852425.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;network-structure&quot;&gt;Network structure&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895692454944.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The first layer is a full convolution.&lt;/li&gt;
  &lt;li&gt;All layers are followed by a batchnorm and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification&lt;/li&gt;
  &lt;li&gt;Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer.&lt;/li&gt;
  &lt;li&gt;A final average pooling reduces the spatial resolution to 1 before the fully connected layer.&lt;/li&gt;
  &lt;li&gt;Counting depthwise and pointwise convo- lutions as separate layers, MobileNet has 28 layers.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;MobileNets structure puts nearly all of the computation into dense 1×1 convolutions. This can be implemented with highly optimized general matrix multiply (GEMM) functions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;tricks&quot;&gt;Tricks&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;contrary to training large models we use less regularization and data augmentation techniques because small models have less trouble with overfitting.&lt;/li&gt;
  &lt;li&gt;it was important to put very little or no weight decay (L2 regularization) on the depthwise filters since their are so few parameters in them.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;hyperparameter&quot;&gt;Hyperparameter&lt;/h4&gt;
&lt;h5 id=&quot;width-multiplier-thinner-models&quot;&gt;Width Multiplier: Thinner Models&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;The role of the width multiplier $\alpha$ is to thin a network uniformly at each layer (the number of input channels $M$ becomes $\alpha M$,  and the number of output channels $N$ becomes $\alpha N$)&lt;/li&gt;
  &lt;li&gt;The computational cost of a depthwise separable convolution with width multiplier $\alpha$:
  &lt;img src=&quot;/img/15895698351332.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;$ \alpha \in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25.&lt;/li&gt;
  &lt;li&gt;Width multiplier has the effect of reducing &lt;strong&gt;computational cost&lt;/strong&gt; and &lt;strong&gt;the number of parameters&lt;/strong&gt; quadratically by roughly $\alpha^2$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;resolution-multiplier-reduced-representation&quot;&gt;Resolution Multiplier: Reduced Representation&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;resolution multiplier $\rho$ is applied to the input image and the internal representation of every layer. In practice we implicitly set $\rho$ by setting the input resolution.&lt;/li&gt;
  &lt;li&gt;the computational cost for the core layers with width multiplier $\alpha$ and resolution multiplier $\alpha$:
  &lt;img src=&quot;/img/15895703362678.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;$\rho \in (0, 1]$, which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128.&lt;/li&gt;
  &lt;li&gt;Resolution multiplier has the effect of reducing &lt;strong&gt;computational cost&lt;/strong&gt; by $\rho^2$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895706506529.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 15 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/15/MobileNets/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/15/MobileNets/</guid>
        
        <category>Baseline</category>
        
        
      </item>
    
      <item>
        <title>DeepLab v3+</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.02611.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/jfzhang95/pytorch-deeplab-xception&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;encoder-decoder structure:
    &lt;ul&gt;
      &lt;li&gt;DeepLabv3 is used to encode the rich contextual information&lt;/li&gt;
      &lt;li&gt;a simple yet effective decoder module is adopted to recover the object boundaries&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;explore the Xception model and atrous separable convolution to make the proposed model faster and stronger&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;introduction&quot;&gt;Introduction&lt;/h4&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;h5 id=&quot;resnet-as-baseline&quot;&gt;ResNet as baseline&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895024156855.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;DeepLab v3 as encoder: use the last feature map before logits in the original DeepLab v3 as the encoder output (contains 256 channels and rich semantic information).&lt;/li&gt;
  &lt;li&gt;Decoder:
    &lt;ul&gt;
      &lt;li&gt;The encoder features from DeepLabv3 are usually computed with output stride = 16&lt;/li&gt;
      &lt;li&gt;first bilinearly upsampled the encoder features by a factor of 4&lt;/li&gt;
      &lt;li&gt;concatenate the upsampled features with the corresponding low-level features from the network backbone that have the same spatial resolution (e.g., Conv2 before striding in ResNet-101)
        &lt;ul&gt;
          &lt;li&gt;1 × 1 convolution on the low-level features to reduce the number of channels, since the corresponding low- level features usually contain a large number of channels (e.g., 256 or 512) which may outweigh the importance of the rich encoder features (only 256 channels in our model) and make the training harder.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;apply a few (two) 3 × 3 convolutions to refine the features&lt;/li&gt;
      &lt;li&gt;another simple bilinear upsampling by a factor of 4&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;DeepLabv3+ model has final output stride = 4
    &lt;ul&gt;
      &lt;li&gt;encoder output stride = 16
        &lt;ul&gt;
          &lt;li&gt;upsample by a factor of 4&lt;/li&gt;
          &lt;li&gt;concatenate with Conv2&lt;/li&gt;
          &lt;li&gt;another upsample by a factor of 4&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;encoder output stride = 8
        &lt;ul&gt;
          &lt;li&gt;upsample by a factor of 2&lt;/li&gt;
          &lt;li&gt;concatenate with Conv2&lt;/li&gt;
          &lt;li&gt;another upsample by a factor of 4&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;modified-aligned-xception&quot;&gt;Modified Aligned Xception&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895033728737.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;h5 id=&quot;resnet-as-baseline-1&quot;&gt;ResNet as baseline&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;reduce the channels of low level features
&lt;img src=&quot;/img/15895034376564.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;number of 3x3 convolutions / low level feature
&lt;img src=&quot;/img/15895034547360.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;detailed performance
&lt;img src=&quot;/img/15895035126473.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;xception&quot;&gt;Xception&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895037132342.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;employing Xception as network backbone improves the performance by about 2% when train output stride = eval output stride = 16 over the case where ResNet-101 is used&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;improvement-on-object-boundaries&quot;&gt;improvement on object boundaries&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895038788124.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Chollet, F.: Xception: Deep learning with depthwise separable convolutions. In: CVPR. (2017)&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 14 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/14/Deeplab-v3+/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/14/Deeplab-v3+/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>PSPNet</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1612.01105.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Observations:&lt;/strong&gt; many errors are partially or completely related to contextual relationship and global information for different receptive field.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Solution:&lt;/strong&gt; exploit the capability of global context information by different-region-based context aggregation.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;h5 id=&quot;observation-about-failures&quot;&gt;observation about failures&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894252749235.jpg&quot; alt=&quot;-w933&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;doesn’t match co-occurrent visual patterns&lt;/li&gt;
  &lt;li&gt;confused by similar categories&lt;/li&gt;
  &lt;li&gt;ignore small-size things / discontinuous prediction on large size things&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many errors are partially or completely related to contextual relationship and global information for different receptive fields. Thus a deep network with a suitable global-scene-level prior can much improve the performance of scene parsing.&lt;/p&gt;

&lt;h5 id=&quot;pyramid-scene-parsing-network-pspnet&quot;&gt;pyramid scene parsing network (PSPNet)&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894259020057.jpg&quot; alt=&quot;-w918&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use a pretrained ResNet model with the dilated network strategy to extract the feature map. The final feature map size is 1/8 of the input image, as shown in Fig. 3(b).&lt;/li&gt;
  &lt;li&gt;pyramid pooling module for global scene prior construction upon the final-layer-feature-map of the deep neural network.&lt;/li&gt;
  &lt;li&gt;Use 1×1 convolution layer after each pyramid level to reduce the dimension of context representation to $\frac{1}{N}$ of the original one if the level size of pyramid is $N$. Then we directly upsample the low-dimension feature maps to get the same size feature as the original feature map via bilinear interpolation.&lt;/li&gt;
  &lt;li&gt;Different levels of features are concatenated as the final pyramid pooling global feature. It is followed by a convolution layer to generate the final prediction map in (d).&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;deep-supervised-loss&quot;&gt;Deep supervised loss&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894269590978.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Generating initial results by supervision with an additional loss, and learning the residue afterwards with the final loss. Thus, optimization of the deep network is decomposed into two, each is simpler to solve.&lt;/li&gt;
  &lt;li&gt;The auxiliary loss helps optimize the learning process, while the master branch loss takes the most responsibility.&lt;/li&gt;
  &lt;li&gt;In the testing phase, we abandon this auxiliary branch and only use the well optimized master branch for final prediction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;h5 id=&quot;architecture-for-pspnet&quot;&gt;Architecture for PSPNet&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894274392623.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;average pooling works better than max pooling in all settings&lt;/li&gt;
  &lt;li&gt;Pooling with pyramid parsing (B1236) outperforms that using global pooling (B1)&lt;/li&gt;
  &lt;li&gt;With dimension reduction, the performance is further enhanced.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;auxiliary-loss&quot;&gt;Auxiliary Loss&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894275459126.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;pretrained-model&quot;&gt;Pretrained model&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894276146708.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;detailed-analysis&quot;&gt;Detailed analysis&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894276963001.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;others&quot;&gt;Others&lt;/h5&gt;
&lt;p&gt;需要注意各个方法的baseline是不是resnet
&lt;img src=&quot;/img/15894278542140.jpg&quot; alt=&quot;-w923&quot; /&gt;
&lt;img src=&quot;/img/15894279159043.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;B. Zhou, A. Khosla, A.` Lapedriza, A. Oliva, and A. Torralba. Object detectors emerge in deep scene cnns. arXiv:1412.6856, 2014. 3. (The paper shows the empirical receptive field of CNN is much smaller than the theoretical one especially on high-level layers.)&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 13 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/13/PSPNet/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/13/PSPNet/</guid>
        
        <category>Semantic Segmentation</category>
        
        <category>Context information</category>
        
        
      </item>
    
      <item>
        <title>DeepLab v3</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.05587.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;modules employing atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates.&lt;/li&gt;
  &lt;li&gt;主要还是各种tricks&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;output_stride: the ratio of input image spatial resolution to final output resolution.&lt;/p&gt;
&lt;h5 id=&quot;atrous-convolution-in-cascade&quot;&gt;atrous convolution in cascade&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15893990006461.jpg&quot; alt=&quot;-w840&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;duplicate several copies of the last ResNet block (Block 5, Block 6, Block 7)&lt;/li&gt;
  &lt;li&gt;apply atrous convolution with rates determined by the desired output stride value (output stride = 256 if no atrous convolution is applied)&lt;/li&gt;
  &lt;li&gt;multi-grid method: adopt different atrous rates within block4 to block7 in the proposed model. For example, when $\text{output stride} = 16$ and $\text{Multi Grid} = (1, 2, 4)$, the three convolutions will have $rates = 2 · (1, 2, 4) = (2, 4, 8)$ in the block4, respectively.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;atrous-convolution-in-parallel&quot;&gt;atrous convolution in parallel&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15893997101137.jpg&quot; alt=&quot;-w778&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;include batch normalization within ASPP.&lt;/li&gt;
  &lt;li&gt;image level feature: apply global average pooling on the last feature map of the model, feed the resulting image-level features to a 1 × 1 convolution with 256 filters (and batch normalization), and then bilinearly upsample the feature to the desired spatial dimension.&lt;/li&gt;
  &lt;li&gt;the improved ASPP consists of
    &lt;ul&gt;
      &lt;li&gt;one 1×1 convolution, and three 3 × 3 convolutions with rates = (6, 12, 18) when output stride = 16 (all with 256 filters and batch normalization). Note that the rates are doubled when output stride = 8.&lt;/li&gt;
      &lt;li&gt;the image-level features (the rates are doubled when output stride = 8)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The resulting features from all the branches are concatenated and pass through another 1 × 1 convolution (with 256 filters and batch normalization) before the final 1 × 1 convolution which generates the final logits.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;“poly” learning rate policy: the initial learning rate is
multiplied by $(1 −\text{maxiter})^{\text{power}}$ with $\text{power} = 0.9 $.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;upsampling logits: upsample the final logits, since downsampling the groundtruths removes the fine annotations resulting in no back-propagation of details.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;$\text{output stride}=16$ during training, and $\text{output stride}=8$ during inference to get more detailed feature map.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;cascade:
&lt;img src=&quot;/img/15894010210339.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;parallel:
&lt;img src=&quot;/img/15894010431381.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Both their best cascaded model (in Tab. 4) and ASPP model (in Tab. 6) (in both cases without DenseCRF post-processing or MS-COCO pre-training) already outperform DeepLabv2 (77.69% with DenseCRF and pretrained on MS-COCO) on the PASCAL VOC 2012 val set.&lt;/li&gt;
  &lt;li&gt;The improvement mainly comes from including and fine-tuning batch normalization parameters in the proposed models and having a better way to encode multi-scale context.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167, 2015.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 13 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/13/DeepLab-v3/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/13/DeepLab-v3/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
  </channel>
</rss>
