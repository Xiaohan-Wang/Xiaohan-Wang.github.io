<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 05 May 2020 23:22:48 -0400</pubDate>
    <lastBuildDate>Tue, 05 May 2020 23:22:48 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Dilated convolutions</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.07122.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Dilated convolution operator, which can expend the receptive field without losing resolution or coverage, is very suitable for dense prediction task.&lt;/li&gt;
  &lt;li&gt;multi-scale context module, can reliably increases accuracy when plugged into existing semantic segmentation systems.&lt;/li&gt;
  &lt;li&gt;Replace pooling layer (designed for classification task) with dilated convolution (designed for segmentation task) can increase accuracy.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;dilated convolutions
    &lt;ul&gt;
      &lt;li&gt;$l$-dilated convolution: The familiar discrete convolution is simply the 1-dilated convolution.&lt;br /&gt;
 &lt;img src=&quot;/img/15887283730533.jpg&quot; width=&quot;35%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
 use $∗_l$ to represent an $l$-dilated convolution&lt;/li&gt;
      &lt;li&gt;Let $F_0, F_1, \cdots , F_{n−1} : Z^2 \rightarrow R$ be discrete functions and let $k_0, k_1, \cdots, k_{n−2} : Ω_1 \rightarrow R$ ($Ω_r = [−r, r]^2 ∩ Z^2$) be discrete 3×3 filter. Consider applying the filters with exponentially increasing dilation: 
  &lt;script type=&quot;math/tex&quot;&gt;F_{i+1} = F_i ∗_{2^i}k_i\text{   for   } i = 0, 1, \cdots , n − 2&lt;/script&gt;, then the size of the receptive field of each element in $F_{i+1}$ is $(2^{i+2} − 1)×(2^{i+2} − 1)$.
&lt;img src=&quot;/img/15887296456656.jpg&quot; alt=&quot;-w709&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;context module
    &lt;blockquote&gt;
      &lt;p&gt;The context module is designed to increase the performance of dense prediction architectures by aggregating multi-scale contextual information. The module takes C feature maps as input and produces C feature maps as output. The input and output have the same form, thus the module can be plugged into existing dense prediction architectures.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;module architecture&lt;/p&gt;

        &lt;p&gt;here truncation is a ReLU function $f(\cdot)=max(\cdot, 0)$.&lt;br /&gt;
&lt;img src=&quot;/img/15887338289922.jpg&quot; alt=&quot;-w656&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;initialization&lt;/p&gt;

        &lt;p&gt;Experiments revealed that standard initialization procedures (random initialization) do not readily support the training of the module.&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;Basic: identity initialization&lt;br /&gt;
  &lt;img src=&quot;/img/15887343089814.jpg&quot; width=&quot;30%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;Large:&lt;br /&gt;
  &lt;img src=&quot;/img/15887343601296.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;front end
    &lt;ul&gt;
      &lt;li&gt;adapted the VGG-16 network for dense prediction and removed the last two pooling and striding layers. Specifically, each of these pooling and striding layers was removed and convolutions in all subsequent layers were dilated by a factor of 2 for each pooling layer that was ablated.&lt;/li&gt;
      &lt;li&gt;use reflection padding: the buffer zone is filled by reflecting the image about each edge.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Front end
&lt;img src=&quot;/img/15887346292420.jpg&quot; alt=&quot;-w599&quot; /&gt;
&lt;img src=&quot;/img/15887346668224.jpg&quot; alt=&quot;-w609&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;context module
&lt;img src=&quot;/img/15887347716594.jpg&quot; alt=&quot;-w599&quot; /&gt;
&lt;img src=&quot;/img/15887348908756.jpg&quot; alt=&quot;-w606&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 05 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/05/Dilated-convolutions/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/05/Dilated-convolutions/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>U-Net</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1505.04597.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/milesial/Pytorch-UNet&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;contracting path + expending path + skip connection&lt;/li&gt;
  &lt;li&gt;strong data augmentation, works well for very few images (it was designed for biomedical images)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15882170880287.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15882176961707.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It use valid convolution, thus the resolution gets lower and lower in the contracting path and expending path.&lt;/li&gt;
  &lt;li&gt;Because of the high resolution of biomedical images, overlap-tile strategy is used.&lt;/li&gt;
  &lt;li&gt;To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;contracting path:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels.&lt;/li&gt;
  &lt;li&gt;an upsampling of the feature map by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU.&lt;/li&gt;
  &lt;li&gt;At the final layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Training objective:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;softmax + cross entropy&lt;/li&gt;
  &lt;li&gt;class balancing (it’s important because of the large background)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;/h4&gt;
&lt;p&gt;Teach the network the desired invariance and robustness properties, when only few training samples are available:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;shift&lt;/li&gt;
  &lt;li&gt;rotation&lt;/li&gt;
  &lt;li&gt;gray value deformations&lt;/li&gt;
  &lt;li&gt;random elastic deformation&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size.&lt;/li&gt;
  &lt;li&gt;favor large input tiles over a large batch size and hence reduce the batch to a single image.&lt;/li&gt;
  &lt;li&gt;use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step.&lt;/li&gt;
  &lt;li&gt;Xavier initialization.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;time&quot;&gt;Time&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Training time: 10 hours on a NVidia Titan GPU (6 GB)&lt;/li&gt;
  &lt;li&gt;Inference time: Segmentation of a 512x512 image takes less than a second on a recent GPU&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 29 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/29/U-Net/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/29/U-Net/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>Conditional Generative Adversarial Network for Structured Domain Adaptation</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;We shouldn’t assume that the source and target domains share same intermediate feature space(s), i.e. using loss to obtain domain-invariant features.&lt;/li&gt;
  &lt;li&gt;Use a conditional generator to transform source feature maps into target-like, and trained on the target-like feature maps and original labels (和pixel level domain adaptation类似，但pixel level DA将source images转化为target-like，而本文将source features转化为target-like).&lt;/li&gt;
  &lt;li&gt;文章中提到 without relying on the assumption that the source and target domains share a same prediction function in a domain-invariant feature space。 实际只完成了not in a domain-invariant feature space， 本质上还是same prediction function (source和target domain的encoder和decoder都相同)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15881099962969.jpg&quot; alt=&quot;-w913&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;a conditional generator to generate residual features, to transform features of synthetic images to real-image like features.
    &lt;ul&gt;
      &lt;li&gt;a noise map $z$ is addd for randomness to create an unlimited number of training samples.&lt;/li&gt;
      &lt;li&gt;expect that $x_f$ preserves the semantic of the source feature map $x_s$, meanwhile appears as if it were extracted from a target domain image, i.e., a real image.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;a discriminator to distinguish adapted source features and real target features.&lt;/li&gt;
  &lt;li&gt;task-specific loss (decoder part)
    &lt;ul&gt;
      &lt;li&gt;train T with both adapted and non-adapted source feature maps (Training T solely on adapted feature maps leads to similar performance, but requires many runs with different initializations and learning rates due to the instability of the GAN).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;overall minimax objective:&lt;br /&gt;
&lt;img src=&quot;/img/15881108285954.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;overall performance: 涨幅非常大&lt;br /&gt;
&lt;img src=&quot;/img/15881108796823.jpg&quot; alt=&quot;-w905&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;amount of synthetic data
 &lt;img src=&quot;/img/15881109617007.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ablation Studies&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The effectiveness of conditional generator&lt;br /&gt;
 &lt;img src=&quot;/img/15881110275006.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Different lower layers for learning generator
 &lt;img src=&quot;/img/15881111038400.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;On the number of residual blocks （上图）&lt;/li&gt;
      &lt;li&gt;How much does the noise channel contribute?
 &lt;img src=&quot;/img/15881111498987.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 28 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/28/Conditional-Generative-Adversarial-Network-for-Structured-Domain-Adaptation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/28/Conditional-Generative-Adversarial-Network-for-Structured-Domain-Adaptation/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>SegNet:_A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.00561.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;In practice, we have to consider the cost of memory and computational time.&lt;/li&gt;
  &lt;li&gt;Most recent deep architectures for segmentation have identical encoder networks, i.e VGG16, but differ in the form of the decoder network, training and inference.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15880309668474.jpg&quot; alt=&quot;-w901&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Encoder:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The encoder network consists of 13 convolutional layers which correspond to the first 13 convolutional layers in the VGG16 network.&lt;/li&gt;
  &lt;li&gt;Initialize the training process from weights trained for classification on large datasets.&lt;/li&gt;
  &lt;li&gt;Benifit: Removing the fully connected layers of VGG16 makes the SegNet encoder network significantly smaller [reduces the number of parameters in the SegNet encoder network significantly (from 134M to 14.7M)] and easier to train.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Decoder:
&lt;img src=&quot;/img/15880312579950.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The appropriate decoder in the decoder network upsamples its input feature map(s) using the memorized max-pooling indices from the corresponding encoder feature map(s). This step produces sparse feature map(s).&lt;/li&gt;
  &lt;li&gt;These feature maps are then convolved with a trainable decoder filter bank to produce dense feature maps.&lt;/li&gt;
  &lt;li&gt;Except for the last decoder (which corresponds to the first encoder), the other decoders in the network produce feature maps with the same number of size and channels as their encoder inputs.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;p&gt;class balancing&lt;/p&gt;

</description>
        <pubDate>Mon, 27 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/27/SegNet-A-Deep-Convolutional-Encoder-Decoder-Architecture-for-Image-Segmentation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/27/SegNet-A-Deep-Convolutional-Encoder-Decoder-Architecture-for-Image-Segmentation/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>Fully Convolutional Networks for Semantic Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1411.4038.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/wkentaro/pytorch-fcn&quot;&gt;code (pytorch)&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;将分类网络中的全连接层变为对应的卷积层，从而得到全卷积网络。全卷积网络可用于实现如语义分割的稠密预测任务。添加skip-connecting结合deep, coarse, semantic information和shallow, fine, appearance information。&lt;/p&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;全连接层变为卷积层
&lt;img src=&quot;/img/15876140745604.jpg&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;view fully connected layers as convolutions with kernels that cover their entire input regions.&lt;/li&gt;
      &lt;li&gt;优点：When receptive fields overlap significantly, both feedforward computation and back propagation are much more efficient when computed layer-by-layer over an entire image instead of independently patch-by-patch. （receptive field: locations in the image that locations in higher layers are path-connected to.）&lt;/li&gt;
      &lt;li&gt;问题：由于subsampling, output的分辨率远低于input&lt;br /&gt;
 &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;转置卷积增大分辨率(what)+skip connection结合低层位置信息(where)
&lt;img src=&quot;/img/15876162869041.jpg&quot; alt=&quot;-w870&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;转置卷积能实现最好的上采样效果，通过learning可以实现非线性的上采样&lt;/li&gt;
      &lt;li&gt;将upsampled结果 (deep, coarse, semantic information) 和skip connection结果 (shallow, fine, appearance information) 加起来，得到combine what and where的结果&lt;/li&gt;
      &lt;li&gt;Initialize the 2× upsampling to bilinear interpolation, but allow the parameters to be learned）。 但是，在FCN-8s后，继续融合低层信息取得的结果基本不变。因此，最后几层直接使用bilinear interpolation。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;h5 id=&quot;评价指标&quot;&gt;评价指标&lt;/h5&gt;
&lt;p&gt;Let $n_{ij}$ be the number of pixels of class $i$ predicted to belong to class $j$, where there are $n_{cl}$ different classes, and let $t_i=\sum_jn_{ij}$ be the total number of pixels of class $i$.
&lt;img src=&quot;/img/15876175187934.jpg&quot; width=&quot;40%&quot; height=&quot;50%&quot; /&gt;&lt;/p&gt;
&lt;h5 id=&quot;结果&quot;&gt;结果&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15876184066437.jpg&quot; width=&quot;50%&quot; height=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;使用classification的网络参数作为预训练模型，在此基础上fine-tune网络以用于semantic segmentation任务。&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 22 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/22/Fully-Convolutional-Networks-for-Semantic-Segmentation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/22/Fully-Convolutional-Networks-for-Semantic-Segmentation/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>semantic segmentation dataset summary</title>
        <description>&lt;p&gt;部分内容来自于 https://blog.csdn.net/MOU_IT/article/details/82225505&lt;/p&gt;

&lt;h4 id=&quot;总述&quot;&gt;总述&lt;/h4&gt;
&lt;p&gt;目前学术界主要有三个benchmark（数据集）用于模型训练和测试。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Pascal VOC系列
    &lt;ul&gt;
      &lt;li&gt;VOC 2012 (最常见)&lt;/li&gt;
      &lt;li&gt;Pascal Context (偶尔)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Microsoft COCO&lt;br /&gt;
 COCO一共有80个类别。虽然有很详细的像素级别的标注，但由于官方没有专门对语义分割的测评，因此COCO数据集往往被当成是额外的训练数据集用于模型的训练（预训练？）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Cityscapes
    &lt;ul&gt;
      &lt;li&gt;辅助驾驶（自动驾驶）环境&lt;/li&gt;
      &lt;li&gt;使用比较常见的19个类别用于评测&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;voc-2012&quot;&gt;VOC 2012&lt;/h4&gt;

&lt;p&gt;标准的VOC2012数据集有21个类别(包括背景)，包含:{ 0=background，1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car , 8=cat, 9=chair, 10=cow, 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor，255= ’void’ or unlabelled }这些比较常见的类别。&lt;/p&gt;

&lt;p&gt;对于分割任务：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;VOC 2012用于分割的数据中train+val包含 2007-2011年间的所有数据，test包含2008-2011年间的数据，没有包含07年的是因为07年的test数据已经公开了。&lt;/li&gt;
  &lt;li&gt;trainval：2913张图片，共6929个物体&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;补充：&lt;a href=&quot;https://arleyzhang.github.io/articles/1dc20586/&quot;&gt;PASCAL VOC详细介绍&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;ms-coco&quot;&gt;MS COCO&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;官方没有semantic segmentation测评，因此该数据集常常只用于预训练&lt;/li&gt;
  &lt;li&gt;数据集大小？&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;cityscapes&quot;&gt;Cityscapes&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;由奔驰主推，提供无人驾驶环境下的图像分割数据集，用于评估视觉算法在城区场景语义理解方面的性能。&lt;/li&gt;
  &lt;li&gt;主要包含来自50个不同城市的街道场景，拥有5000张fine annotated images，20000张coarse annotated images。5000张fine annotated images中，2975张训练图，500张验证图和1525张测试图，每张图片大小都是1024x2048。（一般用5000张fine annotated images来训练）&lt;/li&gt;
  &lt;li&gt;分为category(大类)和class(小类)。evaluation过程中，会忽略太不常见的classes，即剩余19个classes。
&lt;img src=&quot;/img/15875266001958.jpg&quot; alt=&quot;-w721&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;补充: &lt;a href=&quot;https://niecongchong.github.io/2019/08/10/CityScapes%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AE%80%E4%BB%8B%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E7%B2%BE%E5%BA%A6%E6%8C%87%E6%A0%87/&quot;&gt;Cityscapes详细介绍&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 21 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/21/semantic-segmentation-datasets/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/21/semantic-segmentation-datasets/</guid>
        
        <category>Datasets</category>
        
        
      </item>
    
      <item>
        <title>AutoDIAL:Automatic DomaIn Alignment Layers</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.08082.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/ducksoup/autodial&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;limitation of AdaBN: the target samples have no influence on the network parameters, as they are not observed during training.&lt;/li&gt;
  &lt;li&gt;对训练过程描述的很详细，可以参考&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15873547625687.jpg&quot; alt=&quot;-w821&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15873551722703.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;img src=&quot;/img/15873552325041.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;img src=&quot;/img/15873552520236.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
其中， $\alpha$ 在[0.5, 1]中：1对应AdaBN (full degree of DA)；0.5对应没有DA，因为此时$q_\alpha^{st}$和$q_\alpha^{ts}$相同，即source和target domain会做相同的transformation。通过让网络自己学习参数$\alpha$，实现了automatically learn the degree of alignment that should be pursued at different levels of the network.&lt;/p&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;p&gt;Bayes based training process:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;objective:&lt;br /&gt;
&lt;img src=&quot;/img/15873556873967.jpg&quot; width=&quot;45%&quot; height=&quot;100%&quot; /&gt;&lt;img src=&quot;/img/15873557363954.jpg&quot; width=&quot;30%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;sampling distribution&lt;br /&gt;
&lt;img src=&quot;/img/15873558553822.jpg&quot; width=&quot;35%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;prior
&lt;img src=&quot;/img/15873559027282.jpg&quot; width=&quot;45%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/15873559171140.jpg&quot; width=&quot;45%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;转化为training objective：
&lt;img src=&quot;/img/15873559673684.jpg&quot; width=&quot;45%&quot; height=&quot;100%&quot; /&gt;  &lt;br /&gt;
source domain用standard cross entropy loss， target domain用entropy minimization.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;$\alpha$ 大部分接近1.&lt;br /&gt;
&lt;img src=&quot;/img/15873565357251.jpg&quot; alt=&quot;-w907&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;others:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;the entropy regularization term is especially beneficial when source and target data representations are aligned. (?)&lt;/li&gt;
  &lt;li&gt;lower layers in a network are subject to domain shift even more than the very last layers.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;reflection&quot;&gt;Reflection&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;同时结合了AdaBN和entropy minimization， 不确定结果的提高是否来源于提出的DA layer.&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 15 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/15/AutoDIAL-Automatic-DomaIn-Alignment-Layers/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/15/AutoDIAL-Automatic-DomaIn-Alignment-Layers/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1712.02560.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/mil-tokyo/MCD_DA&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;if only consider distribution matching (based on statistics), a trained generator can generate ambiguous features near class boundaries (在class boundary附近，由于缺少训练数据，相当于随机分类)
&lt;img src=&quot;/img/15873584186697.jpg&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For better distribution alignment:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;maximize the discrepancy between two classifiers’ outputs to detect target samples that are far from the support of the source.&lt;/li&gt;
  &lt;li&gt;A feature generator learns to generate target features near the support to minimize the discrepancy, in order to avoid generating target features near class boundaries.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15873586517441.jpg&quot; alt=&quot;-w775&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;discrepancy-loss&quot;&gt;discrepancy loss&lt;/h5&gt;
&lt;p&gt;use the absolute values of the difference between the two classifiers’ probabilistic outputs as discrepancy loss.
&lt;img src=&quot;/img/15873624804347.jpg&quot; width=&quot;30%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;training-step&quot;&gt;Training step&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;task-specific loss&lt;br /&gt;
train both classifiers and generator to
classify the source samples correctly. In order to make classifiers and generator obtain task-specific discriminative features, this step is crucial.&lt;br /&gt;
&lt;img src=&quot;/img/15873636378997.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15873636851548.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;train classifier&lt;br /&gt;
A classification loss on the source samples is added here. Without this loss, the authors experimentally found that our algorithm’s performance drops significantly.
&lt;img src=&quot;/img/15873638777096.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;train generator
&lt;img src=&quot;/img/15873639472583.jpg&quot; width=&quot;15%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;reflection&quot;&gt;Reflection&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;distribution alignment也可以用adversarial learning来实现。adversarial learning自由度更大，相比于传统的基于参数的hand-crafted transformation， 也许更能实现理想的distribution alignment.&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Fri, 10 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/10/Maximum-classifier-discrepancy-for-unsupervised-domain-adaptation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/10/Maximum-classifier-discrepancy-for-unsupervised-domain-adaptation/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>t-SNE dimension reduction</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;t-SNE was proposed in 2008, which is s a very powerful and state-of-the-art dimension reduction algorithm, and is especially good at data visualization.&lt;/p&gt;

&lt;p&gt;PCA: perseve global structure (principle component)
t-SNE: preverse local structure or global structure&lt;/p&gt;

&lt;p&gt;https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2899/neighborhood-of-a-point-embedding/2/module-2-data-science-exploratory-data-analysis-and-data-visualization&lt;/p&gt;

</description>
        <pubDate>Fri, 03 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/03/t-SNE%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/03/t-SNE%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/</guid>
        
        <category>ML</category>
        
        
      </item>
    
      <item>
        <title>Backbones</title>
        <description>&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;paper&lt;/th&gt;
      &lt;th&gt;code&lt;/th&gt;
      &lt;th&gt;comment&lt;/th&gt;
      &lt;th&gt;experiment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;AlexNet&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://xiaohan-wang.github.io/2020/03/27/AlexNet/&quot;&gt;link&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Inception-BN&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VGG-16&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Thu, 02 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/02/Backbones/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/02/Backbones/</guid>
        
        <category>Backbones</category>
        
        <category>CV paperlist</category>
        
        
      </item>
    
  </channel>
</rss>
