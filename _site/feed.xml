<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 17 Jun 2020 09:27:47 -0400</pubDate>
    <lastBuildDate>Wed, 17 Jun 2020 09:27:47 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>核方法、核函数、核技巧、再生核希尔伯特空间</title>
        <description>&lt;h4 id=&quot;核方法kernel-method&quot;&gt;核方法（kernel method）&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;理论基础: Cover’s theorem，其指出，在低维空间中线性不可分的数据，通过非线性变换将其投影到高维空间之后，大概率会变为线性可分的数据&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;将低维空间的非线性可分问题，转化为高维空间的线性可分问题&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;核技巧kernel-trick&quot;&gt;核技巧（kernel trick）&lt;/h4&gt;
&lt;p&gt;设$\phi(x)$为映射函数，$\phi(x): \mathcal{X} \to \mathcal{H}$，其中$\mathcal{X}$为输入空间，$\mathcal{H}$为特征空间 (特征空间需要是Hilbert space，即完备的内积空间)。&lt;/p&gt;

&lt;p&gt;欲求$&amp;lt;\phi(x_1), \phi(x_2)&amp;gt;$：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;传统方法：先分别计算$\phi(x_1)$和$\phi(x_2)$，再在特征空间中计算二者的内积
    &lt;ul&gt;
      &lt;li&gt;缺点：当特征空间维度很大时，计算非常复杂&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;核技巧：在输入空间找到一个函数$K(x_1, x_2)$，使得$K(x_1, x_2)=&amp;lt;\phi(x_1), \phi(x_2)&amp;gt;$，从而可以直接在低维空间中计算出结果，加速核方法计算。对应的函数 $K$ 就是核函数&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;核函数kernels&quot;&gt;核函数（kernels）&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;在实际应用时，映射函数 $\phi(x)$ 不需要是已知的。换句话说，核技巧的目的就是在不需要显式地定义特征空间和映射函数的条件下，计算映射之后的内积&lt;/li&gt;
  &lt;li&gt;核函数 $K$ 本质上需要满足的条件 (不需要$\phi(x)$已知):
    &lt;ul&gt;
      &lt;li&gt;对称性: $K(\mathrm{x_1},\mathrm{x_2}) = K(\mathrm{x_2},\mathrm{x_1})$&lt;/li&gt;
      &lt;li&gt;半正定性: 对于任意 $n$ 和任意 $x_1, x_2, \cdots, x_n  \in \mathcal{X}$，由 $K(x_i, x_j)$ 定义的 Gram matrix 总是半正定的&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;只要 $K$ 是核函数，那么一定存在一个Hilbert space和一个映射函数$\phi$，使得$K(x_1, x_2)=&amp;lt;\phi(x_1), \phi(x_2)&amp;gt;$&lt;/li&gt;
  &lt;li&gt;常见核函数&lt;br /&gt;
  &lt;img src=&quot;/img/15923213053700.jpg&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
  其它变换得到的核函数：参考资料3 $\text{P}_{17, 18}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;再生核希尔伯特空间reproducing-kernel-hilbert-spacesrkhs&quot;&gt;再生核希尔伯特空间（reproducing kernel Hilbert spaces，RKHS）&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;再生核希尔伯特空间
    &lt;ul&gt;
      &lt;li&gt;设 $\displaystyle \mathcal{H}$ 是希尔伯特空间，其元素为函数 $\displaystyle f:X\rightarrow R$。对于某个固定的 $\displaystyle x\in \mathcal{X}$，映射$\displaystyle \delta _{x} :H\rightarrow R,\delta _{x} :f\rightarrow f( x)$称为点 $x$ 的 (Dirac) evaluation functional&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;设 $\displaystyle \mathcal{H}$ 是希尔伯特空间，其元素为函数 $\displaystyle f:X\rightarrow R$。若对于任意的 $x \in \mathcal{X}$，$\delta_x$ 都是连续的，则 $\mathcal{H}$ 为RKHS&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;RKHS是一个函数空间，其中的元素 $f$ 为函数。通常情况下特征空间 $\mathcal{H}$ 为 RKHS。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;再生核
    &lt;ul&gt;
      &lt;li&gt;再生核 $K$ 是核函数的一种，其满足
        &lt;ul&gt;
          &lt;li&gt;对于任意固定的 $x_0\in\mathcal{X}$，$K(x,x_0)$作为 $x$ 的函数属于我们的函数空间$\mathcal{H}$&lt;/li&gt;
          &lt;li&gt;对于任意 $x\in\mathcal{X}$ 和 $f(\cdot)\in\mathcal{H}$，有 $f(x) = \langle f(\cdot),K(\cdot,x)\rangle$ (再生性质 / reproducing property)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;对于再生核 $K$，我们可以自然的定义映射函数 $\phi(x)=K(\cdot, x)$，此时，通过再生核的再生性质，可知
  &lt;script type=&quot;math/tex&quot;&gt;\langle \phi(x_1),\phi(x_2)\rangle = \langle K(\cdot,x_1),K(\cdot,x_2)\rangle = K(x_1,x_2)&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;关系
    &lt;ul&gt;
      &lt;li&gt;一个希尔伯特空间存在至多一个再生核 (不存在 / 存在一个)&lt;/li&gt;
      &lt;li&gt;存在再生核的希尔伯特空间就是再生核希尔伯特空间&lt;/li&gt;
      &lt;li&gt;一个再生核对应唯一的再生核希尔伯特空间&lt;/li&gt;
      &lt;li&gt;再生核和再生核希尔伯特空间是一一对应的&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;一个核函数 $K$ 可能对应多个映射函数 $\phi$，而每个映射函数$\phi$ 有自己对应的特征空间 (i.e. 希尔伯特空间）
    &lt;ul&gt;
      &lt;li&gt;例子详见 参考资料6 $P_{11}$ example 35&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;任意一个核函数 $K$ 都可以作为再生核，构建其对应的唯一的再生核希尔伯特空间&lt;/li&gt;
  &lt;li&gt;对于任意一个核函数 $K$，可以存在多个对应的特征空间 $\mathcal{H_0}$，但其作为再生核只对应唯一的RKHS。同时，RKHS是所有特征空间最为“精简”的一个，这里的精简体现在，无论在 $\mathcal{H_0}$ 中得到怎样的分类模型 $⟨w,\phi_0(x)⟩_{\mathcal{H_0}}$，在RKHS中都存在一个 $f$ 可以得到和它相同的效果，因此对于某个核函数，RKHS代表了它最本征的信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/61794781&quot;&gt;核方法、核技巧和核函数&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.fanyeong.com/2017/11/13/the-kernel-trick/&quot;&gt;核技巧&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.stat.berkeley.edu/~bartlett/courses/2014spring-cs281bstat241b/lectures/20-notes.pdf&quot;&gt;PPT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cosx.org/2014/05/svm-series-add-2-kernel-ii/&quot;&gt;“支持向量机系列” 的番外篇二: Kernel II&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/54704957&quot;&gt;什么是RKHS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/RKHS_Notes1.pdf&quot;&gt;课程讲义&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://murongxixi.github.io/2013/11/12/%E6%A0%B8%E5%87%BD%E6%95%B0%EF%BC%8C%E5%86%8D%E7%94%9F%E6%A0%B8Hilbert%E7%A9%BA%E9%97%B4%EF%BC%8C%E8%A1%A8%E7%A4%BA%E5%AE%9A%E7%90%86/&quot;&gt;核函数，再生核Hilbert空间，表示定理&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 16 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/16/%E6%A0%B8%E6%96%B9%E6%B3%95-%E6%A0%B8%E5%87%BD%E6%95%B0-%E6%A0%B8%E6%8A%80%E5%B7%A7/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/16/%E6%A0%B8%E6%96%B9%E6%B3%95-%E6%A0%B8%E5%87%BD%E6%95%B0-%E6%A0%B8%E6%8A%80%E5%B7%A7/</guid>
        
        <category>Math</category>
        
        
      </item>
    
      <item>
        <title>Amazon SageMaker</title>
        <description>
&lt;h4 id=&quot;amazon-sagemaker简介&quot;&gt;Amazon SageMaker简介&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;AWS提供的面向所有开发人员和数据科学家的&lt;strong&gt;机器学习服务&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;可以帮助开快速构建、训练和部署机器学习 (ML) 模型&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Amazon SageMaker Ground Truth
    &lt;ul&gt;
      &lt;li&gt;快速构建和管理高度准确的训练数据集&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Amazon SageMaker Studio
    &lt;ul&gt;
      &lt;li&gt;首个&lt;strong&gt;适用于机器学习的完全集成式开发环境 (IDE)&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;提供了一个基于 Web 的可视化界面，可以通过该界面执行所有 ML 开发步骤，包括笔记本、实验管理、自动创建模型、调试以及模型偏差检测&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;amazon-sagemaker-studio&quot;&gt;Amazon SageMaker Studio&lt;/h4&gt;
&lt;p&gt;待填…&lt;/p&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://aws.amazon.com/cn/sagemaker/?nc2=h_ql_prod_ml_sm&lt;/li&gt;
  &lt;li&gt;https://aws.amazon.com/cn/getting-started/hands-on/create-machine-learning-model-automatically-sagemaker-autopilot/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 12 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/12/SageMaker/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/12/SageMaker/</guid>
        
        <category>Tools</category>
        
        
      </item>
    
      <item>
        <title>AWS</title>
        <description>
&lt;h4 id=&quot;aws简介&quot;&gt;AWS简介&lt;/h4&gt;
&lt;p&gt;Amazon Web Services (AWS) 是全球最全面、应用最广泛的云平台。从全球数据中心提供涉及超过十二种类别的，超过 175 项功能齐全的服务：不仅包括计算、存储和数据库等基础设施技术，而且提供机器学习、人工智能、数据湖和分析以及物联网等新兴技术。&lt;/p&gt;

&lt;h4 id=&quot;amazon-sagemaker&quot;&gt;Amazon SageMaker&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;面向所有开发人员和数据科学家的&lt;strong&gt;机器学习服务&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;可以帮助开快速构建、训练和部署机器学习 (ML) 模型&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;amazon-ec2-amazon-elastic-compute-cloud&quot;&gt;Amazon EC2 (Amazon Elastic Compute Cloud)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;可以在云中提供安全并且可调整大小的计算容量&lt;/li&gt;
  &lt;li&gt;提供多种经过优化，适用于不同使用场景的实例类型以供选择。&lt;strong&gt;不同实例类型具有不同的 CPU、内存、存储和网络容量，可以灵活地为不同应用程序选择适当的资源组合&lt;/strong&gt;。每种实例类型都包括一种或多种实例大小，从而能够扩展资源以满足目标工作负载的要求。
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/cn/ec2/instance-types/&quot;&gt;实例类型&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/cn/ec2/pricing/on-demand/&quot;&gt;定价&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://aws.amazon.com/cn/getting-started/fundamentals-overview/&lt;/li&gt;
  &lt;li&gt;https://aws.amazon.com/cn/sagemaker/?nc2=h_ql_prod_ml_sm&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 12 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/12/AWS/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/12/AWS/</guid>
        
        <category>Tools</category>
        
        
      </item>
    
      <item>
        <title>假设检验</title>
        <description>&lt;h4 id=&quot;各种分布&quot;&gt;各种分布&lt;/h4&gt;
&lt;h5 id=&quot;正态分布&quot;&gt;正态分布&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15919078017261.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$\mu \pm \sigma$ 之间的概率是68%&lt;/li&gt;
  &lt;li&gt;$\mu \pm 1.96\sigma$ 之间的概率是95%&lt;/li&gt;
  &lt;li&gt;$\mu \pm 2.56\sigma$ 之间的概率是99%&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;其它分布&quot;&gt;其它分布&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15919118904133.jpg&quot; width=&quot;30%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
和正态分布不同，需要其它方法计算对应的概率范围&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;假设检验&quot;&gt;假设检验&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;原假设 (Null hypothesis)&lt;/strong&gt;：也叫零假设，用 $H_0$ 来表示，一般是希望能证明为错误的假设。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;备择假设 (Alternative hypothesis)&lt;/strong&gt;：原假设的反面，一般用 $H_1$ 来表示，是希望证明是正确的另一种可能。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;检验统计量 (Test statistic)&lt;/strong&gt;：用于统计假设检验的统计量，是数据集的数字汇总，将数据减少到可用于执行假设检验的一个值。&lt;/li&gt;
  &lt;li&gt;假设检验的目的在于试图找到证据拒绝原假设。当没有足够证据拒绝原假设时，不采用 “接受原假设” 的表述，而采用 “不拒绝原假设” 的表述。“不拒绝”的表述实际上意味着并未给出明确的结论，我们没有说原假设正确，也没有说它不正确。&lt;strong&gt;假设检验的主要目的是为了拒绝而不是接受。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;思路&quot;&gt;思路&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;欲证：备择假设为真&lt;/li&gt;
  &lt;li&gt;原理1: &lt;strong&gt;假设证明难而证伪易&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;只需证：备择假设的否命题（原假设）为假&lt;/li&gt;
  &lt;li&gt;原理2: &lt;strong&gt;小概率事件在少量实验中是不可能出现的&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;只需：观察到“&lt;strong&gt;原假设条件下的小概率事件&lt;/strong&gt;的发生”&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;检验方法&quot;&gt;检验方法&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;首先确定“小概率事件”的概率范围，即&lt;strong&gt;显著性水平&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;e.g. 显著性水平为0.05，即发生概率小于5%的事件为“小概率事件”&lt;/li&gt;
      &lt;li&gt;显著性水平越高，“小概率事件”发生的可能性越大，即越来越容易拒绝原假设&lt;/li&gt;
      &lt;li&gt;“小概率事件”发生的区域对应&lt;strong&gt;拒绝域&lt;/strong&gt;，&lt;strong&gt;拒绝域&lt;/strong&gt;没有覆盖的区域为&lt;strong&gt;置信区间&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;如下图：设已知成绩服从正态分布，原假设为成绩均值为$\mu$，在该条件下，红色区域对应小概率事件 (发生概率为5%)，即拒绝域概率为5%，置信区间概率为95%&lt;br /&gt;
 &lt;img src=&quot;/img/15919111868762.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;检验方法1 (临界概率 to 临界值):
    &lt;ul&gt;
      &lt;li&gt;根据设定的显著度和对应分布找到“小概率事件”对应的临界值&lt;/li&gt;
      &lt;li&gt;计算样本得到的统计量，判定是否越过了临界值&lt;/li&gt;
      &lt;li&gt;若是，则进入了小概率错误域 (拒绝域)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;检验方法2 (样本值 to 样本概率)：
    &lt;ul&gt;
      &lt;li&gt;计算样本得到的统计量，进而在分布中找到对应的概率值，即p值&lt;/li&gt;
      &lt;li&gt;将p值与显著度 (设定的“小概率事件”判定值) 进行比较&lt;/li&gt;
      &lt;li&gt;如果p值比显著度小，则发生的样本属于“小概率事件”，故而拒绝原假设&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;检验方法1和2本质上是相同的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;错误类型&quot;&gt;错误类型&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;I类错误 (Type I error): 拒绝了正确的零假设&lt;/li&gt;
  &lt;li&gt;II类错误 (Type II error): 没有拒绝错误的零假设&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/15920070800446.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://www.zhihu.com/question/20254932&lt;/li&gt;
  &lt;li&gt;https://cosx.org/2009/03/meaning-of-failure-to-reject-h0/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 11 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/11/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/11/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</guid>
        
        <category>Statistics</category>
        
        
      </item>
    
      <item>
        <title>从 总体 &amp; 样本 估计总体方差</title>
        <description>&lt;h4 id=&quot;用总体估计总体方差&quot;&gt;用总体估计总体方差&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;calculating population variance and std for the whole population using all the data&lt;/li&gt;
  &lt;li&gt;e.g. What is the standard deviation of last year’s returns of the 12 funds I have invested in? In this case, we have the data for the whole population available.&lt;/li&gt;
  &lt;li&gt;When using the whole population to calculate population variance, &lt;strong&gt;divide&lt;/strong&gt; the sum of squared deviations from the mean &lt;strong&gt;by the number of items in the population&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;用样本估计总体方差&quot;&gt;用样本估计总体方差&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;calculating population variance and std using only a sample of data&lt;/li&gt;
  &lt;li&gt;e.g. What is the standard deviation of last year’s returns of equity funds in the world? In this case, we don’t have all the data available and we will have to estimate the population’s standard deviation from a sample.&lt;/li&gt;
  &lt;li&gt;When using a sample to calculate population variance, &lt;strong&gt;divide it by the number of items in the sample less one&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;As a result, the calculated variance (and therefore also the standard deviation) will &lt;strong&gt;be slightly higher than if we would have used the population variance formula&lt;/strong&gt;. The purpose of this little difference is to get a better and unbiased estimate of the population‘s variance (by dividing by the sample size lowered by one, we &lt;strong&gt;compensate for the fact that we are working only with a sample rather than with the whole population&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://www.macroption.com/population-sample-variance-standard-deviation/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 11 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/11/population-and-sample-variance/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/11/population-and-sample-variance/</guid>
        
        <category>Statistics</category>
        
        
      </item>
    
      <item>
        <title>EM algorithm, K-means, and GMM</title>
        <description>&lt;h4 id=&quot;em-algorithm&quot;&gt;EM algorithm&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;EM算法是期望最大化(Expectation Maximization)算法的简称&lt;/li&gt;
  &lt;li&gt;用于&lt;strong&gt;含有隐变量(hidden variable)&lt;/strong&gt;的情况下，模型参数的最大似然估计&lt;/li&gt;
  &lt;li&gt;EM算法是一种迭代算法，每次迭代由两步组成：
    &lt;ul&gt;
      &lt;li&gt;E步：根据模型参数的假设值，给出隐变量的期望估计，应用于缺失值&lt;/li&gt;
      &lt;li&gt;M步：根据隐变量的估计值，给出当前的参数的极大似然估计&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;k-means&quot;&gt;K-means&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;先随机选定k个点作为质心 $\mu_1, \mu_2, \cdots, \mu_𝑘$&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;E step: 固定$\mu_k$，将样本划分到距离最近的$\mu_k$所属的簇中&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
r_{nk} = \left. \begin{cases} 1 \,\, &amp; \text{if} \;\;\;k = \mathop{argmin}_j ||\mathbf{x}_n - \boldsymbol{\mu}_j||^2 \\
0 \,\, &amp; \text{otherwise} \end{cases} \right. %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;M step: 对于每一个数据簇，重新计算其中心，目标是最小化簇中每个样本与中心的距离，可表示为&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;J = \sum\limits_{n=1}^N r_{nk} ||\mathbf{x}_n - \boldsymbol{\mu}_k||^2.&lt;/script&gt;

    &lt;p&gt;为求得最小化 $J$ 的 $\mu_k$，可通过&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial\mathbf{\mu}_k}=2\sum\limits_{n=1}^N r_{nk}(\boldsymbol{x}_n - \boldsymbol{\mu}_k) = 0,&lt;/script&gt;

    &lt;p&gt;求得&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\mu}_k = \frac{\sum_nr_{nk} \mathbf{x}_n}{\sum_n r_{nk}},&lt;/script&gt;

    &lt;p&gt;即簇中每个样本的均值向量。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;重复计算 E-step 和 M-step 直至收敛&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;gaussian-mixture-model-gmm&quot;&gt;Gaussian Mixture Model (GMM)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;混合模型 (mixture model)：是一个可以用来表示在总体分布中含有 K 个子分布的概率模型。换句话说，总体的概率分布，是一个由 K 个子分布组成的混合分布。
    &lt;ul&gt;
      &lt;li&gt;混合模型不要求观测数据提供其属于哪个子分布 =&amp;gt; hiddle varibale&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;对于混合模型来说，每个子分布天然地构成了各自的一类&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;高斯混合模型：由 K 个单高斯模型组合而成的模型。一般来说，一个混合模型可以使用任何概率分布，这里使用高斯混合模型是因为高斯分布具备很好的数学性质以及良好的计算性能。
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;高斯混合模型的概率分布为：&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x|\theta) = \sum_{k=1}^{K}{\alpha_{k}\phi(x|\theta_{k})}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;对于这个模型而言，参数 $\theta = (\tilde{\mu_{k}}, \tilde{\sigma_{k}}, \tilde{\alpha_{k}})$ ，也就是每个子模型的期望、方差（或协方差）、在混合模型中发生的概率&lt;/li&gt;
      &lt;li&gt;如果通过使用足够多的高斯分布，并且调节它们的均值和方差以及线性组合的系数，那么几乎所有的连续概率密度都能够以任意的精度近似。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;随机初始化模型参数（各个高斯分布的均值、方差、发生概率）&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;E step: 依据当前参数，计算每个数据 $j$ 来自子模型 $k$ 的可能性&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_{jk} = \frac{\alpha_{k}\phi(x_{j}|\theta_{k})}{\sum_{k=1}^{K}{\alpha_{k}\phi(x_{j}|\theta_{k})}}, j = 1,2,...,N; k = 1,2,...,K&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;M step: 计算新的模型参数&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_{k} = \frac{\sum_{j=1}^{N}{(\gamma_{jk}}x_{j})}{\sum_{j=1}^{N}{\gamma_{jk}}}, k=1,2,...,K&lt;/script&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_{k} = \frac{\sum_{j=1}^{N}{\gamma_{jk}}(x_{j}-\mu_{k})(x_{j}-\mu_{k})^{T}}{\sum_{j=1}^{N}{\gamma_{jk}}}, k = 1,2,...,K&lt;/script&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_{k} = \frac{\sum_{j=1}^{N}{\gamma_{jk}}}{N}, k=1,2,...,K&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;重复计算 E-step 和 M-step 直至收敛&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;difference&quot;&gt;difference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;K-means: hard assignment&lt;/strong&gt;
in each iteration, we are absolutely certain as to which cluster the point belongs to&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GMM: soft assignment&lt;/strong&gt;
 It starts with some prior belief about how certain we are about each point’s cluster assignments. As it goes on, it revises those beliefs&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;reference&quot;&gt;Reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/What-is-the-difference-between-K-means-and-the-mixture-model-of-Gaussian&quot;&gt;difference of k-means and GMM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sites.northwestern.edu/msia/2016/12/08/k-means-shouldnt-be-our-only-choice/&quot;&gt;k-means shouldn’t be our only choice&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/30483076&quot;&gt;EM algorithm, K-means, and GMM 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/75554749&quot;&gt;EM algorithm, K-means, and GMM 2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 09 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/09/K-means%E5%92%8CGMM/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/09/K-means%E5%92%8CGMM/</guid>
        
        <category>Clustering</category>
        
        
      </item>
    
      <item>
        <title>Image classification</title>
        <description>&lt;h4 id=&quot;cifar-10&quot;&gt;CIFAR-10&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;60000 32x32 color images in 10 classes, with 6000 images per class&lt;/li&gt;
  &lt;li&gt;50000 training images and 10000 test images&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/img/15917518925606.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 09 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/09/Image-classification-dataset/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/09/Image-classification-dataset/</guid>
        
        <category>Datasets</category>
        
        
      </item>
    
      <item>
        <title>slurm使用基础</title>
        <description>
&lt;p&gt;SLURM：开源作业调度系统&lt;/p&gt;

&lt;h4 id=&quot;提交作业&quot;&gt;提交作业&lt;/h4&gt;
&lt;h5 id=&quot;提交方式&quot;&gt;提交方式&lt;/h5&gt;
&lt;p&gt;Slurm提交作业有3种模式，分别为交互模式，批处理模式，分配模式。这三种方式&lt;strong&gt;只是用户使用方式的区别，而在管理，调度，记账时同等对待&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;交互模式(srun)：交互式作业提交，提交命令后，等待作业执行完成之后返回命令行窗口。&lt;/li&gt;
  &lt;li&gt;批处理模式(sbatch)：用户编写作业脚本，指定资源需求约束，提交后台执行作业。&lt;/li&gt;
  &lt;li&gt;分配模式(salloc)：结点资源抢占命令。该命令支持用户在提交作业前，抢占所需计算资源。&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;运行参数&quot;&gt;运行参数&lt;/h5&gt;
&lt;p&gt;以下参数适用于所有作业提交命令(srun, sbatch, salloc)。&lt;strong&gt;sbatch时可以通过脚本提交或命令行提交&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;常用：&lt;/p&gt;

&lt;style&gt;
table th:first-of-type {
    width: 8%;
}
table th:nth-of-type(2) {
    width: 28%;
}
table th:nth-of-type(3) {
    width: 25%;
}
table th:nth-of-type(4) {
    width: 39%;
}
&lt;/style&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;参数&lt;/th&gt;
      &lt;th&gt;简写&lt;/th&gt;
      &lt;th&gt;作用&lt;/th&gt;
      &lt;th&gt;备注&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;-p&lt;/td&gt;
      &lt;td&gt;–partition&lt;/td&gt;
      &lt;td&gt;指定队列资源&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;–gres=gpu:&amp;lt;number&amp;gt;&lt;/td&gt;
      &lt;td&gt;每个节点的GPU数&lt;/td&gt;
      &lt;td&gt;gres是generic resource&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-J&lt;/td&gt;
      &lt;td&gt;–job-name&lt;/td&gt;
      &lt;td&gt;指定作业名称&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-w&lt;/td&gt;
      &lt;td&gt;–nodelist=&amp;lt;host1,host2,…&amp;gt;&lt;/td&gt;
      &lt;td&gt;在指定的节点上运行&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;其他：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;参数&lt;/th&gt;
      &lt;th&gt;简写&lt;/th&gt;
      &lt;th&gt;作用&lt;/th&gt;
      &lt;th&gt;备注&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;-N&lt;/td&gt;
      &lt;td&gt;–nodes=&amp;lt;number&amp;gt;&lt;/td&gt;
      &lt;td&gt;指定节点数量&lt;/td&gt;
      &lt;td&gt;是节点数，不是CPU核数，实际分配的是节点数×每节点CPU核数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-n&lt;/td&gt;
      &lt;td&gt;–ntasks=&amp;lt;number&amp;gt;&lt;/td&gt;
      &lt;td&gt;运行&amp;lt;number&amp;gt;个任务&lt;/td&gt;
      &lt;td&gt;默认为每个节点一个任务，注意是所需总CPU核数&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;–ntasks-per-node=&amp;lt;ntasks&amp;gt;&lt;/td&gt;
      &lt;td&gt;每个节点运行&amp;lt;ntasks&amp;gt;个任务&lt;/td&gt;
      &lt;td&gt;需与-n=&amp;lt;number&amp;gt;配合&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;–ntasks-per-core=&amp;lt;ntasks&amp;gt;&lt;/td&gt;
      &lt;td&gt;每颗CPU核运行&amp;lt;ntasks&amp;gt;个任务&lt;/td&gt;
      &lt;td&gt;需与-n=&amp;lt;number&amp;gt;配合，并自动绑定&amp;lt;ntasks&amp;gt;个任务到每个CPU核&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;查看信息&quot;&gt;查看信息&lt;/h4&gt;
&lt;h5 id=&quot;队列&quot;&gt;队列&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;sinfo：显示队列中各个节点的状态（idle, mix, alloc, drain）
    &lt;ul&gt;
      &lt;li&gt;idle，表示节点处于空闲状态&lt;/li&gt;
      &lt;li&gt;mix，节点具有分配CPU的作业，而其他的CPU状态是IDLE，新提交的作业继续运行&lt;/li&gt;
      &lt;li&gt;alloc，节点所有CPU都被占用，新提交的作业将排队&lt;/li&gt;
      &lt;li&gt;drain，出现这个状态时，不影响正在运行的作业，但是不接受新的作业调度，可以使用命令sinfo –R打印节点不正常的状态产生原因&lt;/li&gt;
      &lt;li&gt;down 故障节点不可用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;scontrol show partition &amp;lt;partition name&amp;gt;：显示队列详细信息&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;作业&quot;&gt;作业&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;squeue：显示排队和运行中的作业（可以设置参数限制显示范围，e.g. -u显示特定user的作业）&lt;/li&gt;
  &lt;li&gt;scontrol show job &amp;lt;job id&amp;gt;：实时作业详细信息&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;节点&quot;&gt;节点&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;scontrol show node &amp;lt;node name&amp;gt;：显示节点状态
    &lt;ul&gt;
      &lt;li&gt;CfgTRES: 该节点的总资源（TRES表示Trackable Resource）
        &lt;ul&gt;
          &lt;li&gt;CfgTRES=cpu=32,mem=257828M,billing=32,gres/gpu=8&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;AllocTRES: 已经分配的资源
        &lt;ul&gt;
          &lt;li&gt;AllocTRES=cpu=4,mem=8800M,gres/gpu=2：已经占用 了 4 个 CPU 核心，8800 MB 内存和 2 块 GPU 卡&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;简单使用方法&quot;&gt;简单使用方法&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;sinfo -&amp;gt; 查看总体GPU使用情况&lt;/li&gt;
  &lt;li&gt;scontrol show node &amp;lt;node name&amp;gt; -&amp;gt; 查看具体节点GPU剩余情况&lt;/li&gt;
  &lt;li&gt;sbatch -w &amp;lt;node name&amp;gt; –gres=gpu:&amp;lt;number&amp;gt; train.sh -&amp;gt; 在相应节点上request对应的GPU资源&lt;/li&gt;
  &lt;li&gt;scontrol show job &amp;lt;jobid&amp;gt; -&amp;gt; 查看任务详细信息&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在申请相应资源后，任务只能&lt;strong&gt;获取到&lt;/strong&gt;对应部分资源。比如说，无论节点总体的GPU使用情况如何 (e.g. 一共8个GPU，已占用3个)，若–gres=gpu:4 (申请了4个GPU)，那么程序的CUDA_VISIBLE_DEVICES=0,1,2,3&lt;/p&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.hpccube.com/wiki/index.php/SLURM%E4%BD%BF%E7%94%A8%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B#.E6.96.87.E6.A1.A3.E6.A6.82.E8.BF.B0&quot;&gt;SLURM使用基础教程&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://hmli.ustc.edu.cn/doc/userguide/slurm-userguide.pdf&quot;&gt;Slurm作业调度系统使用指南&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 05 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/05/slurm/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/05/slurm/</guid>
        
        <category>Tools</category>
        
        
      </item>
    
      <item>
        <title>多卡训练</title>
        <description>
&lt;h4 id=&quot;cuda_visible_devices&quot;&gt;CUDA_VISIBLE_DEVICES&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;CUDA_VISIBLE_DEVICES环境变量限制CUDA程序可见的GPU设备&lt;/li&gt;
  &lt;li&gt;CUDA应用运行时，CUDA将遍历当前&lt;strong&gt;可见的&lt;/strong&gt;设备，并从零开始为可见设备编号（因此显卡的实际编号和程序看到的编号不同）&lt;/li&gt;
  &lt;li&gt;如果设备序列是存在和不存在设备的混合，那么不存在设备前的所有存在设备将被重新编号，不存在设备之后的所有设备将被屏蔽&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;在终端中设置
 &lt;code class=&quot;highlighter-rouge&quot;&gt;CUDA_VISIBLE_DEVICES=0,1 python my_script.py&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;python代码开头设置
 &lt;code class=&quot;highlighter-rouge&quot;&gt;os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0,2&quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;pytorch使用gpu&quot;&gt;PyTorch使用GPU&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;data.cuda()&lt;/code&gt;: the old, pre-0.4 way&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;data.to(device)&lt;/code&gt;: more flexible. For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;data.to('cuda:1')&lt;/code&gt; put data to GPU1&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;单机单卡&quot;&gt;单机单卡&lt;/h4&gt;
&lt;p&gt;如果不设置Dataparallel，即使一台机器上有多个GPU，pytorch也只会占用编号为0的GPU&lt;/p&gt;

&lt;h4 id=&quot;单机多卡&quot;&gt;单机多卡&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# have to put model and data to GPU0
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# model
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataParallel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda:0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# data
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda:0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.DataParallel&lt;/code&gt;默认使用可见的所有显卡。如果只想使用特定的显卡，可以&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;修改CUDA_VISIBLE_DEVICES环境变量&lt;/li&gt;
  &lt;li&gt;使用&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.DataParallel(model, device_ids=[1,2])&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note：&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.DataParallel&lt;/code&gt;返回的model是DataParallel，而不是原来的model。原来的model被保存在返回的model的module属性中(model.module)&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;多机多卡&quot;&gt;多机多卡&lt;/h4&gt;
&lt;p&gt;待填&lt;/p&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://www.jianshu.com/p/0816c3a5fa5c&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/86441879&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 04 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/04/PyTorch%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/04/PyTorch%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83/</guid>
        
        <category>PyTorch</category>
        
        
      </item>
    
      <item>
        <title>语义分割评价指标</title>
        <description>
&lt;h4 id=&quot;评价指标&quot;&gt;评价指标&lt;/h4&gt;
&lt;p&gt;Let $n_{ij}$ be the number of pixels of class $i$ predicted to belong to class $j$, where there are $n_{cl}$ different classes, and let $t_i=\sum_jn_{ij}$ be the total number of pixels of class $i$.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;pixel accuracy: $\frac{\sum\limits_i n_{ii}}{\sum\limits_i t_{i}}$&lt;/li&gt;
  &lt;li&gt;mean pixel accuracy $\frac{1}{n_{cl}}\sum\limits_i \frac{n_{ii}}{ t_{i}}$&lt;/li&gt;
  &lt;li&gt;mean IoU: $\frac{1}{n_{cl}}\sum\limits_i \frac{n_{ii}}{t_i+\sum\limits_j n_{ji}-n_{ii}}$&lt;/li&gt;
  &lt;li&gt;frequency weighted IoU $\frac{1}{\sum\limits_k t_k}\sum\limits_i \frac{t_i n_{ii}}{t_i+\sum\limits_j n_{ji}-n_{ii}}$&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;reference&quot;&gt;Reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Fully Convolutional Networks for Semantic Segmentation&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 01 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/01/segmentation-metrics/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/01/segmentation-metrics/</guid>
        
        <category>Metrics</category>
        
        
      </item>
    
  </channel>
</rss>
