<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 18 May 2020 22:44:55 -0400</pubDate>
    <lastBuildDate>Mon, 18 May 2020 22:44:55 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>PyTorch模型检查/可视化</title>
        <description>&lt;h4 id=&quot;print&quot;&gt;print&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;c1&quot;&gt;# 打印所有子模块，但不能显示各个模块间的关系
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_ASPP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backbone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'resnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;named_children&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;':'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/15898472827399.jpg&quot; alt=&quot;-w817&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;模型可视化&quot;&gt;模型可视化&lt;/h4&gt;

&lt;h5 id=&quot;torchviz--graphviz&quot;&gt;torchviz + graphviz&lt;/h5&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchviz&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 加载并运行模型
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_ASPP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backbone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'resnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 构造图对象，保存为pdf
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;torchviz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'aspp'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/15898469921259.jpg&quot; alt=&quot;-w1157&quot; /&gt;&lt;/p&gt;
&lt;h5 id=&quot;netron&quot;&gt;netron&lt;/h5&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lutzroeder/Netron&quot;&gt;github link&lt;/a&gt; ，超有潜力，但目前还不能稳定支持pytorch&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;netron&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_ASPP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backbone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'resnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;# 保存模型
&lt;/span&gt;   
    &lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;aspp.pth&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 用python server打开
&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;netron&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/img/15898517608333.jpg&quot; alt=&quot;-w1008&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;tensorboard&quot;&gt;tensorboard&lt;/h5&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.tensorboard&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SummaryWriter&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_ASPP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backbone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'resnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 保存模型结构，在当前文件夹下生成runs
&lt;/span&gt;    
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SummaryWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'aspp'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;通过&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorboard --logdir=runs&lt;/code&gt;, 可以在浏览器打开。但感觉不太直观，有点看不懂&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15898488137991.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;tensorwatch&quot;&gt;tensorwatch&lt;/h5&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorwatch&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tw&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build_ASPP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backbone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'resnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;draw_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;只能在jupyter notebook上运行，但生成的结果是横屏，很乱，就不贴图了&lt;/p&gt;
</description>
        <pubDate>Mon, 18 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/18/pytorch%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/18/pytorch%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96/</guid>
        
        <category>PyTorch</category>
        
        
      </item>
    
      <item>
        <title>代码管理、实验管理</title>
        <description>&lt;p&gt;目前对代码管理以及实验管理的一些小想法，部分来自&lt;a href=&quot;https://www.zhihu.com/question/269707221&quot;&gt;知乎&lt;/a&gt;，随时修改…&lt;/p&gt;

&lt;h4 id=&quot;项目代码&quot;&gt;项目代码&lt;/h4&gt;
&lt;h5 id=&quot;before-starting&quot;&gt;before starting&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;实现前先思考代码的整体框架和结构、各部分功能，可以用xmind做一个简单思维导图&lt;/li&gt;
  &lt;li&gt;尽量解耦代码，从而获得更高的复用性
    &lt;ul&gt;
      &lt;li&gt;实现一个基础整体框架 e.g. &lt;a href=&quot;https://github.com/MrGemy95/Tensorflow-Project-Template&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;单独实现各个模块&lt;/li&gt;
      &lt;li&gt;组合代码，同时添加一些单独的细节部分&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;tricks&quot;&gt;tricks&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;分离代码和数据
    &lt;ul&gt;
      &lt;li&gt;最好假设它们不在一个文件夹&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;试验参数
    &lt;ul&gt;
      &lt;li&gt;尽量使用config文件传入，并且config尽量与log文件同名保存。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;实验管理&quot;&gt;实验管理&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;使用&lt;a href=&quot;https://mlflow.org/docs/latest/index.html&quot;&gt;MLflow&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;实验记录&quot;&gt;实验记录&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;内容
    &lt;ul&gt;
      &lt;li&gt;记录每个实验的目的、方法、结果&lt;/li&gt;
      &lt;li&gt;尽量附代码 （用代码的 commit id）、数据&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;工具
    &lt;ul&gt;
      &lt;li&gt;最好用overleaf，可以与github repo 或者本地 git repo 同步 【TODO】&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 15 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/15/Patterns-for-Research/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/15/Patterns-for-Research/</guid>
        
        <category>Q &amp; A</category>
        
        
      </item>
    
      <item>
        <title>MobileNets</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1704.04861.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;MobileNets: light weight deep neural networks that use depth-wise separable convolutions&lt;/li&gt;
  &lt;li&gt;two simple global hyper-parameters that efficiently trade off between latency and accuracy&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;h5 id=&quot;depth-separable-convolution&quot;&gt;Depth separable convolution&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895586236484.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;depthwise convolution: apply a single filter per each input channel&lt;/li&gt;
  &lt;li&gt;pointwise convolution: a simple 1×1 convolution to create a linear combination of the output of the depthwise layer&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;MobileNets use both batchnorm and ReLU nonlinearities for both layers.
&lt;img src=&quot;/img/15895586917635.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;computation cost
    &lt;ul&gt;
      &lt;li&gt;symbols
        &lt;ul&gt;
          &lt;li&gt;$D_F$: input feature size / output feature size&lt;/li&gt;
          &lt;li&gt;$D_K$: kernel size&lt;/li&gt;
          &lt;li&gt;$M$: input channels&lt;/li&gt;
          &lt;li&gt;$N$: outout channels&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;standard convolutions&lt;br /&gt;
  &lt;img src=&quot;/img/15895585271744.jpg&quot; width=&quot;33%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;depthwise separable convolutions&lt;br /&gt;
  &lt;img src=&quot;/img/15895588519425.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;reduction in computation: MobileNet uses 3 × 3 depthwise separable convolutions, which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy&lt;br /&gt;
  &lt;img src=&quot;/img/15895588852425.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;network-structure&quot;&gt;Network structure&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895692454944.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The first layer is a full convolution.&lt;/li&gt;
  &lt;li&gt;All layers are followed by a batchnorm and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification&lt;/li&gt;
  &lt;li&gt;Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer.&lt;/li&gt;
  &lt;li&gt;A final average pooling reduces the spatial resolution to 1 before the fully connected layer.&lt;/li&gt;
  &lt;li&gt;Counting depthwise and pointwise convo- lutions as separate layers, MobileNet has 28 layers.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;MobileNets structure puts nearly all of the computation into dense 1×1 convolutions. This can be implemented with highly optimized general matrix multiply (GEMM) functions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;tricks&quot;&gt;Tricks&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;contrary to training large models we use less regularization and data augmentation techniques because small models have less trouble with overfitting.&lt;/li&gt;
  &lt;li&gt;it was important to put very little or no weight decay (L2 regularization) on the depthwise filters since their are so few parameters in them.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;hyperparameter&quot;&gt;Hyperparameter&lt;/h4&gt;
&lt;h5 id=&quot;width-multiplier-thinner-models&quot;&gt;Width Multiplier: Thinner Models&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;The role of the width multiplier $\alpha$ is to thin a network uniformly at each layer (the number of input channels $M$ becomes $\alpha M$,  and the number of output channels $N$ becomes $\alpha N$)&lt;/li&gt;
  &lt;li&gt;The computational cost of a depthwise separable convolution with width multiplier $\alpha$:
  &lt;img src=&quot;/img/15895698351332.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;$ \alpha \in (0, 1]$ with typical settings of 1, 0.75, 0.5 and 0.25.&lt;/li&gt;
  &lt;li&gt;Width multiplier has the effect of reducing &lt;strong&gt;computational cost&lt;/strong&gt; and &lt;strong&gt;the number of parameters&lt;/strong&gt; quadratically by roughly $\alpha^2$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;resolution-multiplier-reduced-representation&quot;&gt;Resolution Multiplier: Reduced Representation&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;resolution multiplier $\rho$ is applied to the input image and the internal representation of every layer. In practice we implicitly set $\rho$ by setting the input resolution.&lt;/li&gt;
  &lt;li&gt;the computational cost for the core layers with width multiplier $\alpha$ and resolution multiplier $\alpha$:
  &lt;img src=&quot;/img/15895703362678.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;$\rho \in (0, 1]$, which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128.&lt;/li&gt;
  &lt;li&gt;Resolution multiplier has the effect of reducing &lt;strong&gt;computational cost&lt;/strong&gt; by $\rho^2$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895706506529.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 15 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/15/MobileNets/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/15/MobileNets/</guid>
        
        <category>Baseline</category>
        
        
      </item>
    
      <item>
        <title>DeepLab v3+</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.02611.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/jfzhang95/pytorch-deeplab-xception&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;encoder-decoder structure:
    &lt;ul&gt;
      &lt;li&gt;DeepLabv3 is used to encode the rich contextual information&lt;/li&gt;
      &lt;li&gt;a simple yet effective decoder module is adopted to recover the object boundaries&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;explore the Xception model and atrous separable convolution to make the proposed model faster and stronger&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;introduction&quot;&gt;Introduction&lt;/h4&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;h5 id=&quot;resnet-as-baseline&quot;&gt;ResNet as baseline&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895024156855.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;DeepLab v3 as encoder: use the last feature map before logits in the original DeepLab v3 as the encoder output (contains 256 channels and rich semantic information).&lt;/li&gt;
  &lt;li&gt;Decoder:
    &lt;ul&gt;
      &lt;li&gt;The encoder features from DeepLabv3 are usually computed with output stride = 16&lt;/li&gt;
      &lt;li&gt;first bilinearly upsampled the encoder features by a factor of 4&lt;/li&gt;
      &lt;li&gt;concatenate the upsampled features with the corresponding low-level features from the network backbone that have the same spatial resolution (e.g., Conv2 before striding in ResNet-101)
        &lt;ul&gt;
          &lt;li&gt;1 × 1 convolution on the low-level features to reduce the number of channels, since the corresponding low- level features usually contain a large number of channels (e.g., 256 or 512) which may outweigh the importance of the rich encoder features (only 256 channels in our model) and make the training harder.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;apply a few (two) 3 × 3 convolutions to refine the features&lt;/li&gt;
      &lt;li&gt;another simple bilinear upsampling by a factor of 4&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;DeepLabv3+ model has final output stride = 4
    &lt;ul&gt;
      &lt;li&gt;encoder output stride = 16
        &lt;ul&gt;
          &lt;li&gt;upsample by a factor of 4&lt;/li&gt;
          &lt;li&gt;concatenate with Conv2&lt;/li&gt;
          &lt;li&gt;another upsample by a factor of 4&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;encoder output stride = 8
        &lt;ul&gt;
          &lt;li&gt;upsample by a factor of 2&lt;/li&gt;
          &lt;li&gt;concatenate with Conv2&lt;/li&gt;
          &lt;li&gt;another upsample by a factor of 4&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;modified-aligned-xception&quot;&gt;Modified Aligned Xception&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895033728737.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;h5 id=&quot;resnet-as-baseline-1&quot;&gt;ResNet as baseline&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;reduce the channels of low level features
&lt;img src=&quot;/img/15895034376564.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;number of 3x3 convolutions / low level feature
&lt;img src=&quot;/img/15895034547360.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;detailed performance
&lt;img src=&quot;/img/15895035126473.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;xception&quot;&gt;Xception&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895037132342.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;employing Xception as network backbone improves the performance by about 2% when train output stride = eval output stride = 16 over the case where ResNet-101 is used&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;improvement-on-object-boundaries&quot;&gt;improvement on object boundaries&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15895038788124.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Chollet, F.: Xception: Deep learning with depthwise separable convolutions. In: CVPR. (2017)&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 14 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/14/Deeplab-v3+/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/14/Deeplab-v3+/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>PSPNet</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1612.01105.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Observations:&lt;/strong&gt; many errors are partially or completely related to contextual relationship and global information for different receptive field.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Solution:&lt;/strong&gt; exploit the capability of global context information by different-region-based context aggregation.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;h5 id=&quot;observation-about-failures&quot;&gt;observation about failures&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894252749235.jpg&quot; alt=&quot;-w933&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;doesn’t match co-occurrent visual patterns&lt;/li&gt;
  &lt;li&gt;confused by similar categories&lt;/li&gt;
  &lt;li&gt;ignore small-size things / discontinuous prediction on large size things&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many errors are partially or completely related to contextual relationship and global information for different receptive fields. Thus a deep network with a suitable global-scene-level prior can much improve the performance of scene parsing.&lt;/p&gt;

&lt;h5 id=&quot;pyramid-scene-parsing-network-pspnet&quot;&gt;pyramid scene parsing network (PSPNet)&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894259020057.jpg&quot; alt=&quot;-w918&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use a pretrained ResNet model with the dilated network strategy to extract the feature map. The final feature map size is 1/8 of the input image, as shown in Fig. 3(b).&lt;/li&gt;
  &lt;li&gt;pyramid pooling module for global scene prior construction upon the final-layer-feature-map of the deep neural network.&lt;/li&gt;
  &lt;li&gt;Use 1×1 convolution layer after each pyramid level to reduce the dimension of context representation to $\frac{1}{N}$ of the original one if the level size of pyramid is $N$. Then we directly upsample the low-dimension feature maps to get the same size feature as the original feature map via bilinear interpolation.&lt;/li&gt;
  &lt;li&gt;Different levels of features are concatenated as the final pyramid pooling global feature. It is followed by a convolution layer to generate the final prediction map in (d).&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;deep-supervised-loss&quot;&gt;Deep supervised loss&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894269590978.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Generating initial results by supervision with an additional loss, and learning the residue afterwards with the final loss. Thus, optimization of the deep network is decomposed into two, each is simpler to solve.&lt;/li&gt;
  &lt;li&gt;The auxiliary loss helps optimize the learning process, while the master branch loss takes the most responsibility.&lt;/li&gt;
  &lt;li&gt;In the testing phase, we abandon this auxiliary branch and only use the well optimized master branch for final prediction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;h5 id=&quot;architecture-for-pspnet&quot;&gt;Architecture for PSPNet&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894274392623.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;average pooling works better than max pooling in all settings&lt;/li&gt;
  &lt;li&gt;Pooling with pyramid parsing (B1236) outperforms that using global pooling (B1)&lt;/li&gt;
  &lt;li&gt;With dimension reduction, the performance is further enhanced.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;auxiliary-loss&quot;&gt;Auxiliary Loss&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894275459126.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;pretrained-model&quot;&gt;Pretrained model&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894276146708.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;detailed-analysis&quot;&gt;Detailed analysis&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15894276963001.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;others&quot;&gt;Others&lt;/h5&gt;
&lt;p&gt;需要注意各个方法的baseline是不是resnet
&lt;img src=&quot;/img/15894278542140.jpg&quot; alt=&quot;-w923&quot; /&gt;
&lt;img src=&quot;/img/15894279159043.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;B. Zhou, A. Khosla, A.` Lapedriza, A. Oliva, and A. Torralba. Object detectors emerge in deep scene cnns. arXiv:1412.6856, 2014. 3. (The paper shows the empirical receptive field of CNN is much smaller than the theoretical one especially on high-level layers.)&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 13 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/13/PSPNet/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/13/PSPNet/</guid>
        
        <category>Semantic Segmentation</category>
        
        <category>Context information</category>
        
        
      </item>
    
      <item>
        <title>DeepLab v3</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.05587.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;modules employing atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates.&lt;/li&gt;
  &lt;li&gt;主要还是各种tricks&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;output_stride: the ratio of input image spatial resolution to final output resolution.&lt;/p&gt;
&lt;h5 id=&quot;atrous-convolution-in-cascade&quot;&gt;atrous convolution in cascade&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15893990006461.jpg&quot; alt=&quot;-w840&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;duplicate several copies of the last ResNet block (Block 5, Block 6, Block 7)&lt;/li&gt;
  &lt;li&gt;apply atrous convolution with rates determined by the desired output stride value (output stride = 256 if no atrous convolution is applied)&lt;/li&gt;
  &lt;li&gt;multi-grid method: adopt different atrous rates within block4 to block7 in the proposed model. For example, when $\text{output stride} = 16$ and $\text{Multi Grid} = (1, 2, 4)$, the three convolutions will have $rates = 2 · (1, 2, 4) = (2, 4, 8)$ in the block4, respectively.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;atrous-convolution-in-parallel&quot;&gt;atrous convolution in parallel&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15893997101137.jpg&quot; alt=&quot;-w778&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;include batch normalization within ASPP.&lt;/li&gt;
  &lt;li&gt;image level feature: apply global average pooling on the last feature map of the model, feed the resulting image-level features to a 1 × 1 convolution with 256 filters (and batch normalization), and then bilinearly upsample the feature to the desired spatial dimension.&lt;/li&gt;
  &lt;li&gt;the improved ASPP consists of
    &lt;ul&gt;
      &lt;li&gt;one 1×1 convolution, and three 3 × 3 convolutions with rates = (6, 12, 18) when output stride = 16 (all with 256 filters and batch normalization). Note that the rates are doubled when output stride = 8.&lt;/li&gt;
      &lt;li&gt;the image-level features (the rates are doubled when output stride = 8)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The resulting features from all the branches are concatenated and pass through another 1 × 1 convolution (with 256 filters and batch normalization) before the final 1 × 1 convolution which generates the final logits.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;“poly” learning rate policy: the initial learning rate is
multiplied by $(1 −\text{maxiter})^{\text{power}}$ with $\text{power} = 0.9 $.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;upsampling logits: upsample the final logits, since downsampling the groundtruths removes the fine annotations resulting in no back-propagation of details.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;$\text{output stride}=16$ during training, and $\text{output stride}=8$ during inference to get more detailed feature map.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;cascade:
&lt;img src=&quot;/img/15894010210339.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;parallel:
&lt;img src=&quot;/img/15894010431381.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Both their best cascaded model (in Tab. 4) and ASPP model (in Tab. 6) (in both cases without DenseCRF post-processing or MS-COCO pre-training) already outperform DeepLabv2 (77.69% with DenseCRF and pretrained on MS-COCO) on the PASCAL VOC 2012 val set.&lt;/li&gt;
  &lt;li&gt;The improvement mainly comes from including and fine-tuning batch normalization parameters in the proposed models and having a better way to encode multi-scale context.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167, 2015.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 13 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/13/DeepLab-v3/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/13/DeepLab-v3/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>为什么现在的CNN模型都是在VGGNet或者ResNet上调整的？</title>
        <description>&lt;p&gt;答案来自&lt;a href=&quot;https://www.zhihu.com/question/43370067/answer/128881262&quot;&gt;知乎&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Q: 为什么现在的CNN模型都是在VGGNet或者ResNet上调整的？&lt;/p&gt;

&lt;p&gt;A:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;公开的论文需要一个标准的baseline及在baseline上改进的比较，因此大家会基于一个公认的baseline开始做实验，这样大家才比较信服。&lt;/li&gt;
  &lt;li&gt;视觉、自然语言等更专注于本领域的内部知识，而非baseline本身。因此常常是在一个base网络的基础之上进行修改，以验证自己方法的有效性。&lt;/li&gt;
  &lt;li&gt;进行基本模型的改进需要大量的实验和尝试，很有可能投入产出比比较小。对于深度学习，很大部分可以提升性能的点在于一些对于细节的精确把握。因此可以看到许多排名靠前的队伍最后讲的关键技术点似乎都是tricks。而这样精确细节的把握是需要大量的时间和计算资源的，往往在学校不可行。&lt;/li&gt;
  &lt;li&gt;AlexNet, Network in Network, VGG, GoogLeNet, Resnet等CNN网络都是图片分类网络, 都是在imagenet上1.2 million数据训练出来的。一般来说，某CNN网络在imagenet上面的分类结果越好，其deep feature的generalization能力越强，可以应用到各种CV问题。&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 10 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/10/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%8E%B0%E5%9C%A8%E7%9A%84CNN%E6%A8%A1%E5%9E%8B%E9%83%BD%E6%98%AF%E5%9C%A8VGGNet%E6%88%96%E8%80%85ResNet%E4%B8%8A%E8%B0%83%E6%95%B4%E7%9A%84/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/10/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%8E%B0%E5%9C%A8%E7%9A%84CNN%E6%A8%A1%E5%9E%8B%E9%83%BD%E6%98%AF%E5%9C%A8VGGNet%E6%88%96%E8%80%85ResNet%E4%B8%8A%E8%B0%83%E6%95%B4%E7%9A%84/</guid>
        
        <category>Q &amp; A</category>
        
        
      </item>
    
      <item>
        <title>ResNet</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;出发点：The depth of representations is of central importance for many visual recognition tasks.&lt;/li&gt;
  &lt;li&gt;问题：Deeper networks have degradation problems.&lt;/li&gt;
  &lt;li&gt;解决方案：We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. Experiments show that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;degradation-problem-of-deeper-network&quot;&gt;Degradation problem of deeper network&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15891414912328.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1 shows adding more layers to a suitably deep model leads to higher training error. This is unexpected because if the added layers are identity mapping, then a deeper model should produce no higher training error than its shallower counterpart.&lt;/p&gt;

&lt;h4 id=&quot;resnet&quot;&gt;ResNet&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15891418268694.jpg&quot; width=&quot;40%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To solve the degradation problem of deeper network, the authors proposed ResNet. They explicitly let these layers fit a residual mapping. Formally, denoting the desired underlying mapping as $H(x)$, they let the stacked nonlinear layers fit another mapping of $F(x):= H(x)−x$. The original mapping is recast into $F(x)+x$.&lt;/p&gt;

&lt;p&gt;Advantage:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;It is easier to optimize the residual mapping than to optimize the original, unreferenced mapping.
    &lt;ul&gt;
      &lt;li&gt;To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.&lt;/li&gt;
      &lt;li&gt;In real cases, it is unlikely that identity mappings are optimal, but our reformulation may help to precondition the problem. If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Identity shortcut connections add neither extra parameter nor computational complexity.&lt;/li&gt;
  &lt;li&gt;The deep residual nets can easily enjoy accuracy gains from greatly increased depth.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Network complexity: There is no fc-4096 in ResNet, which greatly reduce the complexity of ResNet. (Although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 bilion FLOPs).)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Identity mapping and projection shortcuts:
    &lt;ul&gt;
      &lt;li&gt;The identity shortcuts can be directly used when the input and output are of the same dimensions 
  &lt;img src=&quot;/img/15891425389236.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;When the dimensions increase, we consider two options: (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; (B) The projection shortcut is used to match dimensions (done by 1×1 convolutions).
  &lt;img src=&quot;/img/15891426033483.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Deeper Bottleneck Architecture&lt;br /&gt;
 The three layers are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3×3 layer a bottleneck with smaller input/output dimensions. (use option B for increasing dimensions.)
&lt;img src=&quot;/img/15891433710930.jpg&quot; width=&quot;65%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;detailed-illustration&quot;&gt;Detailed illustration&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15891422325129.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15891450870398.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;experiments&quot;&gt;Experiments&lt;/h4&gt;
&lt;h5 id=&quot;plain-network-and-resnet-zero-padding-for-increasing-dimension&quot;&gt;Plain network and ResNet (zero padding for increasing dimension)&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/img/15891427682633.jpg&quot; width=&quot;85%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15891427832726.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data. This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth.&lt;/li&gt;
  &lt;li&gt;The effectiveness of residual learning on extremely deep systems: Compared to its plain counterpart, the 34-layer ResNet reduces the top-1 error by 3.5% (Table 2), resulting from the successfully reduced training error (Fig. 4 right vs. left).&lt;/li&gt;
  &lt;li&gt;When the net is “not overly deep” (18 layers here), the 18-layer plain/residual nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;identity-vs-projection-shortcuts&quot;&gt;Identity vs. Projection Shortcuts&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/img/15891431852656.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
In Table 3 we compare three options: &lt;br /&gt;
(A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter-free &lt;br /&gt;
(B) projection shortcuts are used for increasing dimensions, and other shortcuts are identity&lt;br /&gt;
(C) all shortcuts are projections.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;B is slightly better than A.We argue that this is because the zero-padded dimensions in A indeed have no residual learning.&lt;/li&gt;
  &lt;li&gt;C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts.&lt;/li&gt;
  &lt;li&gt;But the small differences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;deeper-network&quot;&gt;Deeper network&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/img/15891439141245.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
From Table 3 and 4, the 50/101/152-layer ResNets are more accurate than the 34-layer ones by considerable margins. We do not observe the degradation problem and thus en- joy significant accuracy gains from considerably increased depth.&lt;/p&gt;

&lt;h5 id=&quot;analysis-of-layer-reponses&quot;&gt;Analysis of layer reponses&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15891442467427.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ResNets have generally smaller responses than their plain counterparts. These results support our basic motivation that the residual functions might be generally closer to zero than the non-residual functions.&lt;/li&gt;
  &lt;li&gt;The deeper ResNet has smaller magnitudes of responses, showing when there are more layers, an individual layer of ResNets tends to modify the signal less.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;exploring-over-1000-layers&quot;&gt;Exploring Over 1000 layers&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15891445047122.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
The testing result of this 1202-layer network is worse than that of our 110-layer network, although both have similar training error. We argue that this is because of overfitting. The 1202-layer network may be unnecessarily large (19.4M) for this small dataset. But combining with stronger regularization may improve results, which we will study in the future.&lt;/p&gt;
</description>
        <pubDate>Sun, 10 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/10/ResNet/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/10/ResNet/</guid>
        
        <category>Baseline</category>
        
        
      </item>
    
      <item>
        <title>RefineNet:_Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1611.06612.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;设计RefineNet，结合高层语义信息和低层细节信息，得到高分辨率的分割图&lt;/li&gt;
  &lt;li&gt;RefineNet中大量使用了ResNet中Identity mapping (both short range and long range) 的思想，保证了有效的端到端训练&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;introduction&quot;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;网络中pooling操作会降低分割图的分辨率，目前有三种解决方式：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;learn deconvolutional filters as an up-sampling operation: The deconvolution operations are not able to recover the low-level visual features which are lost after the downsampling operation in the convolution forward stage.&lt;/li&gt;
  &lt;li&gt;Deeplab系列中的atrous convolution: &lt;strong&gt;a significant cost in memory, because unlike the image subsampling methods, one must retain very large numbers of feature maps at higher resolution.&lt;/strong&gt; In practice, therefore, dilation convolution methods usually have a resolution prediction of no more than 1/8 size of the original rather than 1/4, when using a deep network.&lt;/li&gt;
  &lt;li&gt;exploits features from intermediate layers for generating high-resolution prediction (本文基于的方法)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15890589297975.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15890589494152.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15890590750133.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15890590946983.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15890591146152.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;reflection&quot;&gt;Reflection&lt;/h4&gt;
&lt;p&gt;感觉整个网络设计并没有太新颖的模块或思路，但可能是通过大量的identity mapping达成了有效的端对端训练，最后得到了非常棒的结果。如果是的话，说明设计网络时，网络各个模块能否得到充分的训练非常重要。&lt;/p&gt;
</description>
        <pubDate>Sat, 09 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/09/RefineNet-Multi-Path-Refinement-Networks-for-High-Resolution-Semantic-Segmentation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/09/RefineNet-Multi-Path-Refinement-Networks-for-High-Resolution-Semantic-Segmentation/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>DeepLab v2</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.00915.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;相比于DeepLab v1:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;atrous spatial pyramid pooling (ASPP) to deal with multiscale context and objects.&lt;/li&gt;
  &lt;li&gt;deeper convolutional neural networks (from VGG-16 to ResNet-101).&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;h5 id=&quot;atrous-convolution-for-dense-feature-extraction&quot;&gt;atrous convolution for dense feature extraction&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15889713549783.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;the number of filter parameters and the number of operations per position stay constant, but the effective filter size increases.&lt;/li&gt;
  &lt;li&gt;Pushing this approach all the way through the network could allow us to compute feature responses at the original image resolution, but this ends up being too costly. Thus the authors have adopted instead a hybrid approach that strikes a good efficiency/accuracy trade-off, using atrous convolution to increase by a factor of 4 the density of computed feature maps, followed by fast bilinear interpolation by an additional factor of 8 to recover feature maps at the original image resolution. Bilinear interpolation is sufficient in this setting because the class score maps are quite smooth.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;multiscale-image-representation&quot;&gt;multiscale image representation&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;standard multiscale processing
    &lt;ul&gt;
      &lt;li&gt;extract DCNN score maps from multiplerescaled versions of the original image using parallel DCNN branches that share the same parameters. To produce the final result, the authors bilinearly interpolate the feature maps from the parallel DCNN branches to the original image resolution and fuse them, by taking at each position the maximum response across the different scales.&lt;/li&gt;
      &lt;li&gt;do this both during training and testing.&lt;/li&gt;
      &lt;li&gt;Multiscale processing significantly improves performance, but at the cost of computing feature responses at all DCNN layers for multiple scales of input.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASPP
&lt;img src=&quot;/img/15889716579187.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15889716791787.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;fully-connected CRF&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;three main improvements compared to DeepLab v1:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;different learning policy during training&lt;/li&gt;
  &lt;li&gt;atrous spatial pyramid pooling&lt;/li&gt;
  &lt;li&gt;multi-scale processing and other factors&lt;/li&gt;
  &lt;li&gt;employment of deeper networks&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;different learning policy during training&lt;br /&gt;
employing a “poly” learning rate iter policy (the learning rate is multiplied by $(1−
\frac{\text{iter}}{\text{max_iter}} )^{\text{power}}$) is more effective than “step” learning rate (reduce the learning rate at a fixed step size).&lt;br /&gt;
 &lt;img src=&quot;/img/15889722172052.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASPP
&lt;img src=&quot;/img/15889726217656.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15889726447069.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;multi-scale processing and other factors (on ResNet 101)
&lt;img src=&quot;/img/15889728730154.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;VGG-16 vs. ResNet-101&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;DeepLab based on ResNet-101 delivers better segmentation results along object boundaries than employing VGG- 16. &lt;br /&gt;
 &lt;img src=&quot;/img/15889733334375.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
 &lt;img src=&quot;/img/15889732341678.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;Post-processing the ResNet-101 result with a CRF further improves the segmentation result. (有提高，但不大)&lt;br /&gt;
 &lt;img src=&quot;/img/15889730892561.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;VGG-16. K. Simonyan and A. Zisserman, “Very deep convolutional net- works for large-scale image recognition,” in ICLR, 2015.&lt;/li&gt;
  &lt;li&gt;ResNet. K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” arXiv:1512.03385, 2015.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 08 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/08/DeepLab-v2/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/08/DeepLab-v2/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
  </channel>
</rss>
