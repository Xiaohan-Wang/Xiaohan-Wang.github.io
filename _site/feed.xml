<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 10 May 2020 17:20:25 -0400</pubDate>
    <lastBuildDate>Sun, 10 May 2020 17:20:25 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>为什么现在的CNN模型都是在VGGNet或者ResNet上调整的？</title>
        <description>&lt;p&gt;答案来自&lt;a href=&quot;https://www.zhihu.com/question/43370067/answer/128881262&quot;&gt;知乎&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Q: 为什么现在的CNN模型都是在VGGNet或者ResNet上调整的？&lt;/p&gt;

&lt;p&gt;A:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;公开的论文需要一个标准的baseline及在baseline上改进的比较，因此大家会基于一个公认的baseline开始做实验，这样大家才比较信服。&lt;/li&gt;
  &lt;li&gt;视觉、自然语言等更专注于本领域的内部知识，而非baseline本身。因此常常是在一个base网络的基础之上进行修改，以验证自己方法的有效性。&lt;/li&gt;
  &lt;li&gt;进行基本模型的改进需要大量的实验和尝试，很有可能投入产出比比较小。对于深度学习，很大部分可以提升性能的点在于一些对于细节的精确把握。因此可以看到许多排名靠前的队伍最后讲的关键技术点似乎都是tricks。而这样精确细节的把握是需要大量的时间和计算资源的，往往在学校不可行。&lt;/li&gt;
  &lt;li&gt;AlexNet, Network in Network, VGG, GoogLeNet, Resnet等CNN网络都是图片分类网络, 都是在imagenet上1.2 million数据训练出来的。一般来说，某CNN网络在imagenet上面的分类结果越好，其deep feature的generalization能力越强，可以应用到各种CV问题。&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 10 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/10/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%8E%B0%E5%9C%A8%E7%9A%84CNN%E6%A8%A1%E5%9E%8B%E9%83%BD%E6%98%AF%E5%9C%A8VGGNet%E6%88%96%E8%80%85ResNet%E4%B8%8A%E8%B0%83%E6%95%B4%E7%9A%84/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/10/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%8E%B0%E5%9C%A8%E7%9A%84CNN%E6%A8%A1%E5%9E%8B%E9%83%BD%E6%98%AF%E5%9C%A8VGGNet%E6%88%96%E8%80%85ResNet%E4%B8%8A%E8%B0%83%E6%95%B4%E7%9A%84/</guid>
        
        <category>Q &amp; A</category>
        
        
      </item>
    
      <item>
        <title>ResNet</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;出发点：The depth of representations is of central importance for many visual recognition tasks.&lt;/li&gt;
  &lt;li&gt;问题：Deeper networks have degradation problems.&lt;/li&gt;
  &lt;li&gt;解决方案：We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. Experiments show that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;degradation-problem-of-deeper-network&quot;&gt;Degradation problem of deeper network&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15891414912328.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1 shows adding more layers to a suitably deep model leads to higher training error. This is unexpected because if the added layers are identity mapping, then a deeper model should produce no higher training error than its shallower counterpart.&lt;/p&gt;

&lt;h4 id=&quot;resnet&quot;&gt;ResNet&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15891418268694.jpg&quot; width=&quot;40%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To solve the degradation problem of deeper network, the authors proposed ResNet. They explicitly let these layers fit a residual mapping. Formally, denoting the desired underlying mapping as $H(x)$, they let the stacked nonlinear layers fit another mapping of $F(x):= H(x)−x$. The original mapping is recast into $F(x)+x$.&lt;/p&gt;

&lt;p&gt;Advantage:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;It is easier to optimize the residual mapping than to optimize the original, unreferenced mapping.
    &lt;ul&gt;
      &lt;li&gt;To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.&lt;/li&gt;
      &lt;li&gt;In real cases, it is unlikely that identity mappings are optimal, but our reformulation may help to precondition the problem. If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Identity shortcut connections add neither extra parameter nor computational complexity.&lt;/li&gt;
  &lt;li&gt;The deep residual nets can easily enjoy accuracy gains from greatly increased depth.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Network complexity: There is no fc-4096 in ResNet, which greatly reduce the complexity of ResNet. (Although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 bilion FLOPs).)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Identity mapping and projection shortcuts:
    &lt;ul&gt;
      &lt;li&gt;The identity shortcuts can be directly used when the input and output are of the same dimensions 
  &lt;img src=&quot;/img/15891425389236.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;When the dimensions increase, we consider two options: (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; (B) The projection shortcut is used to match dimensions (done by 1×1 convolutions).
  &lt;img src=&quot;/img/15891426033483.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Deeper Bottleneck Architecture&lt;br /&gt;
 The three layers are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3×3 layer a bottleneck with smaller input/output dimensions. (use option B for increasing dimensions.)
&lt;img src=&quot;/img/15891433710930.jpg&quot; width=&quot;65%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;detailed-illustration&quot;&gt;Detailed illustration&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15891422325129.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15891450870398.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;experiments&quot;&gt;Experiments&lt;/h4&gt;
&lt;h5 id=&quot;plain-network-and-resnet-zero-padding-for-increasing-dimension&quot;&gt;Plain network and ResNet (zero padding for increasing dimension)&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/img/15891427682633.jpg&quot; width=&quot;85%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15891427832726.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data. This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth.&lt;/li&gt;
  &lt;li&gt;The effectiveness of residual learning on extremely deep systems: Compared to its plain counterpart, the 34-layer ResNet reduces the top-1 error by 3.5% (Table 2), resulting from the successfully reduced training error (Fig. 4 right vs. left).&lt;/li&gt;
  &lt;li&gt;When the net is “not overly deep” (18 layers here), the 18-layer plain/residual nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;identity-vs-projection-shortcuts&quot;&gt;Identity vs. Projection Shortcuts&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/img/15891431852656.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
In Table 3 we compare three options: &lt;br /&gt;
(A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter-free &lt;br /&gt;
(B) projection shortcuts are used for increasing dimensions, and other shortcuts are identity&lt;br /&gt;
(C) all shortcuts are projections.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;B is slightly better than A.We argue that this is because the zero-padded dimensions in A indeed have no residual learning.&lt;/li&gt;
  &lt;li&gt;C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts.&lt;/li&gt;
  &lt;li&gt;But the small differences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;deeper-network&quot;&gt;Deeper network&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/img/15891439141245.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
From Table 3 and 4, the 50/101/152-layer ResNets are more accurate than the 34-layer ones by considerable margins. We do not observe the degradation problem and thus en- joy significant accuracy gains from considerably increased depth.&lt;/p&gt;

&lt;h5 id=&quot;analysis-of-layer-reponses&quot;&gt;Analysis of layer reponses&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15891442467427.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ResNets have generally smaller responses than their plain counterparts. These results support our basic motivation that the residual functions might be generally closer to zero than the non-residual functions.&lt;/li&gt;
  &lt;li&gt;The deeper ResNet has smaller magnitudes of responses, showing when there are more layers, an individual layer of ResNets tends to modify the signal less.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;exploring-over-1000-layers&quot;&gt;Exploring Over 1000 layers&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15891445047122.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
The testing result of this 1202-layer network is worse than that of our 110-layer network, although both have similar training error. We argue that this is because of overfitting. The 1202-layer network may be unnecessarily large (19.4M) for this small dataset. But combining with stronger regularization may improve results, which we will study in the future.&lt;/p&gt;
</description>
        <pubDate>Sun, 10 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/10/ResNet/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/10/ResNet/</guid>
        
        <category>CV paperlist</category>
        
        
      </item>
    
      <item>
        <title>RefineNet:_Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1611.06612.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;设计RefineNet，结合高层语义信息和低层细节信息，得到高分辨率的分割图&lt;/li&gt;
  &lt;li&gt;RefineNet中大量使用了ResNet中Identity mapping (both short range and long range) 的思想，保证了有效的端到端训练&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;introduction&quot;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;网络中pooling操作会降低分割图的分辨率，目前有三种解决方式：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;learn deconvolutional filters as an up-sampling operation: The deconvolution operations are not able to recover the low-level visual features which are lost after the downsampling operation in the convolution forward stage.&lt;/li&gt;
  &lt;li&gt;Deeplab系列中的atrous convolution: &lt;strong&gt;a significant cost in memory, because unlike the image subsampling methods, one must retain very large numbers of feature maps at higher resolution.&lt;/strong&gt; In practice, therefore, dilation convolution methods usually have a resolution prediction of no more than 1/8 size of the original rather than 1/4, when using a deep network.&lt;/li&gt;
  &lt;li&gt;exploits features from intermediate layers for generating high-resolution prediction (本文基于的方法)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15890589297975.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15890589494152.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15890590750133.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15890590946983.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15890591146152.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;reflection&quot;&gt;Reflection&lt;/h4&gt;
&lt;p&gt;感觉整个网络设计并没有太新颖的模块或思路，但可能是通过大量的identity mapping达成了有效的端对端训练，最后得到了非常棒的结果。如果是的话，说明设计网络时，网络各个模块能否得到充分的训练非常重要。&lt;/p&gt;
</description>
        <pubDate>Sat, 09 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/09/RefineNet-Multi-Path-Refinement-Networks-for-High-Resolution-Semantic-Segmentation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/09/RefineNet-Multi-Path-Refinement-Networks-for-High-Resolution-Semantic-Segmentation/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>DeepLab v2</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.00915.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;相比于DeepLab v1:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;atrous spatial pyramid pooling (ASPP) to deal with multiscale context and objects.&lt;/li&gt;
  &lt;li&gt;deeper convolutional neural networks (from VGG-16 to ResNet-101).&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;h5 id=&quot;atrous-convolution-for-dense-feature-extraction&quot;&gt;atrous convolution for dense feature extraction&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15889713549783.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;the number of filter parameters and the number of operations per position stay constant, but the effective filter size increases.&lt;/li&gt;
  &lt;li&gt;Pushing this approach all the way through the network could allow us to compute feature responses at the original image resolution, but this ends up being too costly. Thus the authors have adopted instead a hybrid approach that strikes a good efficiency/accuracy trade-off, using atrous convolution to increase by a factor of 4 the density of computed feature maps, followed by fast bilinear interpolation by an additional factor of 8 to recover feature maps at the original image resolution. Bilinear interpolation is sufficient in this setting because the class score maps are quite smooth.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;multiscale-image-representation&quot;&gt;multiscale image representation&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;standard multiscale processing
    &lt;ul&gt;
      &lt;li&gt;extract DCNN score maps from multiplerescaled versions of the original image using parallel DCNN branches that share the same parameters. To produce the final result, the authors bilinearly interpolate the feature maps from the parallel DCNN branches to the original image resolution and fuse them, by taking at each position the maximum response across the different scales.&lt;/li&gt;
      &lt;li&gt;do this both during training and testing.&lt;/li&gt;
      &lt;li&gt;Multiscale processing significantly improves performance, but at the cost of computing feature responses at all DCNN layers for multiple scales of input.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASPP
&lt;img src=&quot;/img/15889716579187.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15889716791787.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;fully-connected CRF&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;three main improvements compared to DeepLab v1:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;different learning policy during training&lt;/li&gt;
  &lt;li&gt;atrous spatial pyramid pooling&lt;/li&gt;
  &lt;li&gt;multi-scale processing and other factors&lt;/li&gt;
  &lt;li&gt;employment of deeper networks&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;different learning policy during training&lt;br /&gt;
employing a “poly” learning rate iter policy (the learning rate is multiplied by $(1−
\frac{\text{iter}}{\text{max_iter}} )^{\text{power}}$) is more effective than “step” learning rate (reduce the learning rate at a fixed step size).&lt;br /&gt;
 &lt;img src=&quot;/img/15889722172052.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ASPP
&lt;img src=&quot;/img/15889726217656.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15889726447069.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;multi-scale processing and other factors (on ResNet 101)
&lt;img src=&quot;/img/15889728730154.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;VGG-16 vs. ResNet-101&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;DeepLab based on ResNet-101 delivers better segmentation results along object boundaries than employing VGG- 16. &lt;br /&gt;
 &lt;img src=&quot;/img/15889733334375.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;
 &lt;img src=&quot;/img/15889732341678.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;Post-processing the ResNet-101 result with a CRF further improves the segmentation result. (有提高，但不大)&lt;br /&gt;
 &lt;img src=&quot;/img/15889730892561.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;VGG-16. K. Simonyan and A. Zisserman, “Very deep convolutional net- works for large-scale image recognition,” in ICLR, 2015.&lt;/li&gt;
  &lt;li&gt;ResNet. K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” arXiv:1512.03385, 2015.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 08 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/08/DeepLab-v2/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/08/DeepLab-v2/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>DeepLab v1</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1412.7062.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;two technical hurdles in the application of DCNNs to image labeling tasks: signal downsampling, and spatial ‘insensitivity’ (invariance).&lt;/li&gt;
  &lt;li&gt;signal downsampling: reduction of signal resolution incurred by the repeated max-pooling -&amp;gt; employ the atrous algorithm (就是dilated convolution)&lt;/li&gt;
  &lt;li&gt;spatial insensitivity: object-centric decisions from a classifier requires invariance to spatial transformations, inherently limiting the spatial accuracy of the DCNN model -&amp;gt; capture fine details by employing a fully-connected Conditional Random Field (CRF) + multi-scale prediction&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;修改自VGG-16&lt;/li&gt;
  &lt;li&gt;decouple the DCNN and CRF training stages, assuming the DCNN unary terms are fixed when setting the CRF parameters&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/img/15888811976149.jpg&quot; alt=&quot;-w724&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;efficient-feature-extraction-atrous-algorithm--reducing-model&quot;&gt;efficient feature extraction: atrous algorithm + reducing model&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;To compute scores more densely at target stride of 8 pixels:&lt;br /&gt;
 &lt;img src=&quot;/img/15888799732428.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;
    &lt;ol&gt;
      &lt;li&gt;convert the fully-connected layers of VGG-16 into convolutional ones and run the network in a convolutional fashion on the image at its original resolution&lt;/li&gt;
      &lt;li&gt;instead of subsampling after the last two max-pooling layers in the network, use the convolutional filters with an input stride of 2 or 4 pixels, respectively.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;After converting the network to a fully convolutional one, the first fully connected layer has 4,096 filters of large 7×7 spatial size and becomes the computational bottleneck.
    &lt;ol&gt;
      &lt;li&gt;Use filters with 4×4 (or 3×3) spatial size but with the same receptive fields.&lt;/li&gt;
      &lt;li&gt;reducing the number of channels at the fully connected layers from 4,096 down to 1,024.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;detail-boundary-recovery-fully-connected-crf--multi-scale-prediction&quot;&gt;detail boundary recovery: fully-connected CRF + multi-scale prediction&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;fully-connected CRF 
 &lt;img src=&quot;/img/15888811265917.jpg&quot; alt=&quot;-w723&quot; /&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Energy function:&lt;br /&gt;
 &lt;img src=&quot;/img/15888817215422.jpg&quot; width=&quot;40%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;first term:&lt;br /&gt;
 &lt;img src=&quot;/img/15888818741237.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;second term:&lt;br /&gt;
 &lt;img src=&quot;/img/15888819146749.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
 &lt;img src=&quot;/img/15888820310003.jpg&quot; width=&quot;25%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
 the kernels are:&lt;br /&gt;
 &lt;img src=&quot;/img/15888821802918.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
 The first kernel forces pixels with similar color and position to have similar labels, while the second kernel only considers spatial proximity when enforcing smoothness.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;multi-scale prediction
 attach to the input image and the output of each of the first four max pooling layers a two-layer MLP (first layer: 128 3x3 convolutional filters, second layer: 128 1x1 convolutional fil- ters) whose feature map is concatenated to the main network’s last layer feature map. The aggregate feature map fed into the softmax layer is thus enhanced by 5 * 128 = 640 channels.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15888828891435.jpg&quot; alt=&quot;-w794&quot; /&gt;
&lt;img src=&quot;/img/15888829216375.jpg&quot; alt=&quot;-w739&quot; /&gt;
&lt;img src=&quot;/img/15888829687129.jpg&quot; alt=&quot;-w702&quot; /&gt;
&lt;img src=&quot;/img/15888830739237.jpg&quot; alt=&quot;-w725&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://hellodfan.com/2018/01/22/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E8%AE%BA%E6%96%87-DeepLab%E7%B3%BB%E5%88%97/&quot;&gt;部分翻译&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Krahenbuhl, P. and Koltun, V. Efficient inference in fully connected crfs with gaussian edge poten- tials. In NIPS, 2011.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 07 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/07/Deeplab-v1/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/07/Deeplab-v1/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>Dilated convolutions</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.07122.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Dilated convolution operator, which can expend the receptive field without losing resolution or coverage, is very suitable for dense prediction task.&lt;/li&gt;
  &lt;li&gt;multi-scale context module, can reliably increases accuracy when plugged into existing semantic segmentation systems.&lt;/li&gt;
  &lt;li&gt;Replace pooling layer (designed for classification task) with dilated convolution (designed for segmentation task) can increase accuracy.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;dilated convolutions
    &lt;ul&gt;
      &lt;li&gt;$l$-dilated convolution: The familiar discrete convolution is simply the 1-dilated convolution.&lt;br /&gt;
 &lt;img src=&quot;/img/15887283730533.jpg&quot; width=&quot;35%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
 use $∗_l$ to represent an $l$-dilated convolution&lt;/li&gt;
      &lt;li&gt;Let $F_0, F_1, \cdots , F_{n−1} : Z^2 \rightarrow R$ be discrete functions and let $k_0, k_1, \cdots, k_{n−2} : Ω_1 \rightarrow R$ ($Ω_r = [−r, r]^2 ∩ Z^2$) be discrete 3×3 filter. Consider applying the filters with exponentially increasing dilation: 
  &lt;script type=&quot;math/tex&quot;&gt;F_{i+1} = F_i ∗_{2^i}k_i\text{   for   } i = 0, 1, \cdots , n − 2&lt;/script&gt;, then the size of the receptive field of each element in $F_{i+1}$ is $(2^{i+2} − 1)×(2^{i+2} − 1)$.
&lt;img src=&quot;/img/15887296456656.jpg&quot; alt=&quot;-w709&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;context module
    &lt;blockquote&gt;
      &lt;p&gt;The context module is designed to increase the performance of dense prediction architectures by aggregating multi-scale contextual information. The module takes C feature maps as input and produces C feature maps as output. The input and output have the same form, thus the module can be plugged into existing dense prediction architectures.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;module architecture&lt;/p&gt;

        &lt;p&gt;here truncation is a ReLU function $f(\cdot)=max(\cdot, 0)$.&lt;br /&gt;
&lt;img src=&quot;/img/15887338289922.jpg&quot; alt=&quot;-w656&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;initialization&lt;/p&gt;

        &lt;p&gt;Experiments revealed that standard initialization procedures (random initialization) do not readily support the training of the module.&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;Basic: identity initialization&lt;br /&gt;
  &lt;img src=&quot;/img/15887343089814.jpg&quot; width=&quot;30%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
          &lt;li&gt;Large:&lt;br /&gt;
  &lt;img src=&quot;/img/15887343601296.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;front end
    &lt;ul&gt;
      &lt;li&gt;adapted the VGG-16 network for dense prediction and removed the last two pooling and striding layers. Specifically, each of these pooling and striding layers was removed and convolutions in all subsequent layers were dilated by a factor of 2 for each pooling layer that was ablated.&lt;/li&gt;
      &lt;li&gt;use reflection padding: the buffer zone is filled by reflecting the image about each edge.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Front end
&lt;img src=&quot;/img/15887346292420.jpg&quot; alt=&quot;-w599&quot; /&gt;
&lt;img src=&quot;/img/15887346668224.jpg&quot; alt=&quot;-w609&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;context module
&lt;img src=&quot;/img/15887347716594.jpg&quot; alt=&quot;-w599&quot; /&gt;
&lt;img src=&quot;/img/15887348908756.jpg&quot; alt=&quot;-w606&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 05 May 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/05/05/Dilated-convolutions/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/05/Dilated-convolutions/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>U-Net</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1505.04597.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/milesial/Pytorch-UNet&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;contracting path + expending path + skip connection&lt;/li&gt;
  &lt;li&gt;strong data augmentation, works well for very few images (it was designed for biomedical images)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15882170880287.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/15882176961707.jpg&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It use valid convolution, thus the resolution gets lower and lower in the contracting path and expending path.&lt;/li&gt;
  &lt;li&gt;Because of the high resolution of biomedical images, overlap-tile strategy is used.&lt;/li&gt;
  &lt;li&gt;To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;contracting path:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels.&lt;/li&gt;
  &lt;li&gt;an upsampling of the feature map by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU.&lt;/li&gt;
  &lt;li&gt;At the final layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Training objective:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;softmax + cross entropy&lt;/li&gt;
  &lt;li&gt;class balancing (it’s important because of the large background)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;/h4&gt;
&lt;p&gt;Teach the network the desired invariance and robustness properties, when only few training samples are available:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;shift&lt;/li&gt;
  &lt;li&gt;rotation&lt;/li&gt;
  &lt;li&gt;gray value deformations&lt;/li&gt;
  &lt;li&gt;random elastic deformation&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size.&lt;/li&gt;
  &lt;li&gt;favor large input tiles over a large batch size and hence reduce the batch to a single image.&lt;/li&gt;
  &lt;li&gt;use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step.&lt;/li&gt;
  &lt;li&gt;Xavier initialization.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;time&quot;&gt;Time&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Training time: 10 hours on a NVidia Titan GPU (6 GB)&lt;/li&gt;
  &lt;li&gt;Inference time: Segmentation of a 512x512 image takes less than a second on a recent GPU&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 29 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/29/U-Net/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/29/U-Net/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>Conditional Generative Adversarial Network for Structured Domain Adaptation</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;We shouldn’t assume that the source and target domains share same intermediate feature space(s), i.e. using loss to obtain domain-invariant features.&lt;/li&gt;
  &lt;li&gt;Use a conditional generator to transform source feature maps into target-like, and trained on the target-like feature maps and original labels (和pixel level domain adaptation类似，但pixel level DA将source images转化为target-like，而本文将source features转化为target-like).&lt;/li&gt;
  &lt;li&gt;文章中提到 without relying on the assumption that the source and target domains share a same prediction function in a domain-invariant feature space。 实际只完成了not in a domain-invariant feature space， 本质上还是same prediction function (source和target domain的encoder和decoder都相同)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15881099962969.jpg&quot; alt=&quot;-w913&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;a conditional generator to generate residual features, to transform features of synthetic images to real-image like features.
    &lt;ul&gt;
      &lt;li&gt;a noise map $z$ is addd for randomness to create an unlimited number of training samples.&lt;/li&gt;
      &lt;li&gt;expect that $x_f$ preserves the semantic of the source feature map $x_s$, meanwhile appears as if it were extracted from a target domain image, i.e., a real image.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;a discriminator to distinguish adapted source features and real target features.&lt;/li&gt;
  &lt;li&gt;task-specific loss (decoder part)
    &lt;ul&gt;
      &lt;li&gt;train T with both adapted and non-adapted source feature maps (Training T solely on adapted feature maps leads to similar performance, but requires many runs with different initializations and learning rates due to the instability of the GAN).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;overall minimax objective:&lt;br /&gt;
&lt;img src=&quot;/img/15881108285954.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;overall performance: 涨幅非常大&lt;br /&gt;
&lt;img src=&quot;/img/15881108796823.jpg&quot; alt=&quot;-w905&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;amount of synthetic data
 &lt;img src=&quot;/img/15881109617007.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ablation Studies&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The effectiveness of conditional generator&lt;br /&gt;
 &lt;img src=&quot;/img/15881110275006.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Different lower layers for learning generator
 &lt;img src=&quot;/img/15881111038400.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;On the number of residual blocks （上图）&lt;/li&gt;
      &lt;li&gt;How much does the noise channel contribute?
 &lt;img src=&quot;/img/15881111498987.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 28 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/28/Conditional-Generative-Adversarial-Network-for-Structured-Domain-Adaptation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/28/Conditional-Generative-Adversarial-Network-for-Structured-Domain-Adaptation/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>SegNet:_A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.00561.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;In practice, we have to consider the cost of memory and computational time.&lt;/li&gt;
  &lt;li&gt;Most recent deep architectures for segmentation have identical encoder networks, i.e VGG16, but differ in the form of the decoder network, training and inference.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15880309668474.jpg&quot; alt=&quot;-w901&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Encoder:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The encoder network consists of 13 convolutional layers which correspond to the first 13 convolutional layers in the VGG16 network.&lt;/li&gt;
  &lt;li&gt;Initialize the training process from weights trained for classification on large datasets.&lt;/li&gt;
  &lt;li&gt;Benifit: Removing the fully connected layers of VGG16 makes the SegNet encoder network significantly smaller [reduces the number of parameters in the SegNet encoder network significantly (from 134M to 14.7M)] and easier to train.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Decoder:
&lt;img src=&quot;/img/15880312579950.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The appropriate decoder in the decoder network upsamples its input feature map(s) using the memorized max-pooling indices from the corresponding encoder feature map(s). This step produces sparse feature map(s).&lt;/li&gt;
  &lt;li&gt;These feature maps are then convolved with a trainable decoder filter bank to produce dense feature maps.&lt;/li&gt;
  &lt;li&gt;Except for the last decoder (which corresponds to the first encoder), the other decoders in the network produce feature maps with the same number of size and channels as their encoder inputs.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;p&gt;class balancing&lt;/p&gt;

</description>
        <pubDate>Mon, 27 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/27/SegNet-A-Deep-Convolutional-Encoder-Decoder-Architecture-for-Image-Segmentation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/27/SegNet-A-Deep-Convolutional-Encoder-Decoder-Architecture-for-Image-Segmentation/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
      <item>
        <title>Fully Convolutional Networks for Semantic Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1411.4038.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/wkentaro/pytorch-fcn&quot;&gt;code (pytorch)&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;将分类网络中的全连接层变为对应的卷积层，从而得到全卷积网络。全卷积网络可用于实现如语义分割的稠密预测任务。添加skip-connecting结合deep, coarse, semantic information和shallow, fine, appearance information。&lt;/p&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;全连接层变为卷积层
&lt;img src=&quot;/img/15876140745604.jpg&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;view fully connected layers as convolutions with kernels that cover their entire input regions.&lt;/li&gt;
      &lt;li&gt;优点：When receptive fields overlap significantly, both feedforward computation and back propagation are much more efficient when computed layer-by-layer over an entire image instead of independently patch-by-patch. （receptive field: locations in the image that locations in higher layers are path-connected to.）&lt;/li&gt;
      &lt;li&gt;问题：由于subsampling, output的分辨率远低于input&lt;br /&gt;
 &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;转置卷积增大分辨率(what)+skip connection结合低层位置信息(where)
&lt;img src=&quot;/img/15876162869041.jpg&quot; alt=&quot;-w870&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;转置卷积能实现最好的上采样效果，通过learning可以实现非线性的上采样&lt;/li&gt;
      &lt;li&gt;将upsampled结果 (deep, coarse, semantic information) 和skip connection结果 (shallow, fine, appearance information) 加起来，得到combine what and where的结果&lt;/li&gt;
      &lt;li&gt;Initialize the 2× upsampling to bilinear interpolation, but allow the parameters to be learned）。 但是，在FCN-8s后，继续融合低层信息取得的结果基本不变。因此，最后几层直接使用bilinear interpolation。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;h5 id=&quot;评价指标&quot;&gt;评价指标&lt;/h5&gt;
&lt;p&gt;Let $n_{ij}$ be the number of pixels of class $i$ predicted to belong to class $j$, where there are $n_{cl}$ different classes, and let $t_i=\sum_jn_{ij}$ be the total number of pixels of class $i$.
&lt;img src=&quot;/img/15876175187934.jpg&quot; width=&quot;40%&quot; height=&quot;50%&quot; /&gt;&lt;/p&gt;
&lt;h5 id=&quot;结果&quot;&gt;结果&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15876184066437.jpg&quot; width=&quot;50%&quot; height=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;使用classification的网络参数作为预训练模型，在此基础上fine-tune网络以用于semantic segmentation任务。&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 22 Apr 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/04/22/Fully-Convolutional-Networks-for-Semantic-Segmentation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/04/22/Fully-Convolutional-Networks-for-Semantic-Segmentation/</guid>
        
        <category>Semantic Segmentation</category>
        
        
      </item>
    
  </channel>
</rss>
