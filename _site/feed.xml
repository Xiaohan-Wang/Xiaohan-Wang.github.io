<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 11 Jan 2020 20:58:56 -0500</pubDate>
    <lastBuildDate>Sat, 11 Jan 2020 20:58:56 -0500</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>python各类图像库</title>
        <description>&lt;h3 id=&quot;short-version&quot;&gt;short version&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;类型&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;顺序&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;size&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;scale&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;cv2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;numpy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;BGR&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;(h,w,c)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;255(uint8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;PIL&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;Image/numpy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;RGB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;(w,h) / (h,w,c)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;- / 255(uint8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;skimage&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;numpy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;RGB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;(h,w,c)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;rgb:255(uint8) / gray:1(float)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;cv2&quot;&gt;cv2&lt;/h3&gt;
&lt;h4 id=&quot;ioshow&quot;&gt;i/o/show&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#读入图像  
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
  
&lt;span class=&quot;c1&quot;&gt;#显示图像  
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'src'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitkey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;#保存图像  
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imwrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'test.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  

&lt;span class=&quot;c1&quot;&gt;#归一化
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;float&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;255.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;font size=&quot;3&quot;&gt;`imshow` should be followed by `waitKey` function which displays the image for specified milliseconds. **Otherwise, it won’t display the image**. For example, `waitKey(0)` will display the window infinitely until any keypress (it is suitable for image display). waitKey(25) will display a frame for 25 ms, after which display will be automatically closed. (If you put it in a loop to read videos, it will display the video frame-by-frame)&lt;/font&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;attribute&quot;&gt;attribute&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;numpy array&lt;/li&gt;
  &lt;li&gt;默认读入彩色图像
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gray = cv2.imread('1.jpg',cv2.IMREAD_GRAYSCALE)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;channel顺序：BGR
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;im2 = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;img.shape: (h,w,c)&lt;/li&gt;
  &lt;li&gt;img.dtype: uint8(255)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pil&quot;&gt;PIL&lt;/h3&gt;
&lt;h4 id=&quot;ioshow-1&quot;&gt;i/o/show&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;PIL&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#读入图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#显示图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#保存图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'2.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;image对象&quot;&gt;Image对象&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;PIL读入图片后生成Image对象&lt;/li&gt;
  &lt;li&gt;img.size: (w, h)&lt;/li&gt;
  &lt;li&gt;img.mode: 灰度L / 真彩色RGB&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;image转numpy矩阵&quot;&gt;Image转numpy矩阵&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;arr = np.array(img) &lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;channel顺序：RGB&lt;/li&gt;
  &lt;li&gt;arr.shape: (h,w,c)&lt;/li&gt;
  &lt;li&gt;arr.dtype: uint8 (255)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;new_img = Image.fromarray(arr)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;skimage&quot;&gt;skimage&lt;/h3&gt;
&lt;h4 id=&quot;ioshow-2&quot;&gt;i/o/show&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;skimage&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#读入图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#显示图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#保存图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imsave&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;attribute-1&quot;&gt;attribute&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;numpy array&lt;/li&gt;
  &lt;li&gt;channel顺序：RGB&lt;/li&gt;
  &lt;li&gt;img.shape: (h,w,c)&lt;/li&gt;
  &lt;li&gt;img.dtype: RGB uint8 (255) / gray float (1)&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 08 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/08/python%E5%90%84%E7%B1%BB%E5%9B%BE%E5%83%8F%E5%BA%93/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/08/python%E5%90%84%E7%B1%BB%E5%9B%BE%E5%83%8F%E5%BA%93/</guid>
        
        <category>python</category>
        
        
      </item>
    
      <item>
        <title>Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks</title>
        <description>&lt;h3 id=&quot;idea&quot;&gt;Idea&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;适用于domain间存在low-level difference的domain adaptation&lt;/li&gt;
  &lt;li&gt;不需要domain间的corresponding pairs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15785106924617.jpg&quot; alt=&quot;-w857&quot; /&gt;
&lt;img src=&quot;/img/15785107411179.jpg&quot; alt=&quot;-w50&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;主要由discriminator(D)来学习，classifier(T: task)和content-similarity loss主要用来 stabalize GAN的训练&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context-similarity loss: PMSE (pairwise mean squared error)
    &lt;blockquote&gt;
      &lt;p&gt;This loss allows the model to learn to reproduce the overall shape of the objects being modeled without wasting modeling power on the absolute color or intensity of the inputs, while allowing our adversarial training to change the object in a consistent way.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reflection&quot;&gt;Reflection&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;T的参与感觉让domain adaptation变成了迎合任务的domain adaptation, 而不是单纯的pixel-level domain adaptation. (也有优势，在对应任务上应该有更好的表现)&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 08 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/08/Unsupervised-Pixel-Level-Domain-Adaptation-with-GAN/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/08/Unsupervised-Pixel-Level-Domain-Adaptation-with-GAN/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Pixel-Level Domain Transfer</title>
        <description>&lt;h3 id=&quot;idea&quot;&gt;Idea&lt;/h3&gt;
&lt;p&gt;transfer images in one domain into another domain&lt;/p&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15784545847019.jpg&quot; alt=&quot;-w744&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;real/fake discriminator: 判断图像是否来自target domain
&lt;img src=&quot;/img/15784548684906.jpg&quot; alt=&quot;-w727&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;domain (semantic) discriminator: 判断是否和source domain图像具有相同semantic information
&lt;img src=&quot;/img/15784548946537.jpg&quot; alt=&quot;-w729&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;converter
 &lt;img src=&quot;/img/15784556404369.jpg&quot; alt=&quot;-w709&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15784549145475.jpg&quot; alt=&quot;-w771&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reflection&quot;&gt;Reflection&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;作者构建的dataset将source domain和target domain中的图像make pair，从而学习image transfer。但general的DA任务并不会有source和target的image pair，因此很难直接使用&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MSE和discriminator：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;MSE容易产生blurry images, 因为其假设pixels服从Gaussian distribution&lt;/li&gt;
      &lt;li&gt;MSE never allow a small geometric miss-alignment&lt;/li&gt;
      &lt;li&gt;当有关联，但又不容易用公式描述时，可以尝试discriminator&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 07 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/07/Pixel-Level-Domain-Transfer/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/07/Pixel-Level-Domain-Transfer/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Unbiased Look at Dataset Bias</title>
        <description>&lt;h3 id=&quot;promise-and-perils-of-visual-dataset&quot;&gt;Promise and Perils of Visual Dataset&lt;/h3&gt;
&lt;h4 id=&quot;promise&quot;&gt;Promise&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Dataset是CV取得progress的重要因素（甚至超过算法的影响）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;让CV更像一门实验科学（rather than a black art）&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;perils&quot;&gt;Perils&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;(good finetune + bad approach) 表现好于 (bad finetune + good approach) =&amp;gt; 难以决定new approach的真正表现&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;pay too much attention on “winning” a competition, 但其实提高可能很微小 =&amp;gt; 我们应该将performance number作为一个check，而不是competiton，从而给new approach留出发展的机会&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;the-rise-of-modern-dataset&quot;&gt;The rise of modern dataset&lt;/h3&gt;
&lt;p&gt;repeat: one dataset -&amp;gt; being rejected due to its perceived biases -&amp;gt; new dataset -&amp;gt; being rejected due to its perceived biases -&amp;gt; …&lt;/p&gt;

&lt;h3 id=&quot;dataset-biases&quot;&gt;Dataset Biases&lt;/h3&gt;
&lt;p&gt;Instead of representing the visual world, datasets have become closed world onto themselves.&lt;/p&gt;
&lt;h4 id=&quot;selection-bias&quot;&gt;selection bias&lt;/h4&gt;
&lt;p&gt;datasets often prefer particular kinds of images (e.g. street scenes, or nature scenes, or images retrieved via Internet keyword searches)&lt;/p&gt;
&lt;h4 id=&quot;capture-bias&quot;&gt;capture bias&lt;/h4&gt;
&lt;p&gt;photographers tending to take pictures of objects in similar ways (although this bias might be similar across the different datasets)&lt;/p&gt;
&lt;h4 id=&quot;category-or-label-bias&quot;&gt;category or label bias&lt;/h4&gt;
&lt;p&gt;This comes from the fact that semantic categories are often poorly defined, and different labellers may assign differing labels to the same type of object2.&lt;/p&gt;
&lt;h4 id=&quot;negative-set-bias&quot;&gt;&lt;font color=&quot;#0000dd&quot;&gt;negative set bias&lt;/font&gt;&lt;/h4&gt;
&lt;p&gt;Datasets define a visual phenomenon (e.g. object, scene, event) not just by what it is (positive instances), but also by what it is not (negative instances). The negative set defines what the dataset considers to be “the rest of the world”. If that set is not representative, or unbalanced, that could produce classifiers that are overconfident and not very discriminative.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;font size=&quot;3&quot;&gt;This is particularly important for classification tasks, where the number of negatives is only a few orders of magnitude larger than the number of positives for each class. For example, if we want to find all images of “boats” in a PASCAL VOC-like classification task setting, how can we make sure that the classifier focuses on the boat itself, and not on the water below, or shore in the distance (after all, all boats are depicted in water)? This is where a large negative set (including rivers, lakes, sea, etc, without boats) is imperative to “push” the lazy classifier into doing the right thing.&lt;/font&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;dataset-value&quot;&gt;Dataset value&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15783472481154.jpg&quot; alt=&quot;-w842&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ways-to-avoid-biases-during-dataset-construction&quot;&gt;Ways to avoid biases during dataset construction&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=5995347&amp;amp;tag=1&quot;&gt;paper link&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 05 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/05/Unbiased-Look-at-Dataset-Bias/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/05/Unbiased-Look-at-Dataset-Bias/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Domain Adaptation Paper List</title>
        <description>&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=5995347&quot;&gt;Unbiased look at dataset bias&lt;/a&gt; (CVPR 2011) &lt;a href=&quot;https://xiaohan-wang.github.io/2020/01/05/Unbiased-Look-at-Dataset-Bias/&quot;&gt;&lt;em&gt;link&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;da-approaches&quot;&gt;DA Approaches&lt;/h3&gt;
&lt;h4 id=&quot;imagesfeature-transformation&quot;&gt;images/feature transformation&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1603.07442.pdf&quot;&gt;Pixel-Level Domain Transfer&lt;/a&gt; (ECCV 2016) &lt;a href=&quot;https://xiaohan-wang.github.io/2020/01/07/Pixel-Level-Domain-Transfer/&quot;&gt;&lt;em&gt;link&lt;/em&gt;&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;need corresponding pairs between two domains&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1612.05424.pdf&quot;&gt;Unsupervised Pixel–Level Domain Adaptation
with Generative Adversarial Networks&lt;/a&gt; (CVPR 2017) &lt;a href=&quot;https://xiaohan-wang.github.io/2020/01/08/Unsupervised-Pixel-Level-Domain-Adaptation-with-GAN/&quot;&gt;&lt;em&gt;link&lt;/em&gt;&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;assume that the difference between the domains are primarily low-level (due to noise, resolution, illumination, color) rather than high-level (types of objects, geometric variations, etc)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;invariant-representation&quot;&gt;invariant representation&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1412.3474.pdf&quot;&gt;Deep Domain Confusion: Maximizing for Domain Invariance&lt;/a&gt; (arXiv 2014)
    &lt;ul&gt;
      &lt;li&gt;classification loss + MMD loss&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1607.01719.pdf&quot;&gt;Deep CORAL: Correlation Alignment for Deep Domain Adaptation&lt;/a&gt;（Computer Vision – ECCV 2016 Workshops）
    &lt;ul&gt;
      &lt;li&gt;classification loss + coral loss&lt;/li&gt;
      &lt;li&gt;minimize the difference in second-order statistics between the source and target feature activations (CORAL loss)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 04 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/04/Domain-Adapation-paper-list/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/04/Domain-Adapation-paper-list/</guid>
        
        <category>Domain Adaptation</category>
        
        <category>Paper List</category>
        
        
      </item>
    
  </channel>
</rss>
