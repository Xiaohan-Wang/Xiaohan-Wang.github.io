<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 29 Mar 2020 03:17:13 -0400</pubDate>
    <lastBuildDate>Sun, 29 Mar 2020 03:17:13 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Reshaping Visual Datasets for Domain Adaptation</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5210-reshaping-visual-datasets-for-domain-adaptation.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15852773040000.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;图像的domain由多种因素交织影响决定，比如说pose、illumination、camera resolution、background等等。因此图像往往不是按照“clearly identifiable domain”收集的，也就是说，一个数据集可以包含多个不同的domain。发现这些source和target dataset中的latent domain有助于进行domain adaptation.&lt;/p&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Note that the total domain distinctiveness TDD(K) increases as K increases — presumably, in the extreme case, each domain has only a few instances and their distributions would be maximally different from each other. However, such tiny domains would offer insufficient data to separate the categories of interest reliably.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;maximum distinctiveness: 希望不同domain之间的差异尽量大 (domain划分尽量细)&lt;/p&gt;

    &lt;p&gt;采用non-parameter方式:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;the empirical distribution of each domain:
 &lt;img src=&quot;/img/15852779640661.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;the difference between the means of two empirical distributions
 &lt;img src=&quot;/img/15852781025087.jpg&quot; alt=&quot;-w837&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;maximize the total domain distinctiveness
 &lt;img src=&quot;/img/15852781836513.jpg&quot; width=&quot;30%&quot; height=&quot;30%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;maximum learnability: 每个domain包含足够数量的instances用于学习 (domain划分不能过细)&lt;/p&gt;

    &lt;p&gt;从K=2开始遍历寻找最好的K&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;对K个domain分别构建classfier，并计算cross-validation accuracy，并求weighted average
 &lt;img src=&quot;/img/15852795347553.jpg&quot; alt=&quot;-w699&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;选择具有最高的cross-validation accuracy的K&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;domain adaptaion的几种方式&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;合并全部source domain&lt;/li&gt;
  &lt;li&gt;emsemble各个source domain的classfier的结果&lt;/li&gt;
  &lt;li&gt;选择和target domain最接近的一个source domain&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;reshape target domain conditioning on the identified domains from the training datasets — the goal is to discover latent domains in the test datasets that match the domains in the training datasets as much as possible.
&lt;img src=&quot;/img/15852803974334.jpg&quot; alt=&quot;-w752&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/15852803845757.jpg&quot; width=&quot;93%&quot; height=&quot;93%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;J. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discovering latent domains for multisource domain adaptation. In ECCV. 2012. (aim at discovering the latent domains from datasets, by modeling the data with a hierarchical distribution consisting of Gaussian mixtures)&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 26 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/26/Reshaping-Visual-Datasets-for-Domain-Adaptation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/26/Reshaping-Visual-Datasets-for-Domain-Adaptation/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>SPP-Net (spatial pyramid pooling)</title>
        <description>&lt;h4 id=&quot;goal&quot;&gt;Goal&lt;/h4&gt;
&lt;p&gt;消除CNN对图片size/scale的限制&lt;/p&gt;

&lt;h4 id=&quot;motivation&quot;&gt;Motivation&lt;/h4&gt;
&lt;p&gt;CNN = convolutional layers + fully-connected layers&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;convolutional layers不要求图片size&lt;/li&gt;
  &lt;li&gt;fully-connected layers对size有要求&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;思路：在fully-connected layers前面加入SPP layer, 其能对不同size的feature产生fixed-length output，然后送入fully-connected layers&lt;/p&gt;

&lt;p&gt;在image detection上的应用
&lt;img src=&quot;/img/15849366399167.jpg&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15849365194047.jpg&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In each spatial bin, pool (max pooling in the paper) the response of each filter. &lt;br /&gt;
The output of SPP: k*M (M: number  of bins, e.g. 16+4+1; k: number of channels, e.g. 256)&lt;/p&gt;

</description>
        <pubDate>Wed, 25 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/25/SPP-Net/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/25/SPP-Net/</guid>
        
        <category>CV paperlist</category>
        
        
      </item>
    
      <item>
        <title>Domain Adaptation for Semantic Segmentation with Maximum Squares Loss</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Domain_Adaptation_for_Semantic_Segmentation_With_Maximum_Squares_Loss_ICCV_2019_paper.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/ZJULearning/MaxSquareLoss&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;Probability imbalance problem -&amp;gt; maximum square loss&lt;/p&gt;

&lt;h4 id=&quot;probability-imbalance-problem&quot;&gt;Probability imbalance problem&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15851597726007.jpg&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;
Note that uncertain area has prediction probability around 0.5, while prediction probability around 1 corresponds to areas with good accuracy.&lt;/p&gt;

&lt;p&gt;Shannon entropy:
&lt;img style=&quot;text-align: center&quot; src=&quot;/img/15851603980105.jpg&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;
Maximum squares loss:
&lt;img style=&quot;text-align: center&quot; src=&quot;/img/15851604671115.jpg&quot; width=&quot;45%&quot; height=&quot;45%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Classes with high accuracy always have higher prediction probabilities. If Shannon entropy is deployed, as shown in the above figure, the simple class will produce a much larger gradient on each pixel than the difficult class, causing each gradient descent step is basically computed from those simple classes, and difficult classes is still not trained sufficiently. By using MSL, areas with higher confidence still have larger gradients, but their dominant effects have been reduced, allowing other difficult classes to obtain training gradients.&lt;/p&gt;

&lt;h4 id=&quot;class-imbalance-problem&quot;&gt;Class imbalance problem&lt;/h4&gt;
&lt;p&gt;Classes with higher accuracy always have more pixels on the label map, which leads to an imbalance in quantity.&lt;/p&gt;

&lt;p&gt;regular method: introduce weighting factor αc, which is usually set as the inverse class frequency.&lt;/p&gt;

&lt;p&gt;paper: Image-wise Class-balanced Weighting Factor&lt;/p&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;fix class imbalance
    &lt;ul&gt;
      &lt;li&gt;regular way: introduce weighting factor αc, which is usually set as the inverse class frequency.&lt;/li&gt;
      &lt;li&gt;image-wise weighting ratio (this paper)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;multi-level self-produced guidance&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 24 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/24/Maximum-Squares-Loss/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/24/Maximum-Squares-Loss/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>ADVENT:Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.12833.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/valeoai/ADVENT&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;观察到source domain prediction是over-confident / low entropy，而target domain prediction是under-confident / high entropy&lt;/li&gt;
  &lt;li&gt;direct entropy minimization&lt;/li&gt;
  &lt;li&gt;structure adaptation: adversarial learning&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15850310997747.jpg&quot; alt=&quot;-w930&quot; /&gt;&lt;/p&gt;

&lt;p&gt;class ratio prior:&lt;br /&gt;
Entropy minimization can get biased towards some easy classes. Therefore, sometimes it is beneficial to guide the learning with some prior. 此处根据class ratio prior设计了另一个Loss来guide learning.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;entropy map and $L_{ent}$&lt;br /&gt;
&lt;img src=&quot;/img/15850758404417.jpg&quot; width=&quot;40%&quot; height=&quot;40%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/15850759324138.jpg&quot; width=&quot;25%&quot; height=&quot;40%&quot; /&gt;&lt;br /&gt;
a soft-assignment version of the pseudo-label cross-entropy loss&lt;/li&gt;
  &lt;li&gt;Adversarial learning loss&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;In general, AdvEnt works better than MinEnt. By combining results of the two models MinEnt and AdvEnt, we observe a decent boost in performance, compared to results of single models. 
&lt;img src=&quot;/img/15850315105452.jpg&quot; alt=&quot;-w923&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;oracle performance (trained on source and target)
&lt;img src=&quot;/img/15850315456104.jpg&quot; width=&quot;40%&quot; height=&quot;40%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Training on specific entropy ranges.&lt;/li&gt;
  &lt;li&gt;Using class-ratio prior.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;insight&quot;&gt;Insight&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;找source domain结果/中间feature map具备、而target domain不具备的性质，用adversarial learning (e.g. cross entropy)&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sat, 21 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/21/ADVENT/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/21/ADVENT/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Unsupervised Domain Adaptation using Feature-Whitening and Consensus Loss</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1903.03215.pdf&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;https://github.com/roysubhankar/dwt-domain-adaptation&quot;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;take-away-message&quot;&gt;Take Away Message&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Domain alignment layers which implement feature whitening for the purpose of matching source and target feature distributions and increases the smoothness of the loss landscape.&lt;/li&gt;
  &lt;li&gt;Min-Entropy Consensus loss regularizes training while avoiding the adoption of many user-defined hyper-parameters&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15843077382687.jpg&quot; alt=&quot;-w752&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DA的四种方法：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Correlation Alignment paradigm&lt;/li&gt;
  &lt;li&gt;domain-specific alignment layers&lt;/li&gt;
  &lt;li&gt;entropy minimization distribution&lt;/li&gt;
  &lt;li&gt;consistency-enforcing paradigm&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;由 12 -&amp;gt; DWTL layer&lt;br /&gt;
由 34 -&amp;gt; MEC loss&lt;/p&gt;

&lt;p&gt;source: cross-entropy loss
&lt;img src=&quot;/img/15843177084504.jpg&quot; alt=&quot;-w200&quot; /&gt;
target: MEC loss
&lt;img src=&quot;/img/15843177196984.jpg&quot; alt=&quot;-w300&quot; /&gt;
final loss:
&lt;img src=&quot;/img/15843177315198.jpg&quot; alt=&quot;-w300&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;要求在source上表现好&lt;/li&gt;
  &lt;li&gt;要求在target上对perturbation具有robustness，同时对结果high confidence&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;digit classification&lt;/li&gt;
  &lt;li&gt;object recognition&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;potential-improvement&quot;&gt;Potential Improvement&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;try full “coloring”&lt;/li&gt;
  &lt;li&gt;MEC loss中pseudo label的选择方式？&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 14 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/14/Unsupervised-Domain-Adaptation-using-Feature-Whitening-and-Consensus-Loss/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/14/Unsupervised-Domain-Adaptation-using-Feature-Whitening-and-Consensus-Loss/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Domain Adaptation Concept Summary</title>
        <description>&lt;h3 id=&quot;transfer-learning&quot;&gt;Transfer learning&lt;/h3&gt;
&lt;h4 id=&quot;domain-and-task&quot;&gt;domain and task&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;domain = feature space + marginal distribution&lt;/li&gt;
  &lt;li&gt;task = label space + conditional distribution (predictive function)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;transfer-learning定义&quot;&gt;transfer learning定义&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15842471547628.jpg&quot; alt=&quot;-w914&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;正常的machine learning: 只有一个domain，通过自己的X、Y学习predictive function&lt;/li&gt;
  &lt;li&gt;transfer learning：利用source的知识提高target的predictive function的表现
    &lt;ul&gt;
      &lt;li&gt;source和target是不同的
        &lt;ul&gt;
          &lt;li&gt;feature space不同&lt;/li&gt;
          &lt;li&gt;marginal distribution不同&lt;/li&gt;
          &lt;li&gt;label space不同&lt;/li&gt;
          &lt;li&gt;conditional distribution不同  （？？）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;domain adaptation&lt;/em&gt;: transfer learning的子类，source和target只有marginal distribution不同，其余三点都相同&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;domain-adaptation&quot;&gt;Domain adaptation&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15850826248103.jpg&quot; alt=&quot;&quot; /&gt;
from https://en.wikipedia.org/wiki/Domain_adaptation&lt;/p&gt;

</description>
        <pubDate>Sat, 14 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/14/Domain-Adaptation-Concept-Summary/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/14/Domain-Adaptation-Concept-Summary/</guid>
        
        <category>Domain Adaptation</category>
        
        <category>Concept</category>
        
        
      </item>
    
      <item>
        <title>A Few Useful Things to Know About Machine Learning</title>
        <description>&lt;h2 id=&quot;12-trick-lessons-in-ml&quot;&gt;12 trick lessons in ML&lt;/h2&gt;

&lt;h3 id=&quot;1-learning-algorithm--representation--evaluation--optimization&quot;&gt;1. learning algorithm = representation + evaluation + optimization&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;representation&lt;/strong&gt;:  choosing a representation for a learner is tantamount to choosing the hypothesis space&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;evaluation&lt;/strong&gt;: evaluation function&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;optimization&lt;/strong&gt;: the method to search among the hypothesis space&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15817081275431.jpg&quot; alt=&quot;-w801&quot; /&gt;
三者同样重要&lt;/p&gt;

&lt;h3 id=&quot;2-generalization-counts&quot;&gt;2. generalization counts&lt;/h3&gt;
&lt;p&gt;strict separation between training and test set.&lt;/p&gt;

&lt;h3 id=&quot;3-data-alone-is-not-enough&quot;&gt;3. data alone is not enough&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;data alone is not enough, no matter how much of it you have. (e.g. Consider learning a Boolean function of (say) 100 variables from a million examples. There are 2^100 − 10^6 examples whose classes you don’t know.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;every learner must embody some &lt;strong&gt;knowledge or assumption&lt;/strong&gt; beyond the data it’s given in order to generalize beyond it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;learners are like lever: less knowledge/assumption + (data) -&amp;gt; useful results. But it still need more than zero input knowledge to work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;corollary: one of the key criteria for choosing a representation is which kinds of knowledge are easily expressed in it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;4-overfitting&quot;&gt;4. overfitting&lt;/h3&gt;
&lt;p&gt;if the knowledge and data we have are not sufficient to completely determine the correct classifier -&amp;gt; risk of overfitting&lt;/p&gt;

&lt;p&gt;Note: A common misconception about overfitting is that it is caused by noise, but severe overfitting can occur even in the absence of noise.&lt;/p&gt;

&lt;h3 id=&quot;5-high-dimensions&quot;&gt;5. high dimensions&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;curse of dimensionality:  our intuition, which come from a three-dimensional world, often do not apply in high-dimensional ones. Naively, one might think that gathering more features never hurts, since at worst they provide no new information about the class. But in fact their benefits may be outweighed by the curse of dimensionality.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;blessing of non-uniformity: In most applications examples are not spread uniformly throughout the instance space, but are concentrated on or near a lower-dimensional manifold.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;6-theoretical-guarantees&quot;&gt;6. theoretical guarantees&lt;/h3&gt;
&lt;p&gt;The main role of theoretical guarantees in machine learning
is not as a criterion for practical decisions, but as a source of
understanding and driving force for algorithm design. In this
capacity, they are quite useful; indeed, the close interplay
of theory and practice is one of the main reasons machine
learning has made so much progress over the years. But
caveat emptor: learning is a complex phenomenon, and just
because a learner has a theoretical justification and works in
practice doesn’t mean the former is the reason for the latter.&lt;/p&gt;

&lt;h3 id=&quot;7-feature-engineering&quot;&gt;7. feature engineering&lt;/h3&gt;
&lt;p&gt;Often, the raw data is not in a form that is amenable to learning, but you can construct features from it that are.&lt;/p&gt;

&lt;p&gt;Note: features that look irrelevant in isolation may be relevant in combination. For example, if the class is an XOR of k input features, each of them by itself carries no information about the class.&lt;/p&gt;

&lt;h3 id=&quot;8-more-data-beats-a-cleverer-algorithm&quot;&gt;8. MORE DATA BEATS A CLEVERER ALGORITHM&lt;/h3&gt;
&lt;p&gt;As a rule of thumb, a dumb algorithm with lots and lots of data beats a clever one with modest amounts of it.&lt;/p&gt;

&lt;h3 id=&quot;9-learn-many-models-not-just-one&quot;&gt;9. LEARN MANY MODELS, NOT JUST ONE&lt;/h3&gt;
&lt;p&gt;model ensembles (researchers noticed that, if instead
of selecting the best variation found, we combine many variations, the results are better—often much better—and at
little extra effort for the user)&lt;/p&gt;

&lt;h3 id=&quot;10--simplicity-does-not-imply-accuracy&quot;&gt;10.  SIMPLICITY DOES NOT IMPLY ACCURACY&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;contrary to intuition, there is no necessary connection between the number of parameters of a model and its tendency to overfit.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the size of the hypothesis space is only a rough guide to what really matters for relating training and test error: the procedure by which a hypothesis is chosen.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;11-representable-does-not-imply-learnable&quot;&gt;11. REPRESENTABLE DOES NOT IMPLY LEARNABLE&lt;/h3&gt;
&lt;p&gt;Just because a function can be represented does not mean it can be learned (e.g.  if the hypothesis space has many local optima of the evaluation function, as is often the case, the learner may not find the true function even if it is representable.). Therefore the key question is not “Can it be represented?”, to which the answer is often trivial, but “Can it be learned?”&lt;/p&gt;

&lt;h3 id=&quot;12-correlation-does-not-imply-causation&quot;&gt;12. CORRELATION DOES NOT IMPLY CAUSATION&lt;/h3&gt;
</description>
        <pubDate>Tue, 11 Feb 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/02/11/A-Few-Useful-Things-to-Know-About-Machine-Learning/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/11/A-Few-Useful-Things-to-Know-About-Machine-Learning/</guid>
        
        <category>ML</category>
        
        
      </item>
    
      <item>
        <title>python各类图像库</title>
        <description>&lt;h3 id=&quot;short-version&quot;&gt;short version&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;类型&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;顺序&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;size&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;scale&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;cv2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;numpy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;BGR&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;(h,w,c)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;255(uint8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;PIL&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;Image/numpy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;RGB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;(w,h) / (h,w,c)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;- / 255(uint8)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;skimage&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;numpy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;RGB&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;(h,w,c)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;rgb:255(uint8) / gray:1(float)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;cv2&quot;&gt;cv2&lt;/h3&gt;
&lt;h4 id=&quot;ioshow&quot;&gt;i/o/show&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#读入图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
  
&lt;span class=&quot;c1&quot;&gt;#显示图像    
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'src'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitkey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;#保存图像   
&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imwrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'test.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  

&lt;span class=&quot;c1&quot;&gt;#归一化  
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;float&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;255.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;font size=&quot;3&quot;&gt;`imshow` should be followed by `waitKey` function which displays the image for specified milliseconds. **Otherwise, it won’t display the image**. For example, `waitKey(0)` will display the window infinitely until any keypress (it is suitable for image display). waitKey(25) will display a frame for 25 ms, after which display will be automatically closed. (If you put it in a loop to read videos, it will display the video frame-by-frame)&lt;/font&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;attribute&quot;&gt;attribute&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;numpy array&lt;/li&gt;
  &lt;li&gt;默认读入彩色图像
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gray = cv2.imread('1.jpg',cv2.IMREAD_GRAYSCALE)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;channel顺序：BGR
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;im2 = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;img.shape: (h,w,c)&lt;/li&gt;
  &lt;li&gt;img.dtype: uint8(255)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pil&quot;&gt;PIL&lt;/h3&gt;
&lt;h4 id=&quot;ioshow-1&quot;&gt;i/o/show&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;PIL&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#读入图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#显示图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#保存图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'2.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;image对象&quot;&gt;Image对象&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;PIL读入图片后生成Image对象&lt;/li&gt;
  &lt;li&gt;img.size: (w, h)&lt;/li&gt;
  &lt;li&gt;img.mode: 灰度L / 真彩色RGB&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;image转numpy矩阵&quot;&gt;Image转numpy矩阵&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;arr = np.array(img) &lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;channel顺序：RGB&lt;/li&gt;
  &lt;li&gt;arr.shape: (h,w,c)&lt;/li&gt;
  &lt;li&gt;arr.dtype: uint8 (255)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;new_img = Image.fromarray(arr)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;skimage&quot;&gt;skimage&lt;/h3&gt;
&lt;h4 id=&quot;ioshow-2&quot;&gt;i/o/show&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;skimage&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#读入图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#显示图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#保存图像
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imsave&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;attribute-1&quot;&gt;attribute&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;numpy array&lt;/li&gt;
  &lt;li&gt;channel顺序：RGB&lt;/li&gt;
  &lt;li&gt;img.shape: (h,w,c)&lt;/li&gt;
  &lt;li&gt;img.dtype: RGB uint8 (255) / gray float (1)&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 08 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/08/python%E5%90%84%E7%B1%BB%E5%9B%BE%E5%83%8F%E5%BA%93/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/08/python%E5%90%84%E7%B1%BB%E5%9B%BE%E5%83%8F%E5%BA%93/</guid>
        
        <category>python</category>
        
        
      </item>
    
      <item>
        <title>Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks</title>
        <description>&lt;h3 id=&quot;idea&quot;&gt;Idea&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;适用于domain间存在low-level difference的domain adaptation&lt;/li&gt;
  &lt;li&gt;不需要domain间的corresponding pairs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15785106924617.jpg&quot; alt=&quot;-w857&quot; /&gt;
&lt;img src=&quot;/img/15785107411179.jpg&quot; alt=&quot;-w50&quot; height=&quot;100px&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;主要由discriminator(D)来学习，classifier(T: task)和content-similarity loss主要用来 stabalize GAN的训练&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context-similarity loss: PMSE (pairwise mean squared error)
    &lt;blockquote&gt;
      &lt;p&gt;This loss allows the model to learn to reproduce the overall shape of the objects being modeled without wasting modeling power on the absolute color or intensity of the inputs, while allowing our adversarial training to change the object in a consistent way.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reflection&quot;&gt;Reflection&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;T的参与感觉让domain adaptation变成了迎合任务的domain adaptation, 而不是单纯的pixel-level domain adaptation. (也有优势，在对应任务上应该有更好的表现)&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 08 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/08/Unsupervised-Pixel-Level-Domain-Adaptation-with-GAN/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/08/Unsupervised-Pixel-Level-Domain-Adaptation-with-GAN/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Pixel-Level Domain Transfer</title>
        <description>&lt;h3 id=&quot;idea&quot;&gt;Idea&lt;/h3&gt;
&lt;p&gt;transfer images in one domain into another domain&lt;/p&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15784545847019.jpg&quot; alt=&quot;-w744&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;real/fake discriminator: 判断图像是否来自target domain
&lt;img src=&quot;/img/15784548684906.jpg&quot; alt=&quot;-w727&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;domain (semantic) discriminator: 判断是否和source domain图像具有相同semantic information
&lt;img src=&quot;/img/15784548946537.jpg&quot; alt=&quot;-w729&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;converter
 &lt;img src=&quot;/img/15784556404369.jpg&quot; alt=&quot;-w709&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15784549145475.jpg&quot; alt=&quot;-w771&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reflection&quot;&gt;Reflection&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;作者构建的dataset将source domain和target domain中的图像make pair，从而学习image transfer。但general的DA任务并不会有source和target的image pair，因此很难直接使用&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MSE和discriminator：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;MSE容易产生blurry images, 因为其假设pixels服从Gaussian distribution&lt;/li&gt;
      &lt;li&gt;MSE never allow a small geometric miss-alignment&lt;/li&gt;
      &lt;li&gt;当有关联，但又不容易用公式描述时，可以尝试discriminator&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 07 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/01/07/Pixel-Level-Domain-Transfer/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/01/07/Pixel-Level-Domain-Transfer/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
  </channel>
</rss>
