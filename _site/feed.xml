<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 17 Sep 2020 17:36:02 -0400</pubDate>
    <lastBuildDate>Thu, 17 Sep 2020 17:36:02 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>安装tmux</title>
        <description>&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/william_munch/article/details/95764667&quot;&gt;非root用户源码安装Tmux&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://squidszyd.github.io/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/zsh/2017/08/28/zsh.html&quot;&gt;安装ncurses&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://louiszhai.github.io/2017/09/30/tmux/&quot;&gt;tmux配置及使用&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Xiaohan-Wang/Backup/blob/master/.tmux.conf&quot;&gt;my config&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 03 Sep 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/09/03/%E5%AE%89%E8%A3%85tmux/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/09/03/%E5%AE%89%E8%A3%85tmux/</guid>
        
        <category>Configurations</category>
        
        
      </item>
    
      <item>
        <title>安装anaconda</title>
        <description>&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jianshu.com/p/edaa744ea47d&quot;&gt;anaconda安装与使用&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 03 Sep 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/09/03/%E5%AE%89%E8%A3%85anaconda/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/09/03/%E5%AE%89%E8%A3%85anaconda/</guid>
        
        <category>Configurations</category>
        
        
      </item>
    
      <item>
        <title>安装Oh My Zsh</title>
        <description>&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://squidszyd.github.io/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/zsh/2017/08/28/zsh.html&quot;&gt;解决ncurses问题&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;解压 ncurses tar.gz 文件
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;ncurses &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tar&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-zxf&lt;/span&gt; ncurses-6.2.tar.gz &lt;span class=&quot;nt&quot;&gt;-C&lt;/span&gt; ncurses &lt;span class=&quot;nt&quot;&gt;--strip-components&lt;/span&gt; 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://harttle.land/2016/10/25/install-oh-my-zsh-locally.html&quot;&gt;没有root权限zsh&amp;amp;oh-my-zsh安装&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jyzhangchn.github.io/oh-my-zsh.html&quot;&gt;配置oh-my-zsh主题&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Xiaohan-Wang/Backup/blob/master/xh-ys.zsh-theme&quot;&gt;my theme&lt;/a&gt; - modified from theme ys&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 03 Sep 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/09/03/%E5%AE%89%E8%A3%85oh-my-zsh/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/09/03/%E5%AE%89%E8%A3%85oh-my-zsh/</guid>
        
        <category>Configurations</category>
        
        
      </item>
    
      <item>
        <title>在SLURM节点上启动Jupyter Notebook</title>
        <description>&lt;p&gt;原载于 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/65130699&quot;&gt;分享脚本远程登陆 Jupyter Notebook&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;服务器端&quot;&gt;服务器端&lt;/h4&gt;
&lt;h5 id=&quot;jupytergpush脚本&quot;&gt;jupyterGPU.sh脚本&lt;/h5&gt;
&lt;div class=&quot;language-zsh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --partition compsci-gpu&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --gres=gpu:2&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --time 100:00:00&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --job-name jupyterGPU&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# get tunneling info&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;shuf&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i8000-9999&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;whoami&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;.&quot;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'{print $2}'&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 在这里添加你的服务器地址&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;clusterurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;xxx&quot;&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;:~/.local/bin

&lt;span class=&quot;c&quot;&gt;# print tunneling instructions jupyter-log&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;
MacOS or linux terminal command to create your ssh tunnel:
ssh -N -L &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;clusterurl&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;

 Here is the MobaXterm info:

 Forwarded port:same as remote port
 Remote server: &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
 Remote port: &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
 SSH server: &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;clusterurl&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
 SSH login: &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$user&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
 SSH port: 22

 Use a Browser on your local machine to go to:
 localhost:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; (prefix w/ https:// if using password)

 or copy the URL from below and put there localhost after http:// so it would be something like:
 http://localhost:9499/?token=86c93ba16aaead7529a5da0e5e5a46be7ad8cfea35b2d49f
 &quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# load modules or conda environments here&lt;/span&gt;
jupyter notebook &lt;span class=&quot;nt&quot;&gt;--no-browser&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--ip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h5 id=&quot;提交jupytergpush脚本&quot;&gt;提交jupyterGPU.sh脚本&lt;/h5&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;sbatch jupyterGPU.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h5 id=&quot;查看端口信息浏览器信息&quot;&gt;查看端口信息/浏览器信息&lt;/h5&gt;
&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;slurm.output
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;本地&quot;&gt;本地&lt;/h4&gt;
&lt;h5 id=&quot;监听对应端口&quot;&gt;监听对应端口&lt;/h5&gt;
&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;:&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;:&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;@&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;clusterurl&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h5 id=&quot;浏览器打开jupyter-notebook&quot;&gt;浏览器打开jupyter notebook&lt;/h5&gt;
&lt;p&gt;前往&lt;code class=&quot;highlighter-rouge&quot;&gt;http://localhost:${port}&lt;/code&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 16 Jul 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/07/16/%E5%9C%A8slurm%E4%B8%8A%E4%BD%BF%E7%94%A8jupyter-notebook/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/07/16/%E5%9C%A8slurm%E4%B8%8A%E4%BD%BF%E7%94%A8jupyter-notebook/</guid>
        
        <category>Configurations</category>
        
        
      </item>
    
      <item>
        <title>Context Encoders:_Feature Learning by Inpainting</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1604.07379.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;http://people.eecs.berkeley.edu/~pathak/context_encoder/&quot;&gt;website&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Image inpainting:
    &lt;ul&gt;
      &lt;li&gt;to fill in large missing areas of the image, it can’t get “hints” from nearby pixels&lt;/li&gt;
      &lt;li&gt;a model needs to both understand the content of an image, as well as produce a plausible hypothesis for the missing parts&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;a standard pixel-wise reconstruction loss&lt;/strong&gt;: only the reconstruction loss produces blurry results&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;an adversarial loss&lt;/strong&gt;: adding the adversarial loss results in much sharper predictions&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Encoder-decoder pipeline:
    &lt;ul&gt;
      &lt;li&gt;encoder: produces a latent feature representation of that image&lt;/li&gt;
      &lt;li&gt;decoder: takes this feature representation and produces the missing image content
        &lt;ul&gt;
          &lt;li&gt;a series of five up-convolutional layers (upsampling followed by convolution)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;channel-wise fully-connected layer&lt;/strong&gt;: connect the encoder and the decoder
        &lt;ul&gt;
          &lt;li&gt;directly propagate information from one corner of the feature map to another corner (so that we can better capture global semantic information)&lt;/li&gt;
          &lt;li&gt;essentially a fully-connected layer with groups: only propagates information within feature maps, but it has no parameters connecting different feature maps&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Loss function:
    &lt;ul&gt;
      &lt;li&gt;reconstruction (L2) loss: 
  &lt;img src=&quot;/img/15942152334674.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;
        &lt;ul&gt;
          &lt;li&gt;capturing the overall structure of the missing region and coherence with regards to its context, but prefer a blurry solution (because the expectation of all possible values minimizes the mean squared pixel-wise error)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;adversarial loss: 
  &lt;img src=&quot;/img/15942164552075.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;
        &lt;ul&gt;
          &lt;li&gt;tries to make prediction look real, and has the effect of picking a particular mode from the distribution&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Region masks
 &lt;img src=&quot;/img/15942173051541.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;central region&lt;/li&gt;
      &lt;li&gt;random block&lt;/li&gt;
      &lt;li&gt;random region (recommended)&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Random block and random region produce a similarly general feature, while significantly outperforming the central region features.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;experiments&quot;&gt;Experiments&lt;/h4&gt;
&lt;h5 id=&quot;implementation-detail&quot;&gt;Implementation detail&lt;/h5&gt;
&lt;p&gt;Pool-free encoders: replacing all pooling layers with convolutions of the same kernel size and stride. (Intuitively, there is no reason to use pooling for reconstruction based networks.)&lt;/p&gt;

&lt;h5 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;Semantic Inpainting
&lt;img src=&quot;/img/15942140401143.jpg&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;The encoder and discriminator architecture is similar to that of discriminator in [1], and decoder is similar to generator in [1].&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Feature Learning
 &lt;img src=&quot;/img/15942140603053.jpg&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;For consistency with prior work, this paper use AlexNet as its encoder in this part.&lt;/li&gt;
      &lt;li&gt;The authors did not manage to make the adversarial loss converge with AlexNet, so they used just the reconstruction loss.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/img/15942180841172.jpg&quot; alt=&quot;-w892&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. ICLR, 2016.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 07 Jul 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/07/07/Context-Encoders-Feature-Learning-by-Inpainting/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/07/07/Context-Encoders-Feature-Learning-by-Inpainting/</guid>
        
        <category>Self-supervision</category>
        
        
      </item>
    
      <item>
        <title>Self-supervised feature learning 综述</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1902.06162.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;self-supervised-feature-learning&quot;&gt;self-supervised feature learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;learn visual features from large-scale unlabeled images or videos without using any human annotations&lt;/li&gt;
  &lt;li&gt;a subset of unsupervised learning methods&lt;/li&gt;
  &lt;li&gt;pretext task: the supervisory signal is generated from the data itself by leveraging its structure&lt;/li&gt;
  &lt;li&gt;visual features of images or videos need to be captured by ConvNets to solve the pretext tasks&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;作用&quot;&gt;作用&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;作为pretext task获得pre-trained model
    &lt;ul&gt;
      &lt;li&gt;提供一个好的起始点，加速收敛&lt;/li&gt;
      &lt;li&gt;已经学习到hierarchy features，即使downstream task的数据集很小，也不会过拟合太严重&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;作为auxiliary task来添加regularization&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;分类&quot;&gt;分类&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15932024508723.jpg&quot; alt=&quot;-w1112&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Information recovery: 先抹除图片的一部分信息，然后让网络学习恢复这些信息
    &lt;ul&gt;
      &lt;li&gt;generation based:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;image generation&lt;/strong&gt;: help the network to capture the real distribution of the real data and generate realists data&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;color recovery&lt;/strong&gt;: network need to recognize objects and to group pixels of the same part together&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;inpainting&lt;/strong&gt;: networks are required to learn the common knowledge including the color and structure of the common objects&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;super resolution&lt;/strong&gt;: learn the semantic features of images&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;context based:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Context Similarity (contrasting)&lt;/strong&gt;: learn the invariance within one class and the variance among different classes
            &lt;ul&gt;
              &lt;li&gt;contrasting: train networks to maximum agreement of different views of same scene while minimizing agreement of views from different scenes&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Spatial Context Structure&lt;/strong&gt;：learn spatial context information such as the shape of the objects and the relative positions of different parts of an object&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hard-code program
    &lt;ul&gt;
      &lt;li&gt;This type of methods generally has two steps:
        &lt;ul&gt;
          &lt;li&gt;label generation by employing hard-code programs on images or videos to obtain labels,&lt;/li&gt;
          &lt;li&gt;train ConvNets with the generated labels.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;distill knowledge from hard-code detector&lt;/li&gt;
      &lt;li&gt;one drawback is that the semantic labels generated by hard-code detector usually are very noisy which need to specifically cope with.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Game Engines
    &lt;ul&gt;
      &lt;li&gt;game engines are able to render realistic images and provide accurate pixel-level labels&lt;/li&gt;
      &lt;li&gt;one problem is the domain gap between synthetic and real-world images&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;performance&quot;&gt;Performance&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15932060535480.jpg&quot; alt=&quot;-w913&quot; /&gt;
The performance of self-supervised methods are comparable to the supervised methods on some downstream tasks, especially for the object detection and semantic segmentation tasks.&lt;/p&gt;

</description>
        <pubDate>Fri, 26 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/26/self-supervised-feature-learning/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/26/self-supervised-feature-learning/</guid>
        
        <category>Self-supervision</category>
        
        
      </item>
    
      <item>
        <title>Distilling the Knowledge in a Neural Network</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;transfer the knowledge from the cumbersome model (used in training stage) to a small model (suitable for deployment)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;“knowledge”: a learned mapping from input vectors to output vectors&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;use the class probabilities produced by the cumbersome model as “soft targets” for training the small model.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;small probabilities in the soft targets define a rich similarity structure over the data, but it has very little influence on the cross-entropy cost function during the transfer stage because the probabilities are so close to zero&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;use “distillation” to raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;adding a small term to the objective function that encourages the small model to &lt;strong&gt;predict the true targets as well as matching the soft targets provided by the cumbersome model&lt;/strong&gt; works pretty well.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;h5 id=&quot;整体思路&quot;&gt;整体思路：&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;想要transfer的knowledge是从input到output的映射。也就是说，对于相同的input，希望cumbersome model和small model能生成相同的output (soft label)&lt;/li&gt;
  &lt;li&gt;用cumbersome model和small model产生的soft label做cross entropy loss
    &lt;ul&gt;
      &lt;li&gt;问题：除了hard target对应的概率，其它负标签的概率其实也提供了大量的信息。比如说一张2的图片，对应3的soft label是1e-6，而7的soft label是1e-9，说明这张图片除2之外，更像3而不像7。但是，如果使用传统的softmax层 (T=1)，我们很难有效利用这些信息，因为这些负标签的soft label太小了，基本不能影响cross entropy loss的结果&lt;/li&gt;
      &lt;li&gt;为了能利用负标签对应概率所提供的信息，使用distillation，通过增大T来产生softer probability distribution，此时正负标签对应的soft label差距将被缩小，因此负标签的soft label也能影响最终的cross entropy loss&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;知识蒸馏&quot;&gt;知识蒸馏&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;带温度T的softmax表达式：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;q_{i}=\frac{\exp \left(z_{i}/T\right)}{\sum_{j} \exp \left(z_{j}/T \right)}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;温度T的影响&lt;br /&gt;
  &lt;img src=&quot;/img/15928689216135.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;原始的softmax函数是 $T=1$ 时的特例&lt;/li&gt;
      &lt;li&gt;温度越高，softmax后各个值z的分布就越平均，原本较小的soft label此时相对增大，对最终cross entropy loss的影响增大，也就提高了对这些原本soft label较小的负标签的关注程度&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如何选择温度T
    &lt;ul&gt;
      &lt;li&gt;不是所有的负标签对应的soft label都是有效的：这些soft label本身是由Teacher-Net (cumbersome model) 训练得到的，因此结果一定存在noise，而且soft label越小，noise的影响越大&lt;/li&gt;
      &lt;li&gt;温度T决定了对负标签的关注程度
        &lt;ul&gt;
          &lt;li&gt;从有部分信息量的负标签中学习 –&amp;gt; 温度要高一些&lt;/li&gt;
          &lt;li&gt;防止受负标签中噪声的影响 –&amp;gt;温度要低一些&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;知识蒸馏方法&quot;&gt;知识蒸馏方法&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15928705010652.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;训练Teacher model&lt;/li&gt;
  &lt;li&gt;对Teacher model蒸馏，得到student distilled model
    &lt;ul&gt;
      &lt;li&gt;soft prediction loss：Teacher model和 Student model在相同温度T下产生的soft label的cross entropy loss&lt;/li&gt;
      &lt;li&gt;hard prediction loss：Student model在T=1时和groundtruth的cross entropy loss&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;note&quot;&gt;Note&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;the magnitudes of the gradients produced by the soft targets scale as $\frac{1}{T^2}$, so it is important to multiply them by $T^2$ when using both hard and soft targets&lt;/li&gt;
  &lt;li&gt;matching logits is a special case of distillation&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/102038521&quot;&gt;知识蒸馏&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/61944055&quot;&gt;交叉熵与KL散度&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 21 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/21/Distilling-the-Knowledge-in-a-Neural-Network/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/21/Distilling-the-Knowledge-in-a-Neural-Network/</guid>
        
        <category>Knowledge Distillation</category>
        
        
      </item>
    
      <item>
        <title>迁移学习损失函数</title>
        <description>&lt;h4 id=&quot;基于统计量&quot;&gt;基于统计量&lt;/h4&gt;
&lt;h5 id=&quot;mmd-maximum-mean-discrepancy&quot;&gt;MMD (maximum mean discrepancy)&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;通过比较两个distribution的sample，直接获得两个distribution的差异&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://xiaohan-wang.github.io/2020/06/17/MMD/&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;基于对抗学习&quot;&gt;基于对抗学习&lt;/h4&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
</description>
        <pubDate>Wed, 17 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/17/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/17/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</guid>
        
        <category>Loss</category>
        
        
      </item>
    
      <item>
        <title>估计量的无偏性、有效性、一致性</title>
        <description>&lt;h4 id=&quot;估计量&quot;&gt;估计量&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;根据样本构造一个统计量，作为总体未知参数的估计，则该统计量为估计量&lt;/li&gt;
  &lt;li&gt;估计量可视为一个随机变量
    &lt;ul&gt;
      &lt;li&gt;同一个总体可以进行多次抽样，不同的抽样结果可以计算出不同的估计量取值&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;无偏性&quot;&gt;无偏性&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;无偏估计指，估计量的数学期望等于被估计参数的真实值&lt;/li&gt;
  &lt;li&gt;例子&lt;br /&gt;
  &lt;img src=&quot;/img/15924068989806.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
  假设圆心是被估计参数的真实值，粉色x代表每次抽样计算出的估计量。左图估计量的期望等于被估计参数的真实值，是无偏的。右图估计量的期望不等于被估计参数的真实值，是有偏的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;有效性&quot;&gt;有效性&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;指估计量的离散程度，离散程度越小越有效&lt;/li&gt;
  &lt;li&gt;注意，有效性和无偏性是不相关的&lt;br /&gt;
  &lt;img src=&quot;/img/15924081407687.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;无偏估计不一定是最好的&lt;br /&gt;
  &lt;img src=&quot;/img/15924083443839.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;一致性&quot;&gt;一致性&lt;/h4&gt;
&lt;p&gt;待填…&lt;/p&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/22983179&quot;&gt;什么是无偏估计？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/qq_40597317/article/details/80639511&quot;&gt;估计量的无偏性、有效性、一致性&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 17 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/17/%E6%9C%89%E5%81%8F%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/17/%E6%9C%89%E5%81%8F%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/</guid>
        
        <category>Statistics</category>
        
        
      </item>
    
      <item>
        <title>MMD</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.aaai.org/Papers/AAAI/2007/AAAI07-262.pdf&quot;&gt;AAAI 2007 paper&lt;/a&gt; and &lt;a href=&quot;http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf&quot;&gt;JMLR 2012 paper&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;传统的用于衡量两个概率分布P和Q差别的方法，例如KL divergence，要求概率分布P和Q已知。也就是说，&lt;strong&gt;如果我们只有来自P和Q的样本，那么我们需要先进行概率密度估计 (density estimation)，然后才能衡量两个分布的差异&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;MMD：利用来自P和Q的样本，直接获得它们对应的总体概率分布的差异，而无需density estimation这一中间步骤。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;mmd-metric&quot;&gt;MMD metric&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Give observations $X := {x_1, \cdots , x_m}$ and $Y := {y_1, \cdots, y_n}$, which are independently and identically distributed (i.i.d.) from distribution $p$ and $q$. Let $\mathcal{F}$ be a class of functions $f : X \to \mathbb{R}$, and shorthand notation $E_x[ f(x)] :=E_{x \sim p}[ f(x)]$ and $E_y[ f(y)] := E_{y\sim q}[ f(y)]$ denote expectations with respect to $p$ and $q$, respectively, where $x \sim p$ indicates x has distribution $p$.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maximum mean discrepancy (MMD) is defined as:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;MMD[\mathcal{F}, p,q] := \sup_{f \in \mathcal{F}}(E_x[ f(x)]−E_y[ f(y)]) .&lt;/script&gt;

    &lt;p&gt;We must therefore dentify a function class that is &lt;strong&gt;rich enough to uniquely identify whether $p = q$&lt;/strong&gt;. And since we want to obtain this discrepancy by sample $X$ and $Y$,  the function class should also be &lt;strong&gt;restrictive enough to provide useful finite sample estimates&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Propose the unit ball in a reproducing kernel Hilbert space $\mathcal H$ as our MMD function class $\mathcal{F}$. Define &lt;strong&gt;mean embedding&lt;/strong&gt; $\mu_p(t) \in \mathcal{H}$ such that &lt;script type=&quot;math/tex&quot;&gt;E_x [f(x)] = \lt f, \mu_p \gt _{\mathcal{H}}&lt;/script&gt; for all $f \in \mathcal{H}$. If we set $f= \phi (t) = k(t, \cdot)$, we obtain $\mu_p(t) = &amp;lt;\mu_p, k(t, ·)&amp;gt;_\mathcal{H} =E_xk(t, x)$, in other words, the mean embedding of the distribution $p$ is the expectation under $p$ of the canonical feature map.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;We thus have
    &lt;center&gt; 
 $$
 \begin{split}
 MMD^2 \left[ \mathcal{F}, p,q \right] &amp;amp;= \left[ \sup_{||f||_\mathcal{H} \leq 1}(E_x [ f(x)]−E_y [ f(y)])
\right]^2\\&amp;amp;=    \left[ \sup_{||f||_\mathcal{H} \leq 1}&amp;lt;\mu_p-\mu_q,f&amp;gt;_\mathcal{H}
\right]^2\\&amp;amp;=||\mu_p-\mu_q||_\mathcal{H}^2\\&amp;amp;=||E_x\phi(x)-E_y\phi(y)||_\mathcal{H}^2
 \end{split}
 $$
 &lt;/center&gt;
  &lt;/li&gt;
  &lt;li&gt;The MMD is a metric, when $\mathcal{H}$ is a universal RKHSs, defined on a compact metric space X. It can be proven that &lt;strong&gt;the Gaussian and Laplace RKHSs&lt;/strong&gt; are universal.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;finite-sample-estimate&quot;&gt;Finite sample estimate&lt;/h4&gt;
&lt;p&gt;Given $x$ and $x^{\prime}$ independent random variables with distribution $p$, and $y$ and $y^\prime$ independent random variables with distribution $q$, the squared population MMD is&lt;/p&gt;
&lt;center&gt;
$$
MMD^2 [\mathcal{F}, p,q] = E_{x,x^\prime}
\left[k(x, x^\prime)\right] +E_{y,y^\prime}\left[ k(y, y′)\right]−2E_{x,y} \left[k(x, y)\right]
$$
&lt;/center&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;An unbiased empirical estimate
 &lt;img src=&quot;/img/15924519721534.jpg&quot; width=&quot;75%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An biased empirical estimate
 &lt;img src=&quot;/img/15924520728791.jpg&quot; width=&quot;75%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A linear time statistics&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf&quot;&gt;link&lt;/a&gt; $P_{739}$ Lemma 14&lt;/li&gt;
      &lt;li&gt;need sufficient data (many more samples than the quadratic-cost tests)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Estimate 1 and 2 cost $O((m+n)^2)$ time to compute both statistics&lt;/li&gt;
  &lt;li&gt;Estimate 3 can be computed in linear time&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cnblogs.com/kailugaji/p/11004246.html&quot;&gt;MATLAB最大均值差异&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 17 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/17/MMD/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/17/MMD/</guid>
        
        <category>Statistics</category>
        
        
      </item>
    
  </channel>
</rss>
