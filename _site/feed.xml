<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 15 Sep 2020 23:12:07 -0400</pubDate>
    <lastBuildDate>Tue, 15 Sep 2020 23:12:07 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>安装anaconda</title>
        <description>&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jianshu.com/p/edaa744ea47d&quot;&gt;anaconda安装与使用&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 03 Sep 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/09/03/%E5%AE%89%E8%A3%85anaconda/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/09/03/%E5%AE%89%E8%A3%85anaconda/</guid>
        
        <category>Configurations</category>
        
        
      </item>
    
      <item>
        <title>安装Oh My Zsh</title>
        <description>&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://squidszyd.github.io/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/zsh/2017/08/28/zsh.html&quot;&gt;解决ncurses问题&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;解压 ncurses tar.gz 文件
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;ncurses &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tar&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-zxf&lt;/span&gt; ncurses-6.2.tar.gz &lt;span class=&quot;nt&quot;&gt;-C&lt;/span&gt; ncurses &lt;span class=&quot;nt&quot;&gt;--strip-components&lt;/span&gt; 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://harttle.land/2016/10/25/install-oh-my-zsh-locally.html&quot;&gt;没有root权限zsh&amp;amp;oh-my-zsh安装&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jyzhangchn.github.io/oh-my-zsh.html&quot;&gt;配置oh-my-zsh主题&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Xiaohan-Wang/Backup/blob/master/xh-ys.zsh-theme&quot;&gt;my theme&lt;/a&gt; - modified from theme ys&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;zsh插件管理&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 03 Sep 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/09/03/%E5%AE%89%E8%A3%85oh-my-zsh/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/09/03/%E5%AE%89%E8%A3%85oh-my-zsh/</guid>
        
        <category>Configurations</category>
        
        
      </item>
    
      <item>
        <title>在SLURM节点上启动Jupyter Notebook</title>
        <description>&lt;p&gt;原载于 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/65130699&quot;&gt;分享脚本远程登陆 Jupyter Notebook&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;服务器端&quot;&gt;服务器端&lt;/h4&gt;
&lt;h5 id=&quot;jupytergpush脚本&quot;&gt;jupyterGPU.sh脚本&lt;/h5&gt;
&lt;div class=&quot;language-zsh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --partition compsci-gpu&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --gres=gpu:2&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --time 100:00:00&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#SBATCH --job-name jupyterGPU&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# get tunneling info&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;shuf&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i8000-9999&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;whoami&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;.&quot;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'{print $2}'&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 在这里添加你的服务器地址&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;clusterurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;xxx&quot;&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;:~/.local/bin

&lt;span class=&quot;c&quot;&gt;# print tunneling instructions jupyter-log&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;
MacOS or linux terminal command to create your ssh tunnel:
ssh -N -L &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;clusterurl&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;

 Here is the MobaXterm info:

 Forwarded port:same as remote port
 Remote server: &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
 Remote port: &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
 SSH server: &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;clusterurl&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
 SSH login: &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$user&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
 SSH port: 22

 Use a Browser on your local machine to go to:
 localhost:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; (prefix w/ https:// if using password)

 or copy the URL from below and put there localhost after http:// so it would be something like:
 http://localhost:9499/?token=86c93ba16aaead7529a5da0e5e5a46be7ad8cfea35b2d49f
 &quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# load modules or conda environments here&lt;/span&gt;
jupyter notebook &lt;span class=&quot;nt&quot;&gt;--no-browser&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--ip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h5 id=&quot;提交jupytergpush脚本&quot;&gt;提交jupyterGPU.sh脚本&lt;/h5&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;sbatch jupyterGPU.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h5 id=&quot;查看端口信息浏览器信息&quot;&gt;查看端口信息/浏览器信息&lt;/h5&gt;
&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;slurm.output
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;本地&quot;&gt;本地&lt;/h4&gt;
&lt;h5 id=&quot;监听对应端口&quot;&gt;监听对应端口&lt;/h5&gt;
&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;:&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;:&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;@&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;clusterurl&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h5 id=&quot;浏览器打开jupyter-notebook&quot;&gt;浏览器打开jupyter notebook&lt;/h5&gt;
&lt;p&gt;前往&lt;code class=&quot;highlighter-rouge&quot;&gt;http://localhost:${port}&lt;/code&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 16 Jul 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/07/16/%E5%9C%A8slurm%E4%B8%8A%E4%BD%BF%E7%94%A8jupyter-notebook/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/07/16/%E5%9C%A8slurm%E4%B8%8A%E4%BD%BF%E7%94%A8jupyter-notebook/</guid>
        
        <category>Configurations</category>
        
        
      </item>
    
      <item>
        <title>Context Encoders:_Feature Learning by Inpainting</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1604.07379.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;http://people.eecs.berkeley.edu/~pathak/context_encoder/&quot;&gt;website&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Image inpainting:
    &lt;ul&gt;
      &lt;li&gt;to fill in large missing areas of the image, it can’t get “hints” from nearby pixels&lt;/li&gt;
      &lt;li&gt;a model needs to both understand the content of an image, as well as produce a plausible hypothesis for the missing parts&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;a standard pixel-wise reconstruction loss&lt;/strong&gt;: only the reconstruction loss produces blurry results&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;an adversarial loss&lt;/strong&gt;: adding the adversarial loss results in much sharper predictions&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Encoder-decoder pipeline:
    &lt;ul&gt;
      &lt;li&gt;encoder: produces a latent feature representation of that image&lt;/li&gt;
      &lt;li&gt;decoder: takes this feature representation and produces the missing image content
        &lt;ul&gt;
          &lt;li&gt;a series of five up-convolutional layers (upsampling followed by convolution)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;channel-wise fully-connected layer&lt;/strong&gt;: connect the encoder and the decoder
        &lt;ul&gt;
          &lt;li&gt;directly propagate information from one corner of the feature map to another corner (so that we can better capture global semantic information)&lt;/li&gt;
          &lt;li&gt;essentially a fully-connected layer with groups: only propagates information within feature maps, but it has no parameters connecting different feature maps&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Loss function:
    &lt;ul&gt;
      &lt;li&gt;reconstruction (L2) loss: 
  &lt;img src=&quot;/img/15942152334674.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;
        &lt;ul&gt;
          &lt;li&gt;capturing the overall structure of the missing region and coherence with regards to its context, but prefer a blurry solution (because the expectation of all possible values minimizes the mean squared pixel-wise error)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;adversarial loss: 
  &lt;img src=&quot;/img/15942164552075.jpg&quot; width=&quot;55%&quot; height=&quot;100%&quot; /&gt;
        &lt;ul&gt;
          &lt;li&gt;tries to make prediction look real, and has the effect of picking a particular mode from the distribution&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Region masks
 &lt;img src=&quot;/img/15942173051541.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;central region&lt;/li&gt;
      &lt;li&gt;random block&lt;/li&gt;
      &lt;li&gt;random region (recommended)&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Random block and random region produce a similarly general feature, while significantly outperforming the central region features.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;experiments&quot;&gt;Experiments&lt;/h4&gt;
&lt;h5 id=&quot;implementation-detail&quot;&gt;Implementation detail&lt;/h5&gt;
&lt;p&gt;Pool-free encoders: replacing all pooling layers with convolutions of the same kernel size and stride. (Intuitively, there is no reason to use pooling for reconstruction based networks.)&lt;/p&gt;

&lt;h5 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;Semantic Inpainting
&lt;img src=&quot;/img/15942140401143.jpg&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;The encoder and discriminator architecture is similar to that of discriminator in [1], and decoder is similar to generator in [1].&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Feature Learning
 &lt;img src=&quot;/img/15942140603053.jpg&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;For consistency with prior work, this paper use AlexNet as its encoder in this part.&lt;/li&gt;
      &lt;li&gt;The authors did not manage to make the adversarial loss converge with AlexNet, so they used just the reconstruction loss.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/img/15942180841172.jpg&quot; alt=&quot;-w892&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. ICLR, 2016.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 07 Jul 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/07/07/Context-Encoders-Feature-Learning-by-Inpainting/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/07/07/Context-Encoders-Feature-Learning-by-Inpainting/</guid>
        
        <category>Self-supervision</category>
        
        
      </item>
    
      <item>
        <title>Self-supervised feature learning 综述</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1902.06162.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;self-supervised-feature-learning&quot;&gt;self-supervised feature learning&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;learn visual features from large-scale unlabeled images or videos without using any human annotations&lt;/li&gt;
  &lt;li&gt;a subset of unsupervised learning methods&lt;/li&gt;
  &lt;li&gt;pretext task: the supervisory signal is generated from the data itself by leveraging its structure&lt;/li&gt;
  &lt;li&gt;visual features of images or videos need to be captured by ConvNets to solve the pretext tasks&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;作用&quot;&gt;作用&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;作为pretext task获得pre-trained model
    &lt;ul&gt;
      &lt;li&gt;提供一个好的起始点，加速收敛&lt;/li&gt;
      &lt;li&gt;已经学习到hierarchy features，即使downstream task的数据集很小，也不会过拟合太严重&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;作为auxiliary task来添加regularization&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;分类&quot;&gt;分类&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15932024508723.jpg&quot; alt=&quot;-w1112&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Information recovery: 先抹除图片的一部分信息，然后让网络学习恢复这些信息
    &lt;ul&gt;
      &lt;li&gt;generation based:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;image generation&lt;/strong&gt;: help the network to capture the real distribution of the real data and generate realists data&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;color recovery&lt;/strong&gt;: network need to recognize objects and to group pixels of the same part together&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;inpainting&lt;/strong&gt;: networks are required to learn the common knowledge including the color and structure of the common objects&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;super resolution&lt;/strong&gt;: learn the semantic features of images&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;context based:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Context Similarity (contrasting)&lt;/strong&gt;: learn the invariance within one class and the variance among different classes
            &lt;ul&gt;
              &lt;li&gt;contrasting: train networks to maximum agreement of different views of same scene while minimizing agreement of views from different scenes&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Spatial Context Structure&lt;/strong&gt;：learn spatial context information such as the shape of the objects and the relative positions of different parts of an object&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hard-code program
    &lt;ul&gt;
      &lt;li&gt;This type of methods generally has two steps:
        &lt;ul&gt;
          &lt;li&gt;label generation by employing hard-code programs on images or videos to obtain labels,&lt;/li&gt;
          &lt;li&gt;train ConvNets with the generated labels.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;distill knowledge from hard-code detector&lt;/li&gt;
      &lt;li&gt;one drawback is that the semantic labels generated by hard-code detector usually are very noisy which need to specifically cope with.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Game Engines
    &lt;ul&gt;
      &lt;li&gt;game engines are able to render realistic images and provide accurate pixel-level labels&lt;/li&gt;
      &lt;li&gt;one problem is the domain gap between synthetic and real-world images&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;performance&quot;&gt;Performance&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15932060535480.jpg&quot; alt=&quot;-w913&quot; /&gt;
The performance of self-supervised methods are comparable to the supervised methods on some downstream tasks, especially for the object detection and semantic segmentation tasks.&lt;/p&gt;

</description>
        <pubDate>Fri, 26 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/26/self-supervised-feature-learning/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/26/self-supervised-feature-learning/</guid>
        
        <category>Self-supervision</category>
        
        
      </item>
    
      <item>
        <title>Distilling the Knowledge in a Neural Network</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;transfer the knowledge from the cumbersome model (used in training stage) to a small model (suitable for deployment)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;“knowledge”: a learned mapping from input vectors to output vectors&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;use the class probabilities produced by the cumbersome model as “soft targets” for training the small model.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;small probabilities in the soft targets define a rich similarity structure over the data, but it has very little influence on the cross-entropy cost function during the transfer stage because the probabilities are so close to zero&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;use “distillation” to raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;adding a small term to the objective function that encourages the small model to &lt;strong&gt;predict the true targets as well as matching the soft targets provided by the cumbersome model&lt;/strong&gt; works pretty well.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;h5 id=&quot;整体思路&quot;&gt;整体思路：&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;想要transfer的knowledge是从input到output的映射。也就是说，对于相同的input，希望cumbersome model和small model能生成相同的output (soft label)&lt;/li&gt;
  &lt;li&gt;用cumbersome model和small model产生的soft label做cross entropy loss
    &lt;ul&gt;
      &lt;li&gt;问题：除了hard target对应的概率，其它负标签的概率其实也提供了大量的信息。比如说一张2的图片，对应3的soft label是1e-6，而7的soft label是1e-9，说明这张图片除2之外，更像3而不像7。但是，如果使用传统的softmax层 (T=1)，我们很难有效利用这些信息，因为这些负标签的soft label太小了，基本不能影响cross entropy loss的结果&lt;/li&gt;
      &lt;li&gt;为了能利用负标签对应概率所提供的信息，使用distillation，通过增大T来产生softer probability distribution，此时正负标签对应的soft label差距将被缩小，因此负标签的soft label也能影响最终的cross entropy loss&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;知识蒸馏&quot;&gt;知识蒸馏&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;带温度T的softmax表达式：&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;q_{i}=\frac{\exp \left(z_{i}/T\right)}{\sum_{j} \exp \left(z_{j}/T \right)}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;温度T的影响&lt;br /&gt;
  &lt;img src=&quot;/img/15928689216135.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;原始的softmax函数是 $T=1$ 时的特例&lt;/li&gt;
      &lt;li&gt;温度越高，softmax后各个值z的分布就越平均，原本较小的soft label此时相对增大，对最终cross entropy loss的影响增大，也就提高了对这些原本soft label较小的负标签的关注程度&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如何选择温度T
    &lt;ul&gt;
      &lt;li&gt;不是所有的负标签对应的soft label都是有效的：这些soft label本身是由Teacher-Net (cumbersome model) 训练得到的，因此结果一定存在noise，而且soft label越小，noise的影响越大&lt;/li&gt;
      &lt;li&gt;温度T决定了对负标签的关注程度
        &lt;ul&gt;
          &lt;li&gt;从有部分信息量的负标签中学习 –&amp;gt; 温度要高一些&lt;/li&gt;
          &lt;li&gt;防止受负标签中噪声的影响 –&amp;gt;温度要低一些&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;知识蒸馏方法&quot;&gt;知识蒸馏方法&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15928705010652.jpg&quot; width=&quot;70%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;训练Teacher model&lt;/li&gt;
  &lt;li&gt;对Teacher model蒸馏，得到student distilled model
    &lt;ul&gt;
      &lt;li&gt;soft prediction loss：Teacher model和 Student model在相同温度T下产生的soft label的cross entropy loss&lt;/li&gt;
      &lt;li&gt;hard prediction loss：Student model在T=1时和groundtruth的cross entropy loss&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;note&quot;&gt;Note&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;the magnitudes of the gradients produced by the soft targets scale as $\frac{1}{T^2}$, so it is important to multiply them by $T^2$ when using both hard and soft targets&lt;/li&gt;
  &lt;li&gt;matching logits is a special case of distillation&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/102038521&quot;&gt;知识蒸馏&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/61944055&quot;&gt;交叉熵与KL散度&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 21 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/21/Distilling-the-Knowledge-in-a-Neural-Network/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/21/Distilling-the-Knowledge-in-a-Neural-Network/</guid>
        
        <category>Knowledge Distillation</category>
        
        
      </item>
    
      <item>
        <title>迁移学习损失函数</title>
        <description>&lt;h4 id=&quot;基于统计量&quot;&gt;基于统计量&lt;/h4&gt;
&lt;h5 id=&quot;mmd-maximum-mean-discrepancy&quot;&gt;MMD (maximum mean discrepancy)&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;通过比较两个distribution的sample，直接获得两个distribution的差异&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://xiaohan-wang.github.io/2020/06/17/MMD/&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;基于对抗学习&quot;&gt;基于对抗学习&lt;/h4&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
</description>
        <pubDate>Wed, 17 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/17/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/17/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</guid>
        
        <category>Loss</category>
        
        
      </item>
    
      <item>
        <title>估计量的无偏性、有效性、一致性</title>
        <description>&lt;h4 id=&quot;估计量&quot;&gt;估计量&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;根据样本构造一个统计量，作为总体未知参数的估计，则该统计量为估计量&lt;/li&gt;
  &lt;li&gt;估计量可视为一个随机变量
    &lt;ul&gt;
      &lt;li&gt;同一个总体可以进行多次抽样，不同的抽样结果可以计算出不同的估计量取值&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;无偏性&quot;&gt;无偏性&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;无偏估计指，估计量的数学期望等于被估计参数的真实值&lt;/li&gt;
  &lt;li&gt;例子&lt;br /&gt;
  &lt;img src=&quot;/img/15924068989806.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
  假设圆心是被估计参数的真实值，粉色x代表每次抽样计算出的估计量。左图估计量的期望等于被估计参数的真实值，是无偏的。右图估计量的期望不等于被估计参数的真实值，是有偏的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;有效性&quot;&gt;有效性&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;指估计量的离散程度，离散程度越小越有效&lt;/li&gt;
  &lt;li&gt;注意，有效性和无偏性是不相关的&lt;br /&gt;
  &lt;img src=&quot;/img/15924081407687.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;无偏估计不一定是最好的&lt;br /&gt;
  &lt;img src=&quot;/img/15924083443839.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;一致性&quot;&gt;一致性&lt;/h4&gt;
&lt;p&gt;待填…&lt;/p&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/22983179&quot;&gt;什么是无偏估计？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/qq_40597317/article/details/80639511&quot;&gt;估计量的无偏性、有效性、一致性&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 17 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/17/%E6%9C%89%E5%81%8F%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/17/%E6%9C%89%E5%81%8F%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/</guid>
        
        <category>Statistics</category>
        
        
      </item>
    
      <item>
        <title>MMD</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.aaai.org/Papers/AAAI/2007/AAAI07-262.pdf&quot;&gt;AAAI 2007 paper&lt;/a&gt; and &lt;a href=&quot;http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf&quot;&gt;JMLR 2012 paper&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;传统的用于衡量两个概率分布P和Q差别的方法，例如KL divergence，要求概率分布P和Q已知。也就是说，&lt;strong&gt;如果我们只有来自P和Q的样本，那么我们需要先进行概率密度估计 (density estimation)，然后才能衡量两个分布的差异&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;MMD：利用来自P和Q的样本，直接获得它们对应的总体概率分布的差异，而无需density estimation这一中间步骤。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;mmd-metric&quot;&gt;MMD metric&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Give observations $X := {x_1, \cdots , x_m}$ and $Y := {y_1, \cdots, y_n}$, which are independently and identically distributed (i.i.d.) from distribution $p$ and $q$. Let $\mathcal{F}$ be a class of functions $f : X \to \mathbb{R}$, and shorthand notation $E_x[ f(x)] :=E_{x \sim p}[ f(x)]$ and $E_y[ f(y)] := E_{y\sim q}[ f(y)]$ denote expectations with respect to $p$ and $q$, respectively, where $x \sim p$ indicates x has distribution $p$.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maximum mean discrepancy (MMD) is defined as:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;MMD[\mathcal{F}, p,q] := \sup_{f \in \mathcal{F}}(E_x[ f(x)]−E_y[ f(y)]) .&lt;/script&gt;

    &lt;p&gt;We must therefore dentify a function class that is &lt;strong&gt;rich enough to uniquely identify whether $p = q$&lt;/strong&gt;. And since we want to obtain this discrepancy by sample $X$ and $Y$,  the function class should also be &lt;strong&gt;restrictive enough to provide useful finite sample estimates&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Propose the unit ball in a reproducing kernel Hilbert space $\mathcal H$ as our MMD function class $\mathcal{F}$. Define &lt;strong&gt;mean embedding&lt;/strong&gt; $\mu_p(t) \in \mathcal{H}$ such that &lt;script type=&quot;math/tex&quot;&gt;E_x [f(x)] = \lt f, \mu_p \gt _{\mathcal{H}}&lt;/script&gt; for all $f \in \mathcal{H}$. If we set $f= \phi (t) = k(t, \cdot)$, we obtain $\mu_p(t) = &amp;lt;\mu_p, k(t, ·)&amp;gt;_\mathcal{H} =E_xk(t, x)$, in other words, the mean embedding of the distribution $p$ is the expectation under $p$ of the canonical feature map.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;We thus have
    &lt;center&gt; 
 $$
 \begin{split}
 MMD^2 \left[ \mathcal{F}, p,q \right] &amp;amp;= \left[ \sup_{||f||_\mathcal{H} \leq 1}(E_x [ f(x)]−E_y [ f(y)])
\right]^2\\&amp;amp;=    \left[ \sup_{||f||_\mathcal{H} \leq 1}&amp;lt;\mu_p-\mu_q,f&amp;gt;_\mathcal{H}
\right]^2\\&amp;amp;=||\mu_p-\mu_q||_\mathcal{H}^2\\&amp;amp;=||E_x\phi(x)-E_y\phi(y)||_\mathcal{H}^2
 \end{split}
 $$
 &lt;/center&gt;
  &lt;/li&gt;
  &lt;li&gt;The MMD is a metric, when $\mathcal{H}$ is a universal RKHSs, defined on a compact metric space X. It can be proven that &lt;strong&gt;the Gaussian and Laplace RKHSs&lt;/strong&gt; are universal.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;finite-sample-estimate&quot;&gt;Finite sample estimate&lt;/h4&gt;
&lt;p&gt;Given $x$ and $x^{\prime}$ independent random variables with distribution $p$, and $y$ and $y^\prime$ independent random variables with distribution $q$, the squared population MMD is&lt;/p&gt;
&lt;center&gt;
$$
MMD^2 [\mathcal{F}, p,q] = E_{x,x^\prime}
\left[k(x, x^\prime)\right] +E_{y,y^\prime}\left[ k(y, y′)\right]−2E_{x,y} \left[k(x, y)\right]
$$
&lt;/center&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;An unbiased empirical estimate
 &lt;img src=&quot;/img/15924519721534.jpg&quot; width=&quot;75%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An biased empirical estimate
 &lt;img src=&quot;/img/15924520728791.jpg&quot; width=&quot;75%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A linear time statistics&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf&quot;&gt;link&lt;/a&gt; $P_{739}$ Lemma 14&lt;/li&gt;
      &lt;li&gt;need sufficient data (many more samples than the quadratic-cost tests)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Estimate 1 and 2 cost $O((m+n)^2)$ time to compute both statistics&lt;/li&gt;
  &lt;li&gt;Estimate 3 can be computed in linear time&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cnblogs.com/kailugaji/p/11004246.html&quot;&gt;MATLAB最大均值差异&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 17 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/17/MMD/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/17/MMD/</guid>
        
        <category>Statistics</category>
        
        
      </item>
    
      <item>
        <title>核方法、核函数、核技巧、再生核希尔伯特空间</title>
        <description>&lt;h4 id=&quot;核方法kernel-method&quot;&gt;核方法（kernel method）&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;理论基础: Cover’s theorem，其指出，在低维空间中线性不可分的数据，通过非线性变换将其投影到高维空间之后，大概率会变为线性可分的数据&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;将低维空间的非线性可分问题，转化为高维空间的线性可分问题&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;核技巧kernel-trick&quot;&gt;核技巧（kernel trick）&lt;/h4&gt;
&lt;p&gt;设$\phi(x)$为映射函数，$\phi(x): \mathcal{X} \to \mathcal{H}$，其中$\mathcal{X}$为输入空间，$\mathcal{H}$为特征空间 (特征空间需要是Hilbert space，即完备的内积空间)。&lt;/p&gt;

&lt;p&gt;欲求$&amp;lt;\phi(x_1), \phi(x_2)&amp;gt;$：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;传统方法：先分别计算$\phi(x_1)$和$\phi(x_2)$，再在特征空间中计算二者的内积
    &lt;ul&gt;
      &lt;li&gt;缺点：当特征空间维度很大时，计算非常复杂&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;核技巧：在输入空间找到一个函数$K(x_1, x_2)$，使得$K(x_1, x_2)=&amp;lt;\phi(x_1), \phi(x_2)&amp;gt;$，从而可以直接在低维空间中计算出结果，加速核方法计算。对应的函数 $K$ 就是核函数&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;核函数kernels&quot;&gt;核函数（kernels）&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;在实际应用时，映射函数 $\phi(x)$ 不需要是已知的。换句话说，核技巧的目的就是在不需要显式地定义特征空间和映射函数的条件下，计算映射之后的内积&lt;/li&gt;
  &lt;li&gt;核函数 $K$ 本质上需要满足的条件 (不需要$\phi(x)$已知):
    &lt;ul&gt;
      &lt;li&gt;对称性: $K(\mathrm{x_1},\mathrm{x_2}) = K(\mathrm{x_2},\mathrm{x_1})$&lt;/li&gt;
      &lt;li&gt;半正定性: 对于任意 $n$ 和任意 $x_1, x_2, \cdots, x_n  \in \mathcal{X}$，由 $K(x_i, x_j)$ 定义的 Gram matrix 总是半正定的&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;只要 $K$ 是核函数，那么一定存在一个Hilbert space和一个映射函数$\phi$，使得$K(x_1, x_2)=&amp;lt;\phi(x_1), \phi(x_2)&amp;gt;$&lt;/li&gt;
  &lt;li&gt;常见核函数&lt;br /&gt;
  &lt;img src=&quot;/img/15923213053700.jpg&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
  其它变换得到的核函数：参考资料3 $\text{P}_{17, 18}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;再生核希尔伯特空间reproducing-kernel-hilbert-spacesrkhs&quot;&gt;再生核希尔伯特空间（reproducing kernel Hilbert spaces，RKHS）&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;再生核希尔伯特空间
    &lt;ul&gt;
      &lt;li&gt;设 $\displaystyle \mathcal{H}$ 是希尔伯特空间，其元素为函数 $\displaystyle f:X\rightarrow R$。对于某个固定的 $\displaystyle x\in \mathcal{X}$，映射$\displaystyle \delta _{x} :H\rightarrow R,\delta _{x} :f\rightarrow f( x)$称为点 $x$ 的 (Dirac) evaluation functional&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;设 $\displaystyle \mathcal{H}$ 是希尔伯特空间，其元素为函数 $\displaystyle f:X\rightarrow R$。若对于任意的 $x \in \mathcal{X}$，$\delta_x$ 都是连续的，则 $\mathcal{H}$ 为RKHS&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;RKHS是一个函数空间，其中的元素 $f$ 为函数。通常情况下特征空间 $\mathcal{H}$ 为 RKHS。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;再生核
    &lt;ul&gt;
      &lt;li&gt;再生核 $K$ 是核函数的一种，其满足
        &lt;ul&gt;
          &lt;li&gt;对于任意固定的 $x_0\in\mathcal{X}$，$K(x,x_0)$作为 $x$ 的函数属于我们的函数空间$\mathcal{H}$&lt;/li&gt;
          &lt;li&gt;对于任意 $x\in\mathcal{X}$ 和 $f(\cdot)\in\mathcal{H}$，有 $f(x) = \langle f(\cdot),K(\cdot,x)\rangle$ (再生性质 / reproducing property)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;对于再生核 $K$，我们可以自然的定义映射函数 $\phi(x)=K(\cdot, x)$，此时，通过再生核的再生性质，可知
  &lt;script type=&quot;math/tex&quot;&gt;\langle \phi(x_1),\phi(x_2)\rangle = \langle K(\cdot,x_1),K(\cdot,x_2)\rangle = K(x_1,x_2)&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;关系
    &lt;ul&gt;
      &lt;li&gt;一个希尔伯特空间存在至多一个再生核 (不存在 / 存在一个)&lt;/li&gt;
      &lt;li&gt;存在再生核的希尔伯特空间就是再生核希尔伯特空间&lt;/li&gt;
      &lt;li&gt;一个再生核对应唯一的再生核希尔伯特空间&lt;/li&gt;
      &lt;li&gt;再生核和再生核希尔伯特空间是一一对应的&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;一个核函数 $K$ 可能对应多个映射函数 $\phi$，而每个映射函数$\phi$ 有自己对应的特征空间 (i.e. 希尔伯特空间）
    &lt;ul&gt;
      &lt;li&gt;例子详见 参考资料6 $P_{11}$ example 35&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;任意一个核函数 $K$ 都可以作为再生核，构建其对应的唯一的再生核希尔伯特空间&lt;/li&gt;
  &lt;li&gt;对于任意一个核函数 $K$，可以存在多个对应的特征空间 $\mathcal{H_0}$，但其作为再生核只对应唯一的RKHS。同时，RKHS是所有特征空间最为“精简”的一个，这里的精简体现在，无论在 $\mathcal{H_0}$ 中得到怎样的分类模型 $⟨w,\phi_0(x)⟩_{\mathcal{H_0}}$，在RKHS中都存在一个 $f$ 可以得到和它相同的效果，因此对于某个核函数，RKHS代表了它最本征的信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/61794781&quot;&gt;核方法、核技巧和核函数&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.fanyeong.com/2017/11/13/the-kernel-trick/&quot;&gt;核技巧&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.stat.berkeley.edu/~bartlett/courses/2014spring-cs281bstat241b/lectures/20-notes.pdf&quot;&gt;PPT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cosx.org/2014/05/svm-series-add-2-kernel-ii/&quot;&gt;“支持向量机系列” 的番外篇二: Kernel II&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/54704957&quot;&gt;什么是RKHS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/RKHS_Notes1.pdf&quot;&gt;课程讲义&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://murongxixi.github.io/2013/11/12/%E6%A0%B8%E5%87%BD%E6%95%B0%EF%BC%8C%E5%86%8D%E7%94%9F%E6%A0%B8Hilbert%E7%A9%BA%E9%97%B4%EF%BC%8C%E8%A1%A8%E7%A4%BA%E5%AE%9A%E7%90%86/&quot;&gt;核函数，再生核Hilbert空间，表示定理&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 16 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/16/%E6%A0%B8%E6%96%B9%E6%B3%95-%E6%A0%B8%E5%87%BD%E6%95%B0-%E6%A0%B8%E6%8A%80%E5%B7%A7/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/16/%E6%A0%B8%E6%96%B9%E6%B3%95-%E6%A0%B8%E5%87%BD%E6%95%B0-%E6%A0%B8%E6%8A%80%E5%B7%A7/</guid>
        
        <category>Math</category>
        
        
      </item>
    
  </channel>
</rss>
