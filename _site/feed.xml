<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 01 Apr 2020 03:16:54 -0400</pubDate>
    <lastBuildDate>Wed, 01 Apr 2020 03:16:54 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Moment Matching for Multi-Source Domain Adaptation</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Moment_Matching_for_Multi-Source_Domain_Adaptation_ICCV_2019_paper.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;http://ai.bu.edu/M3SDA/&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;the upper bound on the target error of the learned hypothesis depends on the pairwise moment divergence $d_{CM^k}(D_S, D_T)$ between the target domain and each source domain. (This provides a direct motivation for moment matching approaches)&lt;/li&gt;
  &lt;li&gt;The last term target error is lower bounded by the pairwise divergences between source domains (implies we might also need to minimize divergence between source domains)&lt;/li&gt;
  &lt;li&gt;DomainNet: multi-souce domain adaptation dataset&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15857182767863.jpg&quot; alt=&quot;-w909&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;moment distance based loss:&lt;br /&gt;
&lt;img src=&quot;/img/15857184320957.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;overall training objective:&lt;br /&gt;
&lt;img src=&quot;/img/15857185193833.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;In order to align $p(y;x)$ and $p(x)$ at the same time, they follow the training paradigm proposed in [1]. They leverage two classifiers per domain to form N pairs of classifiers $C′ = {(C1, C1′), (C2, C2′), …, (CN, CN ′)}$. The training procedure includes three steps.
    &lt;ol&gt;
      &lt;li&gt;train $G$ and $C′$ to classify the multi-source samples correctly based on the overall training objective.&lt;/li&gt;
      &lt;li&gt;train the classifier pairs for a fixed $G$. The goal is to make the discrepancy of each pair of classifiers as large as possible on the target domain.&lt;/li&gt;
      &lt;li&gt;fix $C′$ and train $G$ to minimize the discrepancy of each classifier pair on the target domain.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15857191596055.jpg&quot; alt=&quot;-w781&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15857190392968.jpg&quot; alt=&quot;-w851&quot; /&gt;个人感觉主要是$M^3SDA-\beta$中的training paradigm在发挥作用。&lt;/p&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), June 2018.&lt;/li&gt;
  &lt;li&gt;Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnab´as P´oczos. Mmd gan: Towards deeper understanding of moment matching network. In Advances in Neural Information Processing Systems, pages 2203–2213, 2017.&lt;/li&gt;
  &lt;li&gt;Yujia Li, Kevin Swersky, and Rich Zemel. Generative mo- ment matching networks. In International Conference on Machine Learning, pages 1718–1727, 2015.&lt;/li&gt;
  &lt;li&gt;Youssef Mroueh, Tom Sercu, and Vaibhava Goel. McGan: Mean and covariance feature matching GAN. In Doina Pre- cup and Yee Whye Teh, editors, Proceedings ofthe 34th In- ternational Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2527– 2535, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2，3，4是Gan based moment matching&lt;/p&gt;

</description>
        <pubDate>Tue, 31 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/31/Moment-Matching-for-Multi-Source-Domain-Adaptation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/31/Moment-Matching-for-Multi-Source-Domain-Adaptation/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Short Summay for Some Papers 1</title>
        <description>&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unsupervised Domain Adaptation with Similarity Learning (CVPR 2018)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/15857245561079.jpg&quot; width=&quot;60%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;a similarity-based classifier in which each image (from either the source or target domain) is compared to a set of prototypes (or centroides). The label associated to the prototype that best matches the query image is given to it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 30 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/30/week-summary/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/30/week-summary/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Boosting Domain Adaptation by Discovering Latent Domains</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://research.mapillary.com/img/publications/CVPR18b.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/mancinimassimiliano/latent_domains_DA&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15854715937974.jpg&quot; width=&quot;40%&quot; height=&quot;60%&quot; /&gt;
第一篇针对classification task的deep learning based multi-source domain adaptation. 本质上是multi-source的AdaBN.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15857203223787.jpg&quot; alt=&quot;-w763&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;a side-output branch is empolyed to predict domain assignment probabilities for each input sample.&lt;/li&gt;
  &lt;li&gt;mDA-layer to renormalize the multi-modal feature distributions&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;domain-discovery&quot;&gt;domain discovery&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;domain prediction branch: sample points at different mDA-layers corresponding to a single input element to the network should share the same probabilities&lt;/li&gt;
  &lt;li&gt;split the network into a domain prediction branch and classification branch at some low level layer: features tend to become increasingly more domain invariant going deeper into the network, meaning that it becomes increasingly harder to compute a sample’s domain as a function of deeper features.&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;multi-source-domain-adaptation-layer&quot;&gt;multi-source domain adaptation layer&lt;/h5&gt;
&lt;blockquote&gt;
  &lt;p&gt;Domain Alignment layers: aim to reduce internal domain shift at different levels within the network by re-normalizing features in a domain-dependent way, matching their distributions to a pre-determined one.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$q_{d|x}(d | x_i)$ is the conditional probability of $x_i$ belonging to $d$, given $x_i$&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;compute $\mu$ and $\sigma^2$&lt;br /&gt;
 &lt;img src=&quot;/img/15857223593758.jpg&quot; width=&quot;30%&quot; height=&quot;40%&quot; /&gt;&lt;br /&gt;
 &lt;img src=&quot;/img/15857224509940.jpg&quot; width=&quot;45%&quot; height=&quot;40%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;normalize each sample&lt;br /&gt;
By substituting $w_{i,d}$ for $q_{d|x}(d | x_i)$&lt;br /&gt;
&lt;img src=&quot;/img/15857225154947.jpg&quot; width=&quot;45%&quot; height=&quot;40%&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;overall-training-objective&quot;&gt;overall training objective&lt;/h5&gt;
&lt;p&gt;only use source dataset now
&lt;img src=&quot;/img/15857216573544.jpg&quot; width=&quot;45%&quot; height=&quot;40%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;labeled data的分类loss (cross entropy)&lt;/li&gt;
  &lt;li&gt;data with known domain的预测loss (cross entropy)&lt;/li&gt;
  &lt;li&gt;unlabeled data的分类 (minimizing uncertainty)&lt;/li&gt;
  &lt;li&gt;data without domain label的预测loss (minimizing uncertainty)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15857232316372.jpg&quot; width=&quot;50%&quot; height=&quot;60%&quot; /&gt;
&lt;img src=&quot;/img/15857232406316.jpg&quot; width=&quot;50%&quot; height=&quot;60%&quot; /&gt;
在latent domain明显的时候(digit和PACS), model表现更好。但对于office-31, 可能因为很难直接发现latent domain，所以基本没有提高。&lt;/p&gt;

&lt;h4 id=&quot;reflection&quot;&gt;Reflection&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;multi-source DA可以地方可以借鉴single-source的思想，比如说本文用的domain-alignment layer&lt;/li&gt;
  &lt;li&gt;难点在于 好的latent domain discovery&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;further-reference-for-latent-domain-discovery&quot;&gt;Further reference (for latent domain discovery)&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;J. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discovering latent domains for multisource domain adaptation. In ECCV, 2012.&lt;/li&gt;
  &lt;li&gt;B. Gong, K. Grauman, and F. Sha. Reshaping visual datasets for domain adaptation. In NIPS, 2013.&lt;/li&gt;
  &lt;li&gt;C. Xiong, S. McCloskey, S.-H. Hsieh, and J. J. Corso. Latent domains modeling for visual domain adaptation. In AAAI, 2014.&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sun, 29 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/29/Boosting-Domain-Adaptation-by-Discovering-Latent-Domains/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/29/Boosting-Domain-Adaptation-by-Discovering-Latent-Domains/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>AlexNet</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;60 million parameters; 650,000 neurons&lt;/p&gt;

&lt;h4 id=&quot;why-works&quot;&gt;Why works&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;数据集的改变：
    &lt;ul&gt;
      &lt;li&gt;以前， on the order of tens of thousands of images&lt;/li&gt;
      &lt;li&gt;最近，Imagenet；LabelMe&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CNN
    &lt;ul&gt;
      &lt;li&gt;enough learning capacity&lt;/li&gt;
      &lt;li&gt;prior knowledge (assumptions): stationarity of statistics and locality of pixel dependencies&lt;/li&gt;
      &lt;li&gt;compared to NN, CNN has fewer parameters: easier to train&lt;/li&gt;
      &lt;li&gt;GPU and large dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;alexnet&quot;&gt;AlexNet&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Input:
    &lt;ul&gt;
      &lt;li&gt;256*256, 首先reshape将短边变成256，然后center crop&lt;/li&gt;
      &lt;li&gt;预处理：减去training set的mean，将输入数据变为centered raw RGB value&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;network
    &lt;ul&gt;
      &lt;li&gt;采用ReLU：sigmoid/tanh梯度下降所需的训练时间比ReLU慢很多倍&lt;/li&gt;
      &lt;li&gt;多GPU训练：GTX 580 GPU只有3GB momory，限制了网络size。为了能扩大网络size，采用多GPU训练&lt;/li&gt;
      &lt;li&gt;local response normalization: a form of lateral inhibition inspired by the type found in real neurons&lt;/li&gt;
      &lt;li&gt;creating competition  for big activities amongst neuron outputs computed using different kernels&lt;/li&gt;
      &lt;li&gt;overlapping pooling: slightly more difficult to overfit&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Architecture
&lt;img src=&quot;/img/15854655847916.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/15854656864364.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;减少过拟合
    &lt;ul&gt;
      &lt;li&gt;Data augmentation
        &lt;ul&gt;
          &lt;li&gt;train：random crop (224*224) + horizontal reflections (increase the size of training set by a factor of 2048)&lt;/li&gt;
          &lt;li&gt;test：predict with 10 224*224 patches (4 corners + center and horizontal reflection), then average&lt;/li&gt;
          &lt;li&gt;PCA color augmentation: 相当于改变了the intensity and color of the illumination&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Dropout&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;quantitative evaluation&lt;br /&gt;
&lt;img src=&quot;/img/15854658726746.jpg&quot; width=&quot;50%&quot; height=&quot;30%&quot; /&gt;
&lt;img src=&quot;/img/15854658948549.jpg&quot; width=&quot;60%&quot; height=&quot;40%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;qualitative evaluation
&lt;img src=&quot;/img/15854660821423.jpg&quot; alt=&quot;&quot; /&gt;
kernels learned on the first layer: frequency- and orientation-selective kernels, and various colored blobs&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Fri, 27 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/27/AlexNet/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/27/AlexNet/</guid>
        
        <category>CV paperlist</category>
        
        
      </item>
    
      <item>
        <title>Reshaping Visual Datasets for Domain Adaptation</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5210-reshaping-visual-datasets-for-domain-adaptation.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/15852773040000.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;图像的domain由多种因素交织影响决定，比如说pose、illumination、camera resolution、background等等。因此图像往往不是按照“clearly identifiable domain”收集的，也就是说，一个数据集可以包含多个不同的domain。发现这些source和target dataset中的latent domain有助于进行domain adaptation.&lt;/p&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Note that the total domain distinctiveness TDD(K) increases as K increases — presumably, in the extreme case, each domain has only a few instances and their distributions would be maximally different from each other. However, such tiny domains would offer insufficient data to separate the categories of interest reliably.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;maximum distinctiveness: 希望不同domain之间的差异尽量大 (domain划分尽量细)&lt;/p&gt;

    &lt;p&gt;采用non-parameter方式:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;the empirical distribution of each domain:
 &lt;img src=&quot;/img/15852779640661.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;the difference between the means of two empirical distributions
 &lt;img src=&quot;/img/15852781025087.jpg&quot; alt=&quot;-w837&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;maximize the total domain distinctiveness
 &lt;img src=&quot;/img/15852781836513.jpg&quot; width=&quot;30%&quot; height=&quot;30%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;maximum learnability: 每个domain包含足够数量的instances用于学习 (domain划分不能过细)&lt;/p&gt;

    &lt;p&gt;从K=2开始遍历寻找最好的K&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;对K个domain分别构建classfier，并计算cross-validation accuracy，并求weighted average
 &lt;img src=&quot;/img/15852795347553.jpg&quot; alt=&quot;-w699&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;选择具有最高的cross-validation accuracy的K&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;p&gt;domain adaptaion的几种方式&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;合并全部source domain&lt;/li&gt;
  &lt;li&gt;emsemble各个source domain的classfier的结果&lt;/li&gt;
  &lt;li&gt;选择和target domain最接近的一个source domain&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;reshape target domain conditioning on the identified domains from the training datasets — the goal is to discover latent domains in the test datasets that match the domains in the training datasets as much as possible.
&lt;img src=&quot;/img/15852803974334.jpg&quot; alt=&quot;-w752&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/img/15852803845757.jpg&quot; width=&quot;93%&quot; height=&quot;93%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;further-reference&quot;&gt;Further reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;J. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discovering latent domains for multisource domain adaptation. In ECCV. 2012. (aim at discovering the latent domains from datasets, by modeling the data with a hierarchical distribution consisting of Gaussian mixtures)&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 26 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/26/Reshaping-Visual-Datasets-for-Domain-Adaptation/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/26/Reshaping-Visual-Datasets-for-Domain-Adaptation/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>SPP-Net (spatial pyramid pooling)</title>
        <description>&lt;h4 id=&quot;goal&quot;&gt;Goal&lt;/h4&gt;
&lt;p&gt;消除CNN对图片size/scale的限制&lt;/p&gt;

&lt;h4 id=&quot;motivation&quot;&gt;Motivation&lt;/h4&gt;
&lt;p&gt;CNN = convolutional layers + fully-connected layers&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;convolutional layers不要求图片size&lt;/li&gt;
  &lt;li&gt;fully-connected layers对size有要求&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;思路：在fully-connected layers前面加入SPP layer, 其能对不同size的feature产生fixed-length output，然后送入fully-connected layers&lt;/p&gt;

&lt;p&gt;在image detection上的应用
&lt;img src=&quot;/img/15849366399167.jpg&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15849365194047.jpg&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In each spatial bin, pool (max pooling in the paper) the response of each filter. &lt;br /&gt;
The output of SPP: k*M (M: number  of bins, e.g. 16+4+1; k: number of channels, e.g. 256)&lt;/p&gt;

</description>
        <pubDate>Wed, 25 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/25/SPP-Net/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/25/SPP-Net/</guid>
        
        <category>CV paperlist</category>
        
        
      </item>
    
      <item>
        <title>Domain Adaptation for Semantic Segmentation with Maximum Squares Loss</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Domain_Adaptation_for_Semantic_Segmentation_With_Maximum_Squares_Loss_ICCV_2019_paper.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/ZJULearning/MaxSquareLoss&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;p&gt;Probability imbalance problem -&amp;gt; maximum square loss&lt;/p&gt;

&lt;h4 id=&quot;probability-imbalance-problem&quot;&gt;Probability imbalance problem&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15851597726007.jpg&quot; width=&quot;70%&quot; height=&quot;70%&quot; /&gt;
Note that uncertain area has prediction probability around 0.5, while prediction probability around 1 corresponds to areas with good accuracy.&lt;/p&gt;

&lt;p&gt;Shannon entropy:
&lt;img style=&quot;text-align: center&quot; src=&quot;/img/15851603980105.jpg&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;
Maximum squares loss:
&lt;img style=&quot;text-align: center&quot; src=&quot;/img/15851604671115.jpg&quot; width=&quot;45%&quot; height=&quot;45%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Classes with high accuracy always have higher prediction probabilities. If Shannon entropy is deployed, as shown in the above figure, the simple class will produce a much larger gradient on each pixel than the difficult class, causing each gradient descent step is basically computed from those simple classes, and difficult classes is still not trained sufficiently. By using MSL, areas with higher confidence still have larger gradients, but their dominant effects have been reduced, allowing other difficult classes to obtain training gradients.&lt;/p&gt;

&lt;h4 id=&quot;class-imbalance-problem&quot;&gt;Class imbalance problem&lt;/h4&gt;
&lt;p&gt;Classes with higher accuracy always have more pixels on the label map, which leads to an imbalance in quantity.&lt;/p&gt;

&lt;p&gt;regular method: introduce weighting factor αc, which is usually set as the inverse class frequency.&lt;/p&gt;

&lt;p&gt;paper: Image-wise Class-balanced Weighting Factor&lt;/p&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;fix class imbalance
    &lt;ul&gt;
      &lt;li&gt;regular way: introduce weighting factor αc, which is usually set as the inverse class frequency.&lt;/li&gt;
      &lt;li&gt;image-wise weighting ratio (this paper)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;multi-level self-produced guidance&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 24 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/24/Maximum-Squares-Loss/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/24/Maximum-Squares-Loss/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>ADVENT:Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1811.12833.pdf&quot;&gt;Paper link&lt;/a&gt; and &lt;a href=&quot;https://github.com/valeoai/ADVENT&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;take-away-message&quot;&gt;Take away message&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;观察到source domain prediction是over-confident / low entropy，而target domain prediction是under-confident / high entropy&lt;/li&gt;
  &lt;li&gt;direct entropy minimization&lt;/li&gt;
  &lt;li&gt;structure adaptation: adversarial learning&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;model&quot;&gt;Model&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15850310997747.jpg&quot; alt=&quot;-w930&quot; /&gt;&lt;/p&gt;

&lt;p&gt;class ratio prior:&lt;br /&gt;
Entropy minimization can get biased towards some easy classes. Therefore, sometimes it is beneficial to guide the learning with some prior. 此处根据class ratio prior设计了另一个Loss来guide learning.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;entropy map and $L_{ent}$&lt;br /&gt;
&lt;img src=&quot;/img/15850758404417.jpg&quot; width=&quot;40%&quot; height=&quot;40%&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/15850759324138.jpg&quot; width=&quot;25%&quot; height=&quot;40%&quot; /&gt;&lt;br /&gt;
a soft-assignment version of the pseudo-label cross-entropy loss&lt;/li&gt;
  &lt;li&gt;Adversarial learning loss&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;result&quot;&gt;Result&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;In general, AdvEnt works better than MinEnt. By combining results of the two models MinEnt and AdvEnt, we observe a decent boost in performance, compared to results of single models. 
&lt;img src=&quot;/img/15850315105452.jpg&quot; alt=&quot;-w923&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;oracle performance (trained on source and target)
&lt;img src=&quot;/img/15850315456104.jpg&quot; width=&quot;40%&quot; height=&quot;40%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;tricks&quot;&gt;Tricks&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Training on specific entropy ranges.&lt;/li&gt;
  &lt;li&gt;Using class-ratio prior.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;insight&quot;&gt;Insight&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;找source domain结果/中间feature map具备、而target domain不具备的性质，用adversarial learning (e.g. cross entropy)&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sat, 21 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/21/ADVENT/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/21/ADVENT/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Unsupervised Domain Adaptation using Feature-Whitening and Consensus Loss</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1903.03215.pdf&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;https://github.com/roysubhankar/dwt-domain-adaptation&quot;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;take-away-message&quot;&gt;Take Away Message&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Domain alignment layers which implement feature whitening for the purpose of matching source and target feature distributions and increases the smoothness of the loss landscape.&lt;/li&gt;
  &lt;li&gt;Min-Entropy Consensus loss regularizes training while avoiding the adoption of many user-defined hyper-parameters&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/15843077382687.jpg&quot; alt=&quot;-w752&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DA的四种方法：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Correlation Alignment paradigm&lt;/li&gt;
  &lt;li&gt;domain-specific alignment layers&lt;/li&gt;
  &lt;li&gt;entropy minimization distribution&lt;/li&gt;
  &lt;li&gt;consistency-enforcing paradigm&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;由 12 -&amp;gt; DWTL layer&lt;br /&gt;
由 34 -&amp;gt; MEC loss&lt;/p&gt;

&lt;p&gt;source: cross-entropy loss
&lt;img src=&quot;/img/15843177084504.jpg&quot; alt=&quot;-w200&quot; /&gt;
target: MEC loss
&lt;img src=&quot;/img/15843177196984.jpg&quot; alt=&quot;-w300&quot; /&gt;
final loss:
&lt;img src=&quot;/img/15843177315198.jpg&quot; alt=&quot;-w300&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;要求在source上表现好&lt;/li&gt;
  &lt;li&gt;要求在target上对perturbation具有robustness，同时对结果high confidence&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;result&quot;&gt;Result&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;digit classification&lt;/li&gt;
  &lt;li&gt;object recognition&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;potential-improvement&quot;&gt;Potential Improvement&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;try full “coloring”&lt;/li&gt;
  &lt;li&gt;MEC loss中pseudo label的选择方式？&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 14 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/14/Unsupervised-Domain-Adaptation-using-Feature-Whitening-and-Consensus-Loss/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/14/Unsupervised-Domain-Adaptation-using-Feature-Whitening-and-Consensus-Loss/</guid>
        
        <category>Domain Adaptation</category>
        
        
      </item>
    
      <item>
        <title>Domain Adaptation Concept Summary</title>
        <description>&lt;h3 id=&quot;transfer-learning&quot;&gt;Transfer learning&lt;/h3&gt;
&lt;h4 id=&quot;domain-and-task&quot;&gt;domain and task&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;domain = feature space + marginal distribution&lt;/li&gt;
  &lt;li&gt;task = label space + conditional distribution (predictive function)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;transfer-learning定义&quot;&gt;transfer learning定义&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15842471547628.jpg&quot; alt=&quot;-w914&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;正常的machine learning: 只有一个domain，通过自己的X、Y学习predictive function&lt;/li&gt;
  &lt;li&gt;transfer learning：利用source的知识提高target的predictive function的表现
    &lt;ul&gt;
      &lt;li&gt;source和target是不同的
        &lt;ul&gt;
          &lt;li&gt;feature space不同&lt;/li&gt;
          &lt;li&gt;marginal distribution不同&lt;/li&gt;
          &lt;li&gt;label space不同&lt;/li&gt;
          &lt;li&gt;conditional distribution不同  （？？）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;domain adaptation&lt;/em&gt;: transfer learning的子类，source和target只有marginal distribution不同，其余三点都相同&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;domain-adaptation&quot;&gt;Domain adaptation&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/img/15850826248103.jpg&quot; alt=&quot;&quot; /&gt;
from https://en.wikipedia.org/wiki/Domain_adaptation&lt;/p&gt;

</description>
        <pubDate>Sat, 14 Mar 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/03/14/Domain-Adaptation-Concept-Summary/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/03/14/Domain-Adaptation-Concept-Summary/</guid>
        
        <category>Domain Adaptation</category>
        
        <category>Concept</category>
        
        
      </item>
    
  </channel>
</rss>
