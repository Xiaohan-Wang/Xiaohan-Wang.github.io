<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiaohan's Blog</title>
    <description>Do it now.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 18 Jun 2020 00:04:02 -0400</pubDate>
    <lastBuildDate>Thu, 18 Jun 2020 00:04:02 -0400</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>迁移学习损失函数</title>
        <description>&lt;h4 id=&quot;基于统计量&quot;&gt;基于统计量&lt;/h4&gt;
&lt;h5 id=&quot;mmd-maximum-mean-discrepancy&quot;&gt;MMD (maximum mean discrepancy)&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;解决two-sample problem：通过比较两个distribution的sample，来判断两个distribution是否相同&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf&quot;&gt;paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;基于对抗学习&quot;&gt;基于对抗学习&lt;/h4&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://www.w3xue.com/exp/article/20196/40020.html&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 17 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/17/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/17/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</guid>
        
        <category>Loss</category>
        
        
      </item>
    
      <item>
        <title>估计量的无偏性、有效性、一致性</title>
        <description>&lt;h4 id=&quot;估计量&quot;&gt;估计量&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;根据样本构造一个统计量，作为总体未知参数的估计，则该统计量为估计量&lt;/li&gt;
  &lt;li&gt;估计量可视为一个随机变量
    &lt;ul&gt;
      &lt;li&gt;同一个总体可以进行多次抽样，不同的抽样结果可以计算出不同的估计量取值&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;无偏性&quot;&gt;无偏性&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;无偏估计指，估计量的数学期望等于被估计参数的真实值&lt;/li&gt;
  &lt;li&gt;例子&lt;br /&gt;
  &lt;img src=&quot;/img/15924068989806.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
  假设圆心是被估计参数的真实值，粉色x代表每次抽样计算出的估计量。左图估计量的期望等于被估计参数的真实值，是无偏的。右图估计量的期望不等于被估计参数的真实值，是有偏的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;有效性&quot;&gt;有效性&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;指估计量的离散程度，离散程度越小越有效&lt;/li&gt;
  &lt;li&gt;注意，有效性和无偏性是不相关的&lt;br /&gt;
  &lt;img src=&quot;/img/15924081407687.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;无偏估计不一定是最好的&lt;br /&gt;
  &lt;img src=&quot;/img/15924083443839.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;一致性&quot;&gt;一致性&lt;/h4&gt;
&lt;p&gt;待填…&lt;/p&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/22983179&quot;&gt;什么是无偏估计？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/qq_40597317/article/details/80639511&quot;&gt;估计量的无偏性、有效性、一致性&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 17 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/17/%E6%9C%89%E5%81%8F%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/17/%E6%9C%89%E5%81%8F%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/</guid>
        
        <category>Statistics</category>
        
        
      </item>
    
      <item>
        <title>MMD</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://www.aaai.org/Papers/AAAI/2007/AAAI07-262.pdf&quot;&gt;AAAI 2007 paper&lt;/a&gt; and &lt;a href=&quot;http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf&quot;&gt;JMLR 2012 paper&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;summary&quot;&gt;Summary&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;传统的用于衡量两个概率分布P和Q差别的方法，例如KL divergence，要求概率分布P和Q已知。也就是说，&lt;strong&gt;如果我们只有来自P和Q的样本，那么我们需要先进行概率密度估计 (density estimation)，然后才能衡量两个分布的差异&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;MMD：利用来自P和Q的样本，直接获得它们对应的总体概率分布的差异，而无需density estimation这一中间步骤。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;mmd-metric&quot;&gt;MMD metric&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Give observations $X := {x_1, \cdots , x_m}$ and $Y := {y_1, \cdots, y_n}$, which are independently and identically distributed (i.i.d.) from distribution $p$ and $q$. Let $\mathcal{F}$ be a class of functions $f : X \to \mathbb{R}$, and shorthand notation $E_x[ f(x)] :=E_{x \sim p}[ f(x)]$ and $E_y[ f(y)] := E_{y\sim q}[ f(y)]$ denote expectations with respect to $p$ and $q$, respectively, where $x \sim p$ indicates x has distribution $p$.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maximum mean discrepancy (MMD) is defined as:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;MMD[\mathcal{F}, p,q] := \sup_{f \in \mathcal{F}}(E_x[ f(x)]−E_y[ f(y)]) .&lt;/script&gt;

    &lt;p&gt;We must therefore dentify a function class that is &lt;strong&gt;rich enough to uniquely identify whether $p = q$&lt;/strong&gt;. And since we want to obtain this discrepancy by sample $X$ and $Y$,  the function class should also be &lt;strong&gt;restrictive enough to provide useful finite sample estimates&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Propose the unit ball in a reproducing kernel Hilbert space $\mathcal H$ as our MMD function class $\mathcal{F}$. Define &lt;strong&gt;mean embedding&lt;/strong&gt; $\mu_p(t) \in \mathcal{H}$ such that &lt;script type=&quot;math/tex&quot;&gt;E_x [f(x)] = \lt f, \mu_p \gt _{\mathcal{H}}&lt;/script&gt; for all $f \in \mathcal{H}$. If we set $f= \phi (t) = k(t, \cdot)$, we obtain $\mu_p(t) = &amp;lt;\mu_p, k(t, ·)&amp;gt;_\mathcal{H} =E_xk(t, x)$, in other words, the mean embedding of the distribution $p$ is the expectation under $p$ of the canonical feature map.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;We thus have
    &lt;center&gt; 
 $$
 \begin{split}
 MMD^2 \left[ \mathcal{F}, p,q \right] &amp;amp;= \left[ \sup_{||f||_\mathcal{H} \leq 1}(E_x [ f(x)]−E_y [ f(y)])
\right]^2\\&amp;amp;=    \left[ \sup_{||f||_\mathcal{H} \leq 1}&amp;lt;\mu_p-\mu_q,f&amp;gt;_\mathcal{H}
\right]^2\\&amp;amp;=||\mu_p-\mu_q||_\mathcal{H}^2\\&amp;amp;=||E_x\phi(x)-E_y\phi(y)||_\mathcal{H}^2
 \end{split}
 $$
 &lt;/center&gt;
  &lt;/li&gt;
  &lt;li&gt;The MMD is a metric, when $\mathcal{H}$ is a universal RKHSs, defined on a compact metric space X. It can be proven that &lt;strong&gt;the Gaussian and Laplace RKHSs&lt;/strong&gt; are universal.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;finite-sample-estimate&quot;&gt;Finite sample estimate&lt;/h4&gt;
&lt;p&gt;Given $x$ and $x^{\prime}$ independent random variables with distribution $p$, and $y$ and $y^\prime$ independent random variables with distribution $q$, the squared population MMD is&lt;/p&gt;
&lt;center&gt;
$$
MMD^2 [\mathcal{F}, p,q] = E_{x,x^\prime}
\left[k(x, x^\prime)\right] +E_{y,y^\prime}\left[ k(y, y′)\right]−2E_{x,y} \left[k(x, y)\right]
$$
&lt;/center&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;An unbiased empirical estimate
 &lt;img src=&quot;/img/15924519721534.jpg&quot; width=&quot;75%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An biased empirical estimate
 &lt;img src=&quot;/img/15924520728791.jpg&quot; width=&quot;75%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A linear time statistics&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf&quot;&gt;link&lt;/a&gt; $P_{739}$ Lemma 14&lt;/li&gt;
      &lt;li&gt;need sufficient data (many more samples than the quadratic-cost tests)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Estimate 1 and 2 cost $O((m+n)^2)$ time to compute both statistics&lt;/li&gt;
  &lt;li&gt;Estimate 3 can be computed in linear time&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cnblogs.com/kailugaji/p/11004246.html&quot;&gt;MATLAB最大均值差异&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 17 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/17/MMD/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/17/MMD/</guid>
        
        <category>Statistics</category>
        
        
      </item>
    
      <item>
        <title>核方法、核函数、核技巧、再生核希尔伯特空间</title>
        <description>&lt;h4 id=&quot;核方法kernel-method&quot;&gt;核方法（kernel method）&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;理论基础: Cover’s theorem，其指出，在低维空间中线性不可分的数据，通过非线性变换将其投影到高维空间之后，大概率会变为线性可分的数据&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;将低维空间的非线性可分问题，转化为高维空间的线性可分问题&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;核技巧kernel-trick&quot;&gt;核技巧（kernel trick）&lt;/h4&gt;
&lt;p&gt;设$\phi(x)$为映射函数，$\phi(x): \mathcal{X} \to \mathcal{H}$，其中$\mathcal{X}$为输入空间，$\mathcal{H}$为特征空间 (特征空间需要是Hilbert space，即完备的内积空间)。&lt;/p&gt;

&lt;p&gt;欲求$&amp;lt;\phi(x_1), \phi(x_2)&amp;gt;$：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;传统方法：先分别计算$\phi(x_1)$和$\phi(x_2)$，再在特征空间中计算二者的内积
    &lt;ul&gt;
      &lt;li&gt;缺点：当特征空间维度很大时，计算非常复杂&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;核技巧：在输入空间找到一个函数$K(x_1, x_2)$，使得$K(x_1, x_2)=&amp;lt;\phi(x_1), \phi(x_2)&amp;gt;$，从而可以直接在低维空间中计算出结果，加速核方法计算。对应的函数 $K$ 就是核函数&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;核函数kernels&quot;&gt;核函数（kernels）&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;在实际应用时，映射函数 $\phi(x)$ 不需要是已知的。换句话说，核技巧的目的就是在不需要显式地定义特征空间和映射函数的条件下，计算映射之后的内积&lt;/li&gt;
  &lt;li&gt;核函数 $K$ 本质上需要满足的条件 (不需要$\phi(x)$已知):
    &lt;ul&gt;
      &lt;li&gt;对称性: $K(\mathrm{x_1},\mathrm{x_2}) = K(\mathrm{x_2},\mathrm{x_1})$&lt;/li&gt;
      &lt;li&gt;半正定性: 对于任意 $n$ 和任意 $x_1, x_2, \cdots, x_n  \in \mathcal{X}$，由 $K(x_i, x_j)$ 定义的 Gram matrix 总是半正定的&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;只要 $K$ 是核函数，那么一定存在一个Hilbert space和一个映射函数$\phi$，使得$K(x_1, x_2)=&amp;lt;\phi(x_1), \phi(x_2)&amp;gt;$&lt;/li&gt;
  &lt;li&gt;常见核函数&lt;br /&gt;
  &lt;img src=&quot;/img/15923213053700.jpg&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
  其它变换得到的核函数：参考资料3 $\text{P}_{17, 18}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;再生核希尔伯特空间reproducing-kernel-hilbert-spacesrkhs&quot;&gt;再生核希尔伯特空间（reproducing kernel Hilbert spaces，RKHS）&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;再生核希尔伯特空间
    &lt;ul&gt;
      &lt;li&gt;设 $\displaystyle \mathcal{H}$ 是希尔伯特空间，其元素为函数 $\displaystyle f:X\rightarrow R$。对于某个固定的 $\displaystyle x\in \mathcal{X}$，映射$\displaystyle \delta _{x} :H\rightarrow R,\delta _{x} :f\rightarrow f( x)$称为点 $x$ 的 (Dirac) evaluation functional&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;设 $\displaystyle \mathcal{H}$ 是希尔伯特空间，其元素为函数 $\displaystyle f:X\rightarrow R$。若对于任意的 $x \in \mathcal{X}$，$\delta_x$ 都是连续的，则 $\mathcal{H}$ 为RKHS&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;RKHS是一个函数空间，其中的元素 $f$ 为函数。通常情况下特征空间 $\mathcal{H}$ 为 RKHS。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;再生核
    &lt;ul&gt;
      &lt;li&gt;再生核 $K$ 是核函数的一种，其满足
        &lt;ul&gt;
          &lt;li&gt;对于任意固定的 $x_0\in\mathcal{X}$，$K(x,x_0)$作为 $x$ 的函数属于我们的函数空间$\mathcal{H}$&lt;/li&gt;
          &lt;li&gt;对于任意 $x\in\mathcal{X}$ 和 $f(\cdot)\in\mathcal{H}$，有 $f(x) = \langle f(\cdot),K(\cdot,x)\rangle$ (再生性质 / reproducing property)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;对于再生核 $K$，我们可以自然的定义映射函数 $\phi(x)=K(\cdot, x)$，此时，通过再生核的再生性质，可知
  &lt;script type=&quot;math/tex&quot;&gt;\langle \phi(x_1),\phi(x_2)\rangle = \langle K(\cdot,x_1),K(\cdot,x_2)\rangle = K(x_1,x_2)&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;关系
    &lt;ul&gt;
      &lt;li&gt;一个希尔伯特空间存在至多一个再生核 (不存在 / 存在一个)&lt;/li&gt;
      &lt;li&gt;存在再生核的希尔伯特空间就是再生核希尔伯特空间&lt;/li&gt;
      &lt;li&gt;一个再生核对应唯一的再生核希尔伯特空间&lt;/li&gt;
      &lt;li&gt;再生核和再生核希尔伯特空间是一一对应的&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;一个核函数 $K$ 可能对应多个映射函数 $\phi$，而每个映射函数$\phi$ 有自己对应的特征空间 (i.e. 希尔伯特空间）
    &lt;ul&gt;
      &lt;li&gt;例子详见 参考资料6 $P_{11}$ example 35&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;任意一个核函数 $K$ 都可以作为再生核，构建其对应的唯一的再生核希尔伯特空间&lt;/li&gt;
  &lt;li&gt;对于任意一个核函数 $K$，可以存在多个对应的特征空间 $\mathcal{H_0}$，但其作为再生核只对应唯一的RKHS。同时，RKHS是所有特征空间最为“精简”的一个，这里的精简体现在，无论在 $\mathcal{H_0}$ 中得到怎样的分类模型 $⟨w,\phi_0(x)⟩_{\mathcal{H_0}}$，在RKHS中都存在一个 $f$ 可以得到和它相同的效果，因此对于某个核函数，RKHS代表了它最本征的信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/61794781&quot;&gt;核方法、核技巧和核函数&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.fanyeong.com/2017/11/13/the-kernel-trick/&quot;&gt;核技巧&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.stat.berkeley.edu/~bartlett/courses/2014spring-cs281bstat241b/lectures/20-notes.pdf&quot;&gt;PPT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cosx.org/2014/05/svm-series-add-2-kernel-ii/&quot;&gt;“支持向量机系列” 的番外篇二: Kernel II&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/54704957&quot;&gt;什么是RKHS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/RKHS_Notes1.pdf&quot;&gt;课程讲义&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://murongxixi.github.io/2013/11/12/%E6%A0%B8%E5%87%BD%E6%95%B0%EF%BC%8C%E5%86%8D%E7%94%9F%E6%A0%B8Hilbert%E7%A9%BA%E9%97%B4%EF%BC%8C%E8%A1%A8%E7%A4%BA%E5%AE%9A%E7%90%86/&quot;&gt;核函数，再生核Hilbert空间，表示定理&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 16 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/16/%E6%A0%B8%E6%96%B9%E6%B3%95-%E6%A0%B8%E5%87%BD%E6%95%B0-%E6%A0%B8%E6%8A%80%E5%B7%A7/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/16/%E6%A0%B8%E6%96%B9%E6%B3%95-%E6%A0%B8%E5%87%BD%E6%95%B0-%E6%A0%B8%E6%8A%80%E5%B7%A7/</guid>
        
        <category>Math</category>
        
        
      </item>
    
      <item>
        <title>Amazon SageMaker</title>
        <description>
&lt;h4 id=&quot;amazon-sagemaker简介&quot;&gt;Amazon SageMaker简介&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;AWS提供的面向所有开发人员和数据科学家的&lt;strong&gt;机器学习服务&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;可以帮助开快速构建、训练和部署机器学习 (ML) 模型&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Amazon SageMaker Ground Truth
    &lt;ul&gt;
      &lt;li&gt;快速构建和管理高度准确的训练数据集&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Amazon SageMaker Studio
    &lt;ul&gt;
      &lt;li&gt;首个&lt;strong&gt;适用于机器学习的完全集成式开发环境 (IDE)&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;提供了一个基于 Web 的可视化界面，可以通过该界面执行所有 ML 开发步骤，包括笔记本、实验管理、自动创建模型、调试以及模型偏差检测&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;amazon-sagemaker-studio&quot;&gt;Amazon SageMaker Studio&lt;/h4&gt;
&lt;p&gt;待填…&lt;/p&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://aws.amazon.com/cn/sagemaker/?nc2=h_ql_prod_ml_sm&lt;/li&gt;
  &lt;li&gt;https://aws.amazon.com/cn/getting-started/hands-on/create-machine-learning-model-automatically-sagemaker-autopilot/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 12 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/12/SageMaker/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/12/SageMaker/</guid>
        
        <category>Tools</category>
        
        
      </item>
    
      <item>
        <title>AWS</title>
        <description>
&lt;h4 id=&quot;aws简介&quot;&gt;AWS简介&lt;/h4&gt;
&lt;p&gt;Amazon Web Services (AWS) 是全球最全面、应用最广泛的云平台。从全球数据中心提供涉及超过十二种类别的，超过 175 项功能齐全的服务：不仅包括计算、存储和数据库等基础设施技术，而且提供机器学习、人工智能、数据湖和分析以及物联网等新兴技术。&lt;/p&gt;

&lt;h4 id=&quot;amazon-sagemaker&quot;&gt;Amazon SageMaker&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;面向所有开发人员和数据科学家的&lt;strong&gt;机器学习服务&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;可以帮助开快速构建、训练和部署机器学习 (ML) 模型&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;amazon-ec2-amazon-elastic-compute-cloud&quot;&gt;Amazon EC2 (Amazon Elastic Compute Cloud)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;可以在云中提供安全并且可调整大小的计算容量&lt;/li&gt;
  &lt;li&gt;提供多种经过优化，适用于不同使用场景的实例类型以供选择。&lt;strong&gt;不同实例类型具有不同的 CPU、内存、存储和网络容量，可以灵活地为不同应用程序选择适当的资源组合&lt;/strong&gt;。每种实例类型都包括一种或多种实例大小，从而能够扩展资源以满足目标工作负载的要求。
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/cn/ec2/instance-types/&quot;&gt;实例类型&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/cn/ec2/pricing/on-demand/&quot;&gt;定价&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://aws.amazon.com/cn/getting-started/fundamentals-overview/&lt;/li&gt;
  &lt;li&gt;https://aws.amazon.com/cn/sagemaker/?nc2=h_ql_prod_ml_sm&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 12 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/12/AWS/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/12/AWS/</guid>
        
        <category>Tools</category>
        
        
      </item>
    
      <item>
        <title>假设检验</title>
        <description>&lt;h4 id=&quot;各种分布&quot;&gt;各种分布&lt;/h4&gt;
&lt;h5 id=&quot;正态分布&quot;&gt;正态分布&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15919078017261.jpg&quot; width=&quot;50%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$\mu \pm \sigma$ 之间的概率是68%&lt;/li&gt;
  &lt;li&gt;$\mu \pm 1.96\sigma$ 之间的概率是95%&lt;/li&gt;
  &lt;li&gt;$\mu \pm 2.56\sigma$ 之间的概率是99%&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;其它分布&quot;&gt;其它分布&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/img/15919118904133.jpg&quot; width=&quot;30%&quot; height=&quot;100%&quot; /&gt;&lt;br /&gt;
和正态分布不同，需要其它方法计算对应的概率范围&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;假设检验&quot;&gt;假设检验&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;原假设 (Null hypothesis)&lt;/strong&gt;：也叫零假设，用 $H_0$ 来表示，一般是希望能证明为错误的假设。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;备择假设 (Alternative hypothesis)&lt;/strong&gt;：原假设的反面，一般用 $H_1$ 来表示，是希望证明是正确的另一种可能。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;检验统计量 (Test statistic)&lt;/strong&gt;：用于统计假设检验的统计量，是数据集的数字汇总，将数据减少到可用于执行假设检验的一个值。&lt;/li&gt;
  &lt;li&gt;假设检验的目的在于试图找到证据拒绝原假设。当没有足够证据拒绝原假设时，不采用 “接受原假设” 的表述，而采用 “不拒绝原假设” 的表述。“不拒绝”的表述实际上意味着并未给出明确的结论，我们没有说原假设正确，也没有说它不正确。&lt;strong&gt;假设检验的主要目的是为了拒绝而不是接受。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;思路&quot;&gt;思路&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;欲证：备择假设为真&lt;/li&gt;
  &lt;li&gt;原理1: &lt;strong&gt;假设证明难而证伪易&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;只需证：备择假设的否命题（原假设）为假&lt;/li&gt;
  &lt;li&gt;原理2: &lt;strong&gt;小概率事件在少量实验中是不可能出现的&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;只需：观察到“&lt;strong&gt;原假设条件下的小概率事件&lt;/strong&gt;的发生”&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;检验方法&quot;&gt;检验方法&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;首先确定“小概率事件”的概率范围，即&lt;strong&gt;显著性水平&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;e.g. 显著性水平为0.05，即发生概率小于5%的事件为“小概率事件”&lt;/li&gt;
      &lt;li&gt;显著性水平越高，“小概率事件”发生的可能性越大，即越来越容易拒绝原假设&lt;/li&gt;
      &lt;li&gt;“小概率事件”发生的区域对应&lt;strong&gt;拒绝域&lt;/strong&gt;，&lt;strong&gt;拒绝域&lt;/strong&gt;没有覆盖的区域为&lt;strong&gt;置信区间&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;如下图：设已知成绩服从正态分布，原假设为成绩均值为$\mu$，在该条件下，红色区域对应小概率事件 (发生概率为5%)，即拒绝域概率为5%，置信区间概率为95%&lt;br /&gt;
 &lt;img src=&quot;/img/15919111868762.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;检验方法1 (临界概率 to 临界值):
    &lt;ul&gt;
      &lt;li&gt;根据设定的显著度和对应分布找到“小概率事件”对应的临界值&lt;/li&gt;
      &lt;li&gt;计算样本得到的统计量，判定是否越过了临界值&lt;/li&gt;
      &lt;li&gt;若是，则进入了小概率错误域 (拒绝域)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;检验方法2 (样本值 to 样本概率)：
    &lt;ul&gt;
      &lt;li&gt;计算样本得到的统计量，进而在分布中找到对应的概率值，即p值&lt;/li&gt;
      &lt;li&gt;将p值与显著度 (设定的“小概率事件”判定值) 进行比较&lt;/li&gt;
      &lt;li&gt;如果p值比显著度小，则发生的样本属于“小概率事件”，故而拒绝原假设&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;检验方法1和2本质上是相同的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;错误类型&quot;&gt;错误类型&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;I类错误 (Type I error): 拒绝了正确的零假设&lt;/li&gt;
  &lt;li&gt;II类错误 (Type II error): 没有拒绝错误的零假设&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/15920070800446.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://www.zhihu.com/question/20254932&lt;/li&gt;
  &lt;li&gt;https://cosx.org/2009/03/meaning-of-failure-to-reject-h0/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 11 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/11/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/11/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/</guid>
        
        <category>Statistics</category>
        
        
      </item>
    
      <item>
        <title>从 总体 &amp; 样本 估计总体方差</title>
        <description>&lt;h4 id=&quot;用总体估计总体方差&quot;&gt;用总体估计总体方差&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;calculating population variance and std for the whole population using all the data&lt;/li&gt;
  &lt;li&gt;e.g. What is the standard deviation of last year’s returns of the 12 funds I have invested in? In this case, we have the data for the whole population available.&lt;/li&gt;
  &lt;li&gt;When using the whole population to calculate population variance, &lt;strong&gt;divide&lt;/strong&gt; the sum of squared deviations from the mean &lt;strong&gt;by the number of items in the population&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;用样本估计总体方差&quot;&gt;用样本估计总体方差&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;calculating population variance and std using only a sample of data&lt;/li&gt;
  &lt;li&gt;e.g. What is the standard deviation of last year’s returns of equity funds in the world? In this case, we don’t have all the data available and we will have to estimate the population’s standard deviation from a sample.&lt;/li&gt;
  &lt;li&gt;When using a sample to calculate population variance, &lt;strong&gt;divide it by the number of items in the sample less one&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;As a result, the calculated variance (and therefore also the standard deviation) will &lt;strong&gt;be slightly higher than if we would have used the population variance formula&lt;/strong&gt;. The purpose of this little difference is to get a better and unbiased estimate of the population‘s variance (by dividing by the sample size lowered by one, we &lt;strong&gt;compensate for the fact that we are working only with a sample rather than with the whole population&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;参考资料&quot;&gt;参考资料&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://www.macroption.com/population-sample-variance-standard-deviation/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 11 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/11/population-and-sample-variance/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/11/population-and-sample-variance/</guid>
        
        <category>Statistics</category>
        
        
      </item>
    
      <item>
        <title>EM algorithm, K-means, and GMM</title>
        <description>&lt;h4 id=&quot;em-algorithm&quot;&gt;EM algorithm&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;EM算法是期望最大化(Expectation Maximization)算法的简称&lt;/li&gt;
  &lt;li&gt;用于&lt;strong&gt;含有隐变量(hidden variable)&lt;/strong&gt;的情况下，模型参数的最大似然估计&lt;/li&gt;
  &lt;li&gt;EM算法是一种迭代算法，每次迭代由两步组成：
    &lt;ul&gt;
      &lt;li&gt;E步：根据模型参数的假设值，给出隐变量的期望估计，应用于缺失值&lt;/li&gt;
      &lt;li&gt;M步：根据隐变量的估计值，给出当前的参数的极大似然估计&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;k-means&quot;&gt;K-means&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;先随机选定k个点作为质心 $\mu_1, \mu_2, \cdots, \mu_𝑘$&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;E step: 固定$\mu_k$，将样本划分到距离最近的$\mu_k$所属的簇中&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
r_{nk} = \left. \begin{cases} 1 \,\, &amp; \text{if} \;\;\;k = \mathop{argmin}_j ||\mathbf{x}_n - \boldsymbol{\mu}_j||^2 \\
0 \,\, &amp; \text{otherwise} \end{cases} \right. %]]&gt;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;M step: 对于每一个数据簇，重新计算其中心，目标是最小化簇中每个样本与中心的距离，可表示为&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;J = \sum\limits_{n=1}^N r_{nk} ||\mathbf{x}_n - \boldsymbol{\mu}_k||^2.&lt;/script&gt;

    &lt;p&gt;为求得最小化 $J$ 的 $\mu_k$，可通过&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial\mathbf{\mu}_k}=2\sum\limits_{n=1}^N r_{nk}(\boldsymbol{x}_n - \boldsymbol{\mu}_k) = 0,&lt;/script&gt;

    &lt;p&gt;求得&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\mu}_k = \frac{\sum_nr_{nk} \mathbf{x}_n}{\sum_n r_{nk}},&lt;/script&gt;

    &lt;p&gt;即簇中每个样本的均值向量。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;重复计算 E-step 和 M-step 直至收敛&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;gaussian-mixture-model-gmm&quot;&gt;Gaussian Mixture Model (GMM)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;混合模型 (mixture model)：是一个可以用来表示在总体分布中含有 K 个子分布的概率模型。换句话说，总体的概率分布，是一个由 K 个子分布组成的混合分布。
    &lt;ul&gt;
      &lt;li&gt;混合模型不要求观测数据提供其属于哪个子分布 =&amp;gt; hiddle varibale&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;对于混合模型来说，每个子分布天然地构成了各自的一类&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;高斯混合模型：由 K 个单高斯模型组合而成的模型。一般来说，一个混合模型可以使用任何概率分布，这里使用高斯混合模型是因为高斯分布具备很好的数学性质以及良好的计算性能。
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;高斯混合模型的概率分布为：&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x|\theta) = \sum_{k=1}^{K}{\alpha_{k}\phi(x|\theta_{k})}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;对于这个模型而言，参数 $\theta = (\tilde{\mu_{k}}, \tilde{\sigma_{k}}, \tilde{\alpha_{k}})$ ，也就是每个子模型的期望、方差（或协方差）、在混合模型中发生的概率&lt;/li&gt;
      &lt;li&gt;如果通过使用足够多的高斯分布，并且调节它们的均值和方差以及线性组合的系数，那么几乎所有的连续概率密度都能够以任意的精度近似。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;随机初始化模型参数（各个高斯分布的均值、方差、发生概率）&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;E step: 依据当前参数，计算每个数据 $j$ 来自子模型 $k$ 的可能性&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_{jk} = \frac{\alpha_{k}\phi(x_{j}|\theta_{k})}{\sum_{k=1}^{K}{\alpha_{k}\phi(x_{j}|\theta_{k})}}, j = 1,2,...,N; k = 1,2,...,K&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;M step: 计算新的模型参数&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_{k} = \frac{\sum_{j=1}^{N}{(\gamma_{jk}}x_{j})}{\sum_{j=1}^{N}{\gamma_{jk}}}, k=1,2,...,K&lt;/script&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_{k} = \frac{\sum_{j=1}^{N}{\gamma_{jk}}(x_{j}-\mu_{k})(x_{j}-\mu_{k})^{T}}{\sum_{j=1}^{N}{\gamma_{jk}}}, k = 1,2,...,K&lt;/script&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_{k} = \frac{\sum_{j=1}^{N}{\gamma_{jk}}}{N}, k=1,2,...,K&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;重复计算 E-step 和 M-step 直至收敛&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;difference&quot;&gt;difference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;K-means: hard assignment&lt;/strong&gt;
in each iteration, we are absolutely certain as to which cluster the point belongs to&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GMM: soft assignment&lt;/strong&gt;
 It starts with some prior belief about how certain we are about each point’s cluster assignments. As it goes on, it revises those beliefs&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;reference&quot;&gt;Reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/What-is-the-difference-between-K-means-and-the-mixture-model-of-Gaussian&quot;&gt;difference of k-means and GMM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sites.northwestern.edu/msia/2016/12/08/k-means-shouldnt-be-our-only-choice/&quot;&gt;k-means shouldn’t be our only choice&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/30483076&quot;&gt;EM algorithm, K-means, and GMM 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/75554749&quot;&gt;EM algorithm, K-means, and GMM 2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 09 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/09/K-means%E5%92%8CGMM/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/09/K-means%E5%92%8CGMM/</guid>
        
        <category>Clustering</category>
        
        
      </item>
    
      <item>
        <title>Image classification</title>
        <description>&lt;h4 id=&quot;cifar-10&quot;&gt;CIFAR-10&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;60000 32x32 color images in 10 classes, with 6000 images per class&lt;/li&gt;
  &lt;li&gt;50000 training images and 10000 test images&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/img/15917518925606.jpg&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 09 Jun 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2020/06/09/Image-classification-dataset/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/09/Image-classification-dataset/</guid>
        
        <category>Datasets</category>
        
        
      </item>
    
  </channel>
</rss>
