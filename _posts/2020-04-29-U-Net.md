---
layout:     post
title: U-Net
subtitle: MICCAI 2015
date:       2020-4-29
author:     Xiaohan
header-img: img/ice.jpg
catalog: true
tags:
    - Semantic Segmentation
---

[Paper link](https://arxiv.org/pdf/1505.04597.pdf) and [code](https://github.com/milesial/Pytorch-UNet)

[//]: #(<img src="" width="100%" height="100%">)

#### Take away message
1. contracting path + expending path + skip connection
2. strong data augmentation, works well for very few images (it was designed for biomedical images)

#### Model
<img src="/img/15882170880287.jpg" width="80%" height="100%">
<img src="/img/15882176961707.jpg" width="80%" height="100%">

* It use valid convolution, thus the resolution gets lower and lower in the contracting path and expending path. 
* Because of the high resolution of biomedical images, overlap-tile strategy is used.
* To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image.

contracting path:
1. repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels.
2. an upsampling of the feature map by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU.
3. At the final layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes.

Training objective:
1. softmax + cross entropy
2. class balancing (it's important because of the large background)

#### Data augmentation
Teach the network the desired invariance and robustness properties, when only few training samples are available:
1. shift 
2. rotation
3. gray value deformations
4. random elastic deformation

#### Tricks
1. select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size.
2. favor large input tiles over a large batch size and hence reduce the batch to a single image.
3. use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step. 
4. Xavier initialization.

#### Time
1. Training time: 10 hours on a NVidia Titan GPU (6 GB)
2. Inference time: Segmentation of a 512x512 image takes less than a second on a recent GPU


