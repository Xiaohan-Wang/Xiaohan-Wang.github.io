---
layout:     post
title: MMD
subtitle: AAAI 2007 & JMLR 2012
date:       2020-6-17
author:     Xiaohan
header-img: img/ice.jpg
catalog: true
tags:
    - Statistics
---

[AAAI 2007 paper](https://www.aaai.org/Papers/AAAI/2007/AAAI07-262.pdf) and [JMLR 2012 paper](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf)

[//]: #(<img src="" width="100%" height="100%">)

#### Summary
* 传统的用于衡量两个概率分布P和Q差别的方法，例如KL divergence，要求概率分布P和Q已知。也就是说，**如果我们只有来自P和Q的样本，那么我们需要先进行概率密度估计 (density estimation)，然后才能衡量两个分布的差异**。
* MMD：利用来自P和Q的样本，直接获得它们对应的总体概率分布的差异，而无需density estimation这一中间步骤。

#### MMD metric
* Give observations $X := {x_1, \cdots , x_m}$ and $Y := {y_1, \cdots, y_n}$, which are independently and identically distributed (i.i.d.) from distribution $p$ and $q$. Let $\mathcal{F}$ be a class of functions $f : X \to \mathbb{R}$, and shorthand notation $E_x[ f(x)] :=E_{x \sim p}[ f(x)]$ and $E_y[ f(y)] := E_{y\sim q}[ f(y)]$ denote expectations with respect to $p$ and $q$, respectively, where $x \sim p$ indicates x has distribution $p$.
* Maximum mean discrepancy (MMD) is defined as:
    
    $$MMD[\mathcal{F}, p,q] := \sup_{f \in \mathcal{F}}(E_x[ f(x)]−E_y[ f(y)]) .$$
    
    We must therefore dentify a function class that is **rich enough to uniquely identify whether $p = q$**. And since we want to obtain this discrepancy by sample $X$ and $Y$,  the function class should also be **restrictive enough to provide useful finite sample estimates**.
* Propose the unit ball in a reproducing kernel Hilbert space $\mathcal H$ as our MMD function class $\mathcal{F}$. Define **mean embedding** $\mu_p(t) \in \mathcal{H}$ such that $$E_x [f(x)] = \lt f, \mu_p \gt _{\mathcal{H}}$$ for all $f \in \mathcal{H}$. If we set $f= \phi (t) = k(t, \cdot)$, we obtain $\mu_p(t) = <\mu_p, k(t, ·)>_\mathcal{H} =E_xk(t, x)$, in other words, the mean embedding of the distribution $p$ is the expectation under $p$ of the canonical feature map. 

*  We thus have 
    <center> 
    $$
    \begin{split}
    MMD^2 \left[ \mathcal{F}, p,q \right] &= \left[ \sup_{||f||_\mathcal{H} \leq 1}(E_x [ f(x)]−E_y [ f(y)])
\right]^2\\&=    \left[ \sup_{||f||_\mathcal{H} \leq 1}<\mu_p-\mu_q,f>_\mathcal{H}
\right]^2\\&=||\mu_p-\mu_q||_\mathcal{H}^2\\&=||E_x\phi(x)-E_y\phi(y)||_\mathcal{H}^2
    \end{split}
    $$
    </center>
* The MMD is a metric, when $\mathcal{H}$ is a universal RKHSs, defined on a compact metric space X. It can be proven that **the Gaussian and Laplace RKHSs** are universal.

#### Finite sample estimate
Given $x$ and $x^{\prime}$ independent random variables with distribution $p$, and $y$ and $y^\prime$ independent random variables with distribution $q$, the squared population MMD is 
<center>
$$
MMD^2 [\mathcal{F}, p,q] = E_{x,x^\prime}
\left[k(x, x^\prime)\right] +E_{y,y^\prime}\left[ k(y, y′)\right]−2E_{x,y} \left[k(x, y)\right]
$$
</center>

1. An unbiased empirical estimate
    <img src="/img/15924519721534.jpg" width="75%" height="100%">

1. An biased empirical estimate
    <img src="/img/15924520728791.jpg" width="75%" height="100%">
    
1. A linear time statistics
    * [link](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf) $P_{739}$ Lemma 14
    * need sufficient data (many more samples than the quadratic-cost tests)
    
* Estimate 1 and 2 cost $O((m+n)^2)$ time to compute both statistics
* Estimate 3 can be computed in linear time
    
    
    
    
#### 参考资料
1. [MATLAB最大均值差异](https://www.cnblogs.com/kailugaji/p/11004246.html)







