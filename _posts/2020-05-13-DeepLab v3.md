---
layout:     post
title: DeepLab v3
subtitle:   arXiv 2017
date:       2020-5-13
author:     Xiaohan
header-img: img/ice.jpg
catalog: true
tags:
    - Semantic Segmentation
---

[Paper link](https://arxiv.org/pdf/1706.05587.pdf)

#### Take away message
1. modules employing atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates.
2. 主要还是各种tricks

#### Model
output_stride: the ratio of input image spatial resolution to final output resolution.
##### atrous convolution in cascade
![-w840](/img/15893990006461.jpg)
1. duplicate several copies of the last ResNet block (Block 5, Block 6, Block 7)
2. apply atrous convolution with rates determined by the desired output stride value (output stride = 256 if no atrous convolution is applied)
3. multi-grid method: adopt different atrous rates within block4 to block7 in the proposed model. For example, when $\text{output stride} = 16$ and $\text{Multi Grid} = (1, 2, 4)$, the three convolutions will have $rates = 2 · (1, 2, 4) = (2, 4, 8)$ in the block4, respectively.

##### atrous convolution in parallel
![-w778](/img/15893997101137.jpg)
1. include batch normalization within ASPP.
2. image level feature: apply global average pooling on the last feature map of the model, feed the resulting image-level features to a 1 × 1 convolution with 256 filters (and batch normalization), and then bilinearly upsample the feature to the desired spatial dimension.
3. the improved ASPP consists of 
    * one 1×1 convolution, and three 3 × 3 convolutions with rates = (6, 12, 18) when output stride = 16 (all with 256 filters and batch normalization). Note that the rates are doubled when output stride = 8.
    * the image-level features (the rates are doubled when output stride = 8)
4. The resulting features from all the branches are concatenated and pass through another 1 × 1 convolution (with 256 filters and batch normalization) before the final 1 × 1 convolution which generates the final logits.

#### Tricks
1. “poly” learning rate policy: the initial learning rate is
multiplied by $(1 −\text{maxiter})^{\text{power}}$ with $\text{power} = 0.9 $.
1. **upsampling logits: upsample the final logits, since downsampling the groundtruths removes the fine annotations resulting in no back-propagation of details.**
2. $\text{output stride}=16$ during training, and $\text{output stride}=8$ during inference to get more detailed feature map.

#### Result
1. cascade:
<img src="/img/15894010210339.jpg" width="60%" height="100%">
1. parallel:
<img src="/img/15894010431381.jpg" width="60%" height="100%">

* Both their best cascaded model (in Tab. 4) and ASPP model (in Tab. 6) (in both cases without DenseCRF post-processing or MS-COCO pre-training) already outperform DeepLabv2 (77.69% with DenseCRF and pretrained on MS-COCO) on the PASCAL VOC 2012 val set. 
* The improvement mainly comes from including and fine-tuning batch normalization parameters in the proposed models and having a better way to encode multi-scale context.

#### Further reference 
1. S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167, 2015.