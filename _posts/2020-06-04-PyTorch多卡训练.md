---
layout:     post
title: 多卡训练
subtitle:   
date:       2000-1-1
author:     Xiaohan
header-img: img/ice.jpg
catalog: true
tags:
    - PyTorch
---

[//]: #(<img src="" width="100%" height="100%">)

#### CUDA_VISIBLE_DEVICES
* CUDA_VISIBLE_DEVICES环境变量限制CUDA程序可见的GPU设备
* CUDA应用运行时，CUDA将遍历当前**可见的**设备，并从零开始为可见设备编号（因此显卡的实际编号和程序看到的编号不同）
* 如果设备序列是存在和不存在设备的混合，那么不存在设备前的所有存在设备将被重新编号，不存在设备之后的所有设备将被屏蔽

1. 在终端中设置
    `CUDA_VISIBLE_DEVICES=0,1 python my_script.py`
2. python代码开头设置
    `os.environ["CUDA_VISIBLE_DEVICES"] = "0,2"`

#### PyTorch使用GPU
* `data.cuda()`: the old, pre-0.4 way
* `data.to(device)`: more flexible. For example, `data.to('cuda:1')` put data to GPU1

#### 单机单卡
如果不设置Dataparallel，即使一台机器上有多个GPU，pytorch也只会占用编号为0的GPU

#### 单机多卡
```python
# have to put model and data to GPU0

# model

model = nn.DataParallel(model)
model = model.to('cuda:0')
# data

data = data.to('cuda:0')
```
Note: `nn.DataParallel`默认使用可见的所有显卡。如果只想使用特定的显卡，可以   
1. 修改CUDA_VISIBLE_DEVICES环境变量  
2. 使用`nn.DataParallel(model, device_ids=[1,2])`

#### 多机多卡
待填

#### 参考资料
1. https://www.jianshu.com/p/0816c3a5fa5c
2. https://zhuanlan.zhihu.com/p/86441879