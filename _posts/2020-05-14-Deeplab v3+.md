---
layout:     post
title: DeepLab v3+
subtitle: ECCV 2018
date:       2020-5-14
author:     Xiaohan
header-img: img/ice.jpg
catalog: true
tags:
    - Semantic Segmentation
---

[Paper link](https://arxiv.org/pdf/1802.02611.pdf) and [code](https://github.com/jfzhang95/pytorch-deeplab-xception)

[//]: #(<img src="" width="100%" height="100%">)

#### Take away message
1. encoder-decoder structure: 
    * DeepLabv3 is used to encode the rich contextual information
    * a simple yet effective decoder module is adopted to recover the object boundaries
2. explore the Xception model and atrous separable convolution to make the proposed model faster and stronger

#### Introduction

#### Model
##### ResNet as baseline
<img src="/img/15895024156855.jpg" width="80%" height="100%">
1. DeepLab v3 as encoder: use the last feature map before logits in the original DeepLab v3 as the encoder output (contains 256 channels and rich semantic information).
2. Decoder:
    * The encoder features from DeepLabv3 are usually computed with output stride = 16 
    * first bilinearly upsampled the encoder features by a factor of 4
    * concatenate the upsampled features with the corresponding low-level features from the network backbone that have the same spatial resolution (e.g., Conv2 before striding in ResNet-101)
        * 1 × 1 convolution on the low-level features to reduce the number of channels, since the corresponding low- level features usually contain a large number of channels (e.g., 256 or 512) which may outweigh the importance of the rich encoder features (only 256 channels in our model) and make the training harder. 
    * apply a few (two) 3 × 3 convolutions to refine the features
    * another simple bilinear upsampling by a factor of 4

* DeepLabv3+ model has final output stride = 4
    * encoder output stride = 16
        * upsample by a factor of 4
        * concatenate with Conv2
        * another upsample by a factor of 4
    * encoder output stride = 8
        * upsample by a factor of 2
        * concatenate with Conv2
        * another upsample by a factor of 4

##### Modified Aligned Xception
<img src="/img/15895033728737.jpg" width="70%" height="100%">

#### Result
##### ResNet as baseline
1. reduce the channels of low level features
<img src="/img/15895034376564.jpg" width="70%" height="100%">
1. number of 3x3 convolutions / low level feature
<img src="/img/15895034547360.jpg" width="70%" height="100%">
1. detailed performance
<img src="/img/15895035126473.jpg" width="70%" height="100%">

##### Xception
<img src="/img/15895037132342.jpg" width="70%" height="100%">
* employing Xception as network backbone improves the performance by about 2% when train output stride = eval output stride = 16 over the case where ResNet-101 is used

##### improvement on object boundaries
<img src="/img/15895038788124.jpg" width="80%" height="100%">

#### Further reference
1. Chollet, F.: Xception: Deep learning with depthwise separable convolutions. In: CVPR. (2017)